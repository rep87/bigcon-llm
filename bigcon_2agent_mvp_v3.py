# -*- coding: utf-8 -*-
# BIGCON 2-Agent MVP (Colab, Gemini API) ‚Äî v3 (fits actual 3-dataset structure)
# %pip -q install google-generativeai pandas openpyxl

import os, json, re, random, sys, glob, datetime
from time import perf_counter
import unicodedata
from difflib import SequenceMatcher
from pathlib import Path
import pandas as pd
import numpy as np
from jsonschema import Draft7Validator, ValidationError

APP_ROOT = Path(__file__).resolve().parent
DATA_DIR = APP_ROOT / 'data'
SHINHAN_DIR = DATA_DIR / 'shinhan'
EXTERNAL_DIR = DATA_DIR / 'external'
OUTPUT_DIR = DATA_DIR / 'outputs'
SCHEMA_PATH = APP_ROOT / 'schemas' / 'actioncard.schema.json'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

DEFAULT_MONTHS = 8
SEED = 42
random.seed(SEED); np.random.seed(SEED)

_SCHEMA_CACHE = None
_SCHEMA_VALIDATOR = None


def tick():
    return perf_counter()


def to_ms(t0):
    return int((perf_counter() - t0) * 1000)


def _env_flag(name: str, default: str) -> str:
    value = os.getenv(name)
    return value if value is not None else default


USE_LLM = _env_flag("AGENT1_USE_LLM", "true").lower() not in {"0", "false", "no"}
DEBUG_MAX_PREVIEW = int(_env_flag("DEBUG_MAX_PREVIEW", "200") or 200)
DEBUG_SHOW_RAW = _env_flag("DEBUG_SHOW_RAW", "true").lower() in {"1", "true", "yes"}


def _mask_debug_preview(text: str | None, limit: int = DEBUG_MAX_PREVIEW) -> str:
    if not text:
        return ""
    masked = re.sub(r"\{[^{}]*\}", "{***}", str(text))
    masked = re.sub(r"([A-Za-z0-9]{4})[A-Za-z0-9]{4,}", r"\1***", masked)
    return masked[:limit]


def _normalize_str(value: str) -> str:
    if value is None:
        return ""
    text = unicodedata.normalize("NFKC", str(value))
    return re.sub(r"\s+", "", text)


def _normalize_compare(value: str | None) -> str:
    if value is None:
        return ""
    text = unicodedata.normalize("NFKC", str(value))
    text = re.sub(r"\s+", "", text)
    return text.upper()


def _wildcard_to_regex(masked: str | None) -> re.Pattern | None:
    if not masked:
        return None
    normalized = _normalize_str(masked)
    if not normalized:
        return None
    pattern = "".join(".*" if ch == "*" else re.escape(ch) for ch in normalized)
    try:
        return re.compile(f"^{pattern}")
    except re.error:
        return None

def load_actioncard_schema():
    global _SCHEMA_CACHE, _SCHEMA_VALIDATOR
    if _SCHEMA_CACHE is not None and _SCHEMA_VALIDATOR is not None:
        return _SCHEMA_CACHE, _SCHEMA_VALIDATOR
    if not SCHEMA_PATH.exists():
        raise FileNotFoundError(f"Ïä§ÌÇ§Îßà ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {SCHEMA_PATH}")
    with open(SCHEMA_PATH, 'r', encoding='utf-8') as f:
        _SCHEMA_CACHE = json.load(f)
    _SCHEMA_VALIDATOR = Draft7Validator(_SCHEMA_CACHE)
    return _SCHEMA_CACHE, _SCHEMA_VALIDATOR

def read_csv_smart(path):
    for enc in ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1']:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path, encoding='utf-8', errors='replace')


def normalize_rate_series(series: pd.Series | None) -> pd.Series | None:
    if series is None:
        return series
    cleaned = (
        series.astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("%", "", regex=False)
        .str.replace("‚àí", "-", regex=False)
    )
    cleaned = cleaned.str.replace(r"[^0-9\-.]", "", regex=True)
    numeric = pd.to_numeric(cleaned, errors='coerce')
    if numeric is None:
        return None
    scaled = numeric.copy()
    mask = scaled.notna() & (scaled.abs() <= 1)
    scaled.loc[mask] = scaled.loc[mask] * 100
    scaled.loc[(scaled < 0) | (scaled > 100)] = np.nan
    return scaled

def ym_to_date(ym_series):
    s = pd.to_datetime(ym_series.astype(str) + '01', format='%Y%m%d', errors='coerce')
    return s

def load_set1(shinhan_dir):
    p = shinhan_dir / 'big_data_set1_f.csv'
    df = read_csv_smart(p)
    ren = {}
    for c in df.columns:
        cu = str(c).upper()
        if cu == 'ENCODED_MCT': ren[c] = 'ENCODED_MCT'
        elif 'SIGUNGU' in cu:   ren[c] = 'SIGUNGU'
        elif 'BSE_AR' in cu:    ren[c] = 'ADDR_BASE'
        elif ('ZCD' in cu) or ('BZN' in cu) or ('ÏóÖÏ¢Ö' in cu): ren[c] = 'CATEGORY'
        elif cu == 'MCT_NM':    ren[c] = 'MCT_NM'
    df = df.rename(columns=ren)
    df = df.loc[:, ~df.columns.duplicated()]
    keep = ['ENCODED_MCT','MCT_NM','ADDR_BASE','SIGUNGU','CATEGORY']
    for k in keep:
        if k not in df.columns: df[k] = np.nan
    df = df[keep].drop_duplicates('ENCODED_MCT')
    if 'ENCODED_MCT' in df.columns:
        df['ENCODED_MCT'] = df['ENCODED_MCT'].apply(lambda v: str(v).strip() if pd.notna(v) else '')
    return df

def load_set2(shinhan_dir):
    p = shinhan_dir / 'big_data_set2_f.csv'
    df = read_csv_smart(p)
    df['TA_YM'] = df['TA_YM'].astype(str)
    df['_date'] = ym_to_date(df['TA_YM'])
    return df

def load_set3(shinhan_dir):
    p = shinhan_dir / 'big_data_set3_f.csv'
    df = read_csv_smart(p)
    df['TA_YM'] = df['TA_YM'].astype(str)
    df['_date'] = ym_to_date(df['TA_YM'])
    keep_cols = [
        'ENCODED_MCT','TA_YM','_date',
        'M12_MAL_1020_RAT','M12_MAL_30_RAT','M12_MAL_40_RAT','M12_MAL_50_RAT','M12_MAL_60_RAT',
        'M12_FME_1020_RAT','M12_FME_30_RAT','M12_FME_40_RAT','M12_FME_50_RAT','M12_FME_60_RAT',
        'MCT_UE_CLN_REU_RAT','MCT_UE_CLN_NEW_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT','RC_M1_SHC_WP_UE_CLN_RAT','RC_M1_SHC_FLP_UE_CLN_RAT',
        'APV_CE_RAT'
    ]
    for c in keep_cols:
        if c not in df.columns:
            df[c] = np.nan
    df = df[keep_cols]
    rate_cols = [
        'M12_MAL_1020_RAT','M12_MAL_30_RAT','M12_MAL_40_RAT','M12_MAL_50_RAT','M12_MAL_60_RAT',
        'M12_FME_1020_RAT','M12_FME_30_RAT','M12_FME_40_RAT','M12_FME_50_RAT','M12_FME_60_RAT',
        'MCT_UE_CLN_REU_RAT','MCT_UE_CLN_NEW_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT','RC_M1_SHC_WP_UE_CLN_RAT','RC_M1_SHC_FLP_UE_CLN_RAT'
    ]
    for col in rate_cols:
        if col in df.columns:
            original = df[col].copy()
            df[f'{col}_raw'] = original
            df[col] = normalize_rate_series(original)
    return df

def load_weather_monthly(external_dir):
    f = None
    for e in ('.csv','.parquet','.parq','.feather'):
        cand = list(external_dir.glob(f'**/*{e}'))
        if cand:
            f = cand[0]; break
    if not f:
        print('‚ö†Ô∏è Ïô∏Î∂Ä(ÎÇ†Ïî®) Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. ÎÇ†Ïî® Î∂ÑÏÑùÏùÄ Ï†úÌïúÎê©ÎãàÎã§.')
        return None
    if f.suffix.lower() == '.csv':
        wx = read_csv_smart(f)
    elif f.suffix.lower() in ('.parquet','.parq'):
        wx = pd.read_parquet(f)
    elif f.suffix.lower() == '.feather':
        wx = pd.read_feather(f)
    else:
        return None

    c_dt = None
    for c in wx.columns:
        cl = str(c).lower()
        if any(k in cl for k in ['date','ymd','dt','ÏùºÏûê','ÎÇ†Ïßú','yyyymm']):
            c_dt = c; break
    if c_dt is None:
        raise ValueError('ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞Ïóê ÎÇ†Ïßú(ÎòêÎäî YYYYMM) Ïª¨ÎüºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
    dt = pd.to_datetime(wx[c_dt].astype(str), errors='coerce')
    wx['_ym'] = dt.dt.strftime('%Y%m')
    c_rain = None
    for c in wx.columns:
        cl = c.lower()
        if any(k in cl for k in ['rain','precip','rn_mm','rainfall','rr','Í∞ïÏàò','Í∞ïÏàòÎüâ','ÎπÑ']):
            c_rain = c; break
    if c_rain is None:
        wx['_rain_val'] = 0.0
    else:
        wx['_rain_val'] = pd.to_numeric(wx[c_rain], errors='coerce').fillna(0.0)

    monthly = wx.groupby('_ym', as_index=False)['_rain_val'].sum().rename(columns={'_ym':'TA_YM','_rain_val':'RAIN_SUM'})
    monthly['TA_YM'] = monthly['TA_YM'].astype(str)
    monthly['_date'] = pd.to_datetime(monthly['TA_YM'] + '01', format='%Y%m%d', errors='coerce')
    return monthly[['TA_YM','_date','RAIN_SUM']]


def _format_percent_debug(value):
    if value is None:
        return None
    try:
        num = float(value)
    except (TypeError, ValueError):
        return None
    if num < 0 or num > 100:
        return None
    return round(num, 2)


def _format_percent_text(value):
    pct = _format_percent_debug(value)
    if pct is None:
        return '‚Äî'
    return f"{pct:.1f}%"


def _format_customer_mix_debug(detail):
    if not isinstance(detail, dict):
        return '‚Äî'
    ordered_labels = ['Ïú†Îèô', 'Í±∞Ï£º', 'ÏßÅÏû•']
    parts = []
    for label in ordered_labels:
        pct = _format_percent_text(detail.get(label))
        if pct != '‚Äî':
            parts.append(f"{label} {pct}")
    for label, value in detail.items():
        if label in ordered_labels:
            continue
        pct = _format_percent_text(value)
        if pct != '‚Äî':
            parts.append(f"{label} {pct}")
    return ', '.join(parts[:3]) if parts else '‚Äî'


def _format_age_segments_debug(segments):
    if not isinstance(segments, (list, tuple)):
        return '‚Äî'
    formatted = []
    for seg in segments:
        if not isinstance(seg, dict):
            continue
        label = seg.get('label') or seg.get('code')
        value = _format_percent_text(seg.get('value'))
        if label and value != '‚Äî':
            formatted.append(f"{label} {value}")
    return ', '.join(formatted[:3]) if formatted else '‚Äî'


def _build_debug_table(qinfo, merchant_match, sanitized_snapshot):
    industry_candidate = None
    if merchant_match:
        industry_candidate = merchant_match.get('category')
    if not industry_candidate:
        industry_candidate = qinfo.get('merchant_industry_label') or qinfo.get('industry')
    industry_labels = {
        'cafe': 'Ïπ¥Ìéò',
        'restaurant': 'ÏùåÏãùÏ†ê',
        'retail': 'ÏÜåÎß§',
    }
    industry = industry_labels.get(industry_candidate, industry_candidate or '‚Äî')

    address = '‚Äî'
    if merchant_match:
        addr = merchant_match.get('address')
        if isinstance(addr, (list, tuple)):
            addr = ' / '.join([str(v) for v in addr if v])
        if addr:
            address = str(addr)

    revisit = _format_percent_text((sanitized_snapshot or {}).get('revisit_pct'))
    new = _format_percent_text((sanitized_snapshot or {}).get('new_pct'))
    revisit_block = '‚Äî'
    if revisit != '‚Äî' or new != '‚Äî':
        revisit_block = f"Ïã†Í∑ú {new} / Ïû¨Î∞©Î¨∏ {revisit}"

    table = {
        'ÏóÖÏ¢Ö': industry,
        'Ï£ºÏÜå': address,
        'Ï£ºÏöî Í≥†Í∞ùÏ∏µ': _format_age_segments_debug((sanitized_snapshot or {}).get('age_top_segments')),
        'Í≥†Í∞ù Ïú†Ìòï': _format_customer_mix_debug((sanitized_snapshot or {}).get('customer_mix_detail')),
        'Ïã†Í∑ú/Ïû¨Î∞©Î¨∏': revisit_block,
        'Í∞ùÎã®Í∞Ä Íµ¨Í∞Ñ': (sanitized_snapshot or {}).get('avg_ticket_band_label') or '‚Äî',
    }
    return table

def build_panel(shinhan_dir, merchants_df=None, target_id=None):
    s1 = merchants_df if merchants_df is not None else load_set1(shinhan_dir)
    s2_all = load_set2(shinhan_dir)
    s3_all = load_set3(shinhan_dir)

    stats = {
        'set2_merchants_before': int(s2_all['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s2_all.columns else 0,
        'set3_merchants_before': int(s3_all['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s3_all.columns else 0,
    }

    s2 = s2_all
    s3 = s3_all
    if target_id:
        tid = str(target_id)
        s2 = s2_all[s2_all['ENCODED_MCT'].astype(str) == tid]
        s3 = s3_all[s3_all['ENCODED_MCT'].astype(str) == tid]

    stats['set2_rows_after'] = int(len(s2))
    stats['set3_rows_after'] = int(len(s3))

    m23 = pd.merge(s2, s3, on=['ENCODED_MCT','TA_YM','_date'], how='outer', suffixes=('_s2',''))
    panel = pd.merge(m23, s1, on='ENCODED_MCT', how='left')
    def nz(x):
        try:
            return pd.to_numeric(x, errors='coerce').fillna(0.0)
        except Exception:
            return pd.Series([0.0] * len(x))
    if 'M12_MAL_1020_RAT' in panel.columns and 'M12_FME_1020_RAT' in panel.columns:
        panel['YOUTH_SHARE'] = nz(panel['M12_MAL_1020_RAT']) + nz(panel['M12_FME_1020_RAT'])
    else:
        panel['YOUTH_SHARE'] = np.nan
    panel['REVISIT_RATE'] = pd.to_numeric(panel.get('MCT_UE_CLN_REU_RAT', np.nan), errors='coerce')
    panel['NEW_RATE'] = pd.to_numeric(panel.get('MCT_UE_CLN_NEW_RAT', np.nan), errors='coerce')
    panel.rename(columns={'ENCODED_MCT':'_merchant_id'}, inplace=True)
    if '_merchant_id' in panel.columns:
        panel['_merchant_id'] = panel['_merchant_id'].astype(str)
    stats['merchants_after'] = int(panel['_merchant_id'].nunique()) if '_merchant_id' in panel.columns else 0
    stats['set2_merchants_after'] = int(s2['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s2.columns else 0
    stats['set3_merchants_after'] = int(s3['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s3.columns else 0
    return panel, stats


def call_llm_for_mask(original_question: str | None, merchant_mask: str | None, sigungu: str | None):
    meta = {
        'used': False,
        'model': 'models/gemini-2.5-flash',
        'prompt_preview': '',
        'resp_bytes': 0,
        'safety_blocked': False,
        'elapsed_ms': 0,
        'error': None,
    }

    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print('‚ö†Ô∏è GEMINI_API_KEY ÎØ∏ÏÑ§Ï†ïÏúºÎ°ú LLM Î≥¥Ï°∞ Îß§Ïπ≠ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.')
        meta['error'] = 'missing_api_key'
        return None, meta
    try:
        import google.generativeai as genai
    except ImportError:
        print('‚ö†Ô∏è google-generativeai ÎØ∏ÏÑ§ÏπòÎ°ú LLM Î≥¥Ï°∞ Îß§Ïπ≠ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.')
        meta['error'] = 'missing_dependency'
        return None, meta

    genai.configure(api_key=api_key)
    model_name = meta['model']
    model = genai.GenerativeModel(
        model_name=model_name,
        generation_config={
            'temperature': 0.1,
            'top_p': 0.8,
            'max_output_tokens': 128,
        },
    )

    prompt = f"""ÎãπÏã†ÏùÄ ÌÖçÏä§Ìä∏ÏóêÏÑú ÎßàÏä§ÌÇπÎêú Í∞ÄÎßπÏ†ê Îã®ÏÑúÎ•º Ï†ïÎ¶¨ÌïòÎäî ÌïúÍµ≠Ïñ¥ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.
ÏßàÎ¨∏ÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìïú ÏÉÅÌò∏ ÎßàÏä§ÌÅ¨ÏôÄ ÏãúÍµ∞Íµ¨Îßå JSONÏúºÎ°ú Îã§Ïãú Ïç®Ï£ºÏÑ∏Ïöî.
Ï∂îÏ†ïÏù¥ÎÇò ÏÉùÏÑ±ÏùÄ Í∏àÏßÄÌïòÎ©∞, Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ nullÏùÑ ÎÑ£ÏäµÎãàÎã§.

ÏßàÎ¨∏: {original_question}
ÌòÑÏû¨ Ï∂îÏ∂ú: merchant_mask={merchant_mask}, sigungu={sigungu}

JSON ÌòïÏãù:
{{"merchant_mask":"Î¨∏ÏûêÏó¥ ÎòêÎäî null","sigungu":"Î¨∏ÏûêÏó¥ ÎòêÎäî null","notes":"Í∞ÑÎã® Î©îÎ™®"}}
"""

    meta['prompt_preview'] = _mask_debug_preview(prompt)
    t0 = tick()

    try:
        response = model.generate_content(prompt)
    except Exception as exc:
        meta['elapsed_ms'] = to_ms(t0)
        meta['error'] = str(exc)
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ Ìò∏Ï∂ú Ïã§Ìå®:', exc)
        return None, meta

    def _response_text(resp):
        parts = []
        for part in getattr(resp, 'parts', []) or []:
            text = getattr(part, 'text', None)
            if text:
                parts.append(text)
        if hasattr(resp, 'text') and resp.text:
            parts.append(resp.text)
        return '\n'.join(parts)

    text = _response_text(response)
    meta['elapsed_ms'] = to_ms(t0)

    prompt_feedback = getattr(response, 'prompt_feedback', None)
    block_reason = None
    if prompt_feedback is not None:
        block_reason = getattr(prompt_feedback, 'block_reason', None)
    meta['safety_blocked'] = bool(block_reason and str(block_reason).lower() != 'block_none')

    if not text:
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ ÏùëÎãµÏù¥ ÎπÑÏóàÏäµÎãàÎã§.')
        meta['used'] = True
        meta['error'] = 'empty_text'
        return None, meta

    meta['used'] = True
    meta['resp_bytes'] = len(text.encode('utf-8'))

    match = re.search(r'\{[\s\S]*\}', text)
    if not match:
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ÏóêÏÑú JSONÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        meta['error'] = 'json_not_found'
        return None, meta

    try:
        data = json.loads(match.group(0))
    except Exception as exc:
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ JSON ÌååÏã± Ïã§Ìå®:', exc)
        meta['error'] = f'json_parse_error: {exc}'
        return None, meta

    return (data if isinstance(data, dict) else None), meta


def resolve_merchant(
    masked_name: str | None,
    mask_prefix: str | None,
    sigungu: str | None,
    merchants_df: pd.DataFrame | None,
    original_question: str | None = None,
    allow_llm: bool = True,
):
    debug_info = {
        'candidates': [],
        'path': None,
        'notes': None,
        'suggestions': None,
        'llm': None,
    }

    if merchants_df is None or merchants_df.empty:
        return None, debug_info

    if not masked_name:
        debug_info['notes'] = 'ÎßàÏä§ÌÇπ ÏÉÅÌò∏ ÎØ∏Ï†úÍ≥µ'
        print("‚ö†Ô∏è resolve_merchant: ÏûÖÎ†•Îêú ÎßàÏä§ÌÇπ ÏÉÅÌò∏Í∞Ä ÏóÜÏñ¥ Îß§Ïπ≠ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.")
        return None, debug_info

    df = merchants_df.copy()
    df['_norm_name'] = df['MCT_NM'].apply(_normalize_compare)
    df['_norm_sigungu'] = df['SIGUNGU'].apply(_normalize_compare)
    df['_norm_category'] = df['CATEGORY'].apply(_normalize_compare)

    norm_sigungu = _normalize_compare(sigungu)
    prefix_norm = _normalize_compare(mask_prefix) if mask_prefix else ''

    base = df
    if norm_sigungu:
        exact = base[base['_norm_sigungu'] == norm_sigungu]
        if exact.empty:
            base = base[base['_norm_sigungu'].str.contains(norm_sigungu, na=False)]
        else:
            base = exact
    sigungu_filter_count = int(len(base))

    def _preview_candidates(frame: pd.DataFrame) -> list[dict]:
        preview = []
        for _, row in frame.head(3).iterrows():
            preview.append({
                'ENCODED_MCT': row['ENCODED_MCT'],
                'MCT_NM': row['MCT_NM'],
                'SIGUNGU': row['SIGUNGU'],
                'CATEGORY': row['CATEGORY'],
                'score': float(row.get('__score')) if '__score' in row else None,
            })
        return preview

    if base.empty:
        debug_payload = {
            'input': {'masked_name': masked_name, 'mask_prefix': mask_prefix, 'sigungu': sigungu},
            'sigungu_filter_count': sigungu_filter_count,
            'rule': 'rule1',
            'candidates': [],
        }
        print("üß≠ resolve_phase:", json.dumps(debug_payload, ensure_ascii=False))
        print(f"‚ö†Ô∏è Í∞ÄÎßπÏ†ê ÎØ∏ÏùºÏπò ‚Äì {masked_name}¬∑{sigungu}Î•º ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.")
        debug_info['notes'] = 'sigungu_filter_empty'
        return None, debug_info

    # Rule-1 strict: startswith
    if prefix_norm:
        rule1 = base[base['_norm_name'].str.startswith(prefix_norm, na=False)]
    else:
        rule1 = base.iloc[0:0]
    rule1_count = int(len(rule1))

    if rule1_count == 1:
        row = rule1.iloc[0]
        resolved = {
            'encoded_mct': str(row['ENCODED_MCT']),
            'masked_name': row.get('MCT_NM'),
            'address': row.get('ADDR_BASE'),
            'sigungu': row.get('SIGUNGU'),
            'category': row.get('CATEGORY'),
            'score': 1.0,
        }
        debug_info['candidates'] = _preview_candidates(rule1.assign(__score=1.0))
        debug_info['path'] = 'rule1'
        print(
            "üß≠ resolve_phase:",
            json.dumps({
                'input': {'masked_name': masked_name, 'mask_prefix': mask_prefix, 'sigungu': sigungu},
                'rule': 'rule1',
                'sigungu_filter_count': sigungu_filter_count,
                'rule1_count': rule1_count,
                'candidates': debug_info['candidates'],
            }, ensure_ascii=False),
        )
        print("‚úÖ resolved_merchant_id:", resolved['encoded_mct'])
        return resolved, debug_info

    def _score_rows(frame: pd.DataFrame) -> pd.DataFrame:
        scored = frame.copy()
        scores = []
        for _, r in scored.iterrows():
            name_norm = r['_norm_name'] or ''
            base_score = 0.0
            if prefix_norm:
                if name_norm.startswith(prefix_norm):
                    base_score = 1.0
                elif prefix_norm in name_norm:
                    base_score = 0.8
                else:
                    base_score = SequenceMatcher(None, prefix_norm, name_norm).ratio()
            fuzzy = SequenceMatcher(None, prefix_norm, name_norm).ratio() if prefix_norm else 0.0
            base_val = max(base_score, fuzzy)
            length_bonus = 0.05 if prefix_norm and len(name_norm) > len(prefix_norm) else 0.0
            scores.append(round(min(base_val + length_bonus, 1.05), 4))
        scored['__score'] = scores
        scored['__name_len'] = scored['_norm_name'].str.len().fillna(0)
        return scored

    # Rule-2 fallback if strict fails
    rule2_base = base if rule1_count == 0 else rule1
    rule2 = _score_rows(rule2_base)
    top = rule2.sort_values(['__score', '__name_len', 'ENCODED_MCT'], ascending=[False, False, True])
    debug_candidates = _preview_candidates(top)
    debug_info['candidates'] = debug_candidates

    chosen = None
    path = 'rule2' if rule1_count == 0 else 'rule1'
    if not top.empty and float(top.iloc[0]['__score']) >= 0.85:
        chosen = top.iloc[0]
        debug_info['path'] = path
    elif not top.empty and rule1_count > 1:
        chosen = top.iloc[0]
        debug_info['path'] = 'rule1'

    print(
        "üß≠ resolve_phase:",
        json.dumps({
            'input': {'masked_name': masked_name, 'mask_prefix': mask_prefix, 'sigungu': sigungu},
            'rule': path,
            'sigungu_filter_count': sigungu_filter_count,
            'rule1_count': rule1_count,
            'candidates': debug_candidates,
        }, ensure_ascii=False),
    )

    if chosen is not None:
        resolved = {
            'encoded_mct': str(chosen['ENCODED_MCT']),
            'masked_name': chosen.get('MCT_NM'),
            'address': chosen.get('ADDR_BASE'),
            'sigungu': chosen.get('SIGUNGU'),
            'category': chosen.get('CATEGORY'),
            'score': float(chosen.get('__score')) if pd.notna(chosen.get('__score')) else None,
        }
        print("‚úÖ resolved_merchant_id:", resolved['encoded_mct'])
        return resolved, debug_info

    # Rule-2 failed ‚Üí optional LLM assist
    if allow_llm:
        llm_result, llm_meta = call_llm_for_mask(original_question, masked_name, sigungu)
        debug_info['notes'] = 'llm_invoked'
        if llm_meta:
            debug_info['llm'] = {'parsed': llm_result, **llm_meta}
        if llm_result:
            new_mask = llm_result.get('merchant_mask') or masked_name
            new_prefix = (new_mask.split('*', 1)[0].strip() if new_mask else mask_prefix)
            new_sigungu = llm_result.get('sigungu') or sigungu
            if (new_mask, new_sigungu) != (masked_name, sigungu):
                match, nested_debug = resolve_merchant(
                    new_mask,
                    new_prefix,
                    new_sigungu,
                    merchants_df,
                    original_question=original_question,
                    allow_llm=False,
                )
                if isinstance(nested_debug, dict):
                    if llm_meta:
                        nested_debug.setdefault('llm', {'parsed': llm_result, **llm_meta})
                    nested_debug['notes'] = nested_debug.get('notes') or 'llm_invoked'
                    if not nested_debug.get('path'):
                        nested_debug['path'] = 'llm'
                return match, nested_debug

    # No match ‚Üí surface suggestions
    base_scores = []
    if prefix_norm:
        for _, row in base.iterrows():
            ratio = SequenceMatcher(None, prefix_norm, row['_norm_name'] or '').ratio()
            base_scores.append((ratio, row))
        base_scores.sort(key=lambda x: x[0], reverse=True)
        suggestions = [
            {
                'ENCODED_MCT': r['ENCODED_MCT'],
                'MCT_NM': r['MCT_NM'],
                'SIGUNGU': r['SIGUNGU'],
                'CATEGORY': r['CATEGORY'],
            }
            for ratio, r in base_scores[:3]
            if ratio > 0
        ]
    else:
        suggestions = []

    debug_info['suggestions'] = suggestions
    print(f"‚ö†Ô∏è Í∞ÄÎßπÏ†ê ÎØ∏ÏùºÏπò ‚Äì {masked_name}¬∑{sigungu}Î•º ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.")
    if suggestions:
        print("üîç Ïú†ÏÇ¨ ÌõÑÎ≥¥:", json.dumps(suggestions, ensure_ascii=False))
    return None, debug_info

def parse_question(q):
    original = q or ''
    normalized = unicodedata.normalize('NFKC', original)
    normalized = re.sub(r"\s+", " ", normalized).strip()
    lower_q = normalized.lower()
    age_cond = None
    if '10ÎåÄ' in original or 'teen' in lower_q or re.search(r'\b1[0-9]\b', lower_q):
        age_cond = ('<=', 19)
    if '20ÎåÄ' in original or '20s' in lower_q or 'twenties' in lower_q:
        if ('Ïù¥Ìïò' in original) or ('under' in lower_q) or ('<=' in lower_q):
            age_cond = ('<=', 20)
        else:
            age_cond = ('range', (20,29))
    if 'Ï≤≠ÏÜåÎÖÑ' in original:
        age_cond = ('<=', 19)

    weather = None
    if ('ÎπÑ' in original) or ('Ïö∞Ï≤ú' in original) or ('rain' in lower_q):
        weather = 'rain'
    elif ('Îßë' in original) or ('sunny' in lower_q) or ('clear' in lower_q):
        weather = 'clear'
    elif ('Îàà' in original) or ('snow' in lower_q):
        weather = 'snow'

    months = DEFAULT_MONTHS
    weeks_requested = None
    week_match = re.search(r'(\d+)\s*Ï£º', original)
    if week_match:
        try:
            weeks_requested = int(week_match.group(1))
        except ValueError:
            weeks_requested = None
        if weeks_requested and weeks_requested > 0:
            months = max(1, round(weeks_requested / 4))
    if 'Ïù¥Î≤àÎã¨' in original or 'this month' in lower_q:
        months = 1
    elif ('ÌïúÎã¨' in original) or ('1Îã¨' in original) or ('month' in lower_q):
        months = 1
    elif 'Î∂ÑÍ∏∞' in original or 'quarter' in lower_q:
        months = 3

    industry = None
    if ('Ïπ¥Ìéò' in original) or ('Ïª§Ìîº' in original):
        industry = 'cafe'
    elif ('ÏöîÏãù' in original) or ('restaurant' in lower_q) or ('ÏãùÎãπ' in original):
        industry = 'restaurant'

    merchant_mask = None
    pattern_used = 'none'
    brace_match = re.search(r'\{([^{}]+)\}', normalized)
    if brace_match:
        merchant_mask = brace_match.group(1).strip()
        pattern_used = 'curly_brace'

    mask_prefix = None
    if merchant_mask:
        mask_prefix = merchant_mask.split('*', 1)[0].strip()

    sigungu_pattern = 'hangul_gu_regex'
    sigungu_match = re.search(r'(?P<sigungu>[Í∞Ä-Ìû£]{2,}Íµ¨)', normalized)
    if sigungu_match:
        merchant_sigungu = sigungu_match.group('sigungu')
    else:
        merchant_sigungu = 'ÏÑ±ÎèôÍµ¨'
        sigungu_pattern = 'default_sigungu'

    merchant_info = {
        'masked_name': merchant_mask,
        'mask_prefix': mask_prefix,
        'sigungu': merchant_sigungu,
        'industry_label': None,
    }

    explicit_id = None
    trimmed = original.strip()
    if re.fullmatch(r'[A-Z0-9]{10,12}', trimmed):
        explicit_id = trimmed

    return {
        'original_question': original,
        'age_cond': age_cond,
        'weather': weather,
        'months': months,
        'weeks_requested': weeks_requested,
        'industry': industry,
        'normalized_question': normalized,
        'merchant_masked_name': merchant_info['masked_name'],
        'merchant_mask_prefix': merchant_info['mask_prefix'],
        'merchant_sigungu': merchant_info['sigungu'],
        'merchant_industry_label': merchant_info['industry_label'],
        'merchant_explicit_id': explicit_id,
        'merchant_pattern_used': pattern_used,
        'merchant_sigungu_pattern': sigungu_pattern,
    }

def subset_period(panel, months=DEFAULT_MONTHS):
    if panel['_date'].isna().all():
        return panel.iloc[0:0]
    maxd = panel['_date'].max()
    thr = maxd - pd.Timedelta(days=31*months)
    return panel[panel['_date'] >= thr]

def kpi_summary(panel_sub):
    if panel_sub.empty:
        return {}, {'latest_raw_snapshot': None, 'sanitized_snapshot': None}
    latest_idx = panel_sub.groupby('_merchant_id')['_date'].idxmax()
    snap = panel_sub.loc[latest_idx].sort_values('_date', ascending=False)

    def _safe_float(val):
        try:
            f = float(val)
        except (TypeError, ValueError):
            return None
        return f

    def _clean_pct(val):
        if val is None or pd.isna(val):
            return None
        try:
            f = float(val)
        except (TypeError, ValueError):
            return None
        if f < 0 or f > 100:
            return None
        return round(f, 1)

    detail_row = snap.iloc[0]
    youth_latest = _safe_float(detail_row.get('YOUTH_SHARE'))
    revisit_latest = _safe_float(detail_row.get('REVISIT_RATE'))
    new_latest = _safe_float(detail_row.get('NEW_RATE'))

    age_labels = {
        '1020': 'Ï≤≠ÎÖÑ(10-20)',
        '30': '30ÎåÄ',
        '40': '40ÎåÄ',
        '50': '50ÎåÄ',
        '60': '60ÎåÄ',
    }
    age_distribution = []
    for code, label in age_labels.items():
        cols = [f'M12_MAL_{code}_RAT', f'M12_FME_{code}_RAT']
        vals = pd.to_numeric(pd.Series([detail_row.get(c) for c in cols]), errors='coerce')
        total = vals.sum(skipna=True)
        if pd.notna(total):
            cleaned = _clean_pct(total)
            if cleaned is not None:
                age_distribution.append({'code': code, 'label': label, 'value': cleaned})
    age_distribution.sort(key=lambda x: x['value'], reverse=True)

    customer_mix_map = [
        ('Ïú†Îèô', 'RC_M1_SHC_FLP_UE_CLN_RAT'),
        ('Í±∞Ï£º', 'RC_M1_SHC_RSD_UE_CLN_RAT'),
        ('ÏßÅÏû•', 'RC_M1_SHC_WP_UE_CLN_RAT'),
    ]
    customer_mix_detail = {}
    for label, col in customer_mix_map:
        customer_mix_detail[label] = _clean_pct(_safe_float(detail_row.get(col)))

    ticket_band_raw = detail_row.get('APV_CE_RAT')
    ticket_band = None
    if isinstance(ticket_band_raw, str):
        parts = ticket_band_raw.split('_', 1)
        ticket_band = parts[-1].strip() if parts else ticket_band_raw.strip()
    elif pd.notna(ticket_band_raw):
        ticket_band = str(ticket_band_raw)

    sanitized = {
        'youth_share_avg': _clean_pct(youth_latest),
        'revisit_rate_avg': _clean_pct(revisit_latest),
        'new_rate_avg': _clean_pct(new_latest),
        'age_distribution': age_distribution,
        'age_top_segments': age_distribution[:3],
        'customer_mix_detail': customer_mix_detail,
        'avg_ticket_band_label': ticket_band,
        'n_merchants': int(snap['_merchant_id'].nunique()),
    }

    raw_snapshot = {
        'TA_YM': detail_row.get('TA_YM'),
        '_date': str(detail_row.get('_date')),
        'MCT_UE_CLN_REU_RAT_raw': detail_row.get('MCT_UE_CLN_REU_RAT_raw'),
        'MCT_UE_CLN_NEW_RAT_raw': detail_row.get('MCT_UE_CLN_NEW_RAT_raw'),
        'M12_MAL_1020_RAT_raw': detail_row.get('M12_MAL_1020_RAT_raw'),
        'M12_FME_1020_RAT_raw': detail_row.get('M12_FME_1020_RAT_raw'),
        'RC_M1_SHC_FLP_UE_CLN_RAT_raw': detail_row.get('RC_M1_SHC_FLP_UE_CLN_RAT_raw'),
        'RC_M1_SHC_RSD_UE_CLN_RAT_raw': detail_row.get('RC_M1_SHC_RSD_UE_CLN_RAT_raw'),
        'RC_M1_SHC_WP_UE_CLN_RAT_raw': detail_row.get('RC_M1_SHC_WP_UE_CLN_RAT_raw'),
    }

    sanitized_snapshot = {
        'revisit_pct': sanitized['revisit_rate_avg'],
        'new_pct': sanitized['new_rate_avg'],
        'youth_pct': sanitized['youth_share_avg'],
        'customer_mix_detail': sanitized['customer_mix_detail'],
        'age_top_segments': sanitized['age_top_segments'],
        'avg_ticket_band_label': sanitized['avg_ticket_band_label'],
    }

    print("üóÇ KPI raw snapshot:", json.dumps(raw_snapshot, ensure_ascii=False))
    print("‚úÖ KPI sanitized:", json.dumps(sanitized_snapshot, ensure_ascii=False))

    return sanitized, {'latest_raw_snapshot': raw_snapshot, 'sanitized_snapshot': sanitized_snapshot}


    sanitized_snapshot = {
        'revisit_pct': sanitized['revisit_rate_avg'],
        'new_pct': sanitized['new_rate_avg'],
        'youth_pct': sanitized['youth_share_avg'],
        'customer_mix_detail': sanitized['customer_mix_detail'],
        'age_top_segments': sanitized['age_top_segments'],
        'avg_ticket_band_label': sanitized['avg_ticket_band_label'],
    }

    print("üóÇ KPI raw snapshot:", json.dumps(raw_snapshot, ensure_ascii=False))
    print("‚úÖ KPI sanitized:", json.dumps(sanitized_snapshot, ensure_ascii=False))

    return sanitized, {'latest_raw_snapshot': raw_snapshot, 'sanitized_snapshot': sanitized_snapshot}


def weather_effect(panel_sub, wx_monthly):
    if (wx_monthly is None) or panel_sub.empty or ('REVISIT_RATE' not in panel_sub):
        return {'metric':'REVISIT_RATE','effect':None,'ci':[None,None],'note':'ÎÇ†Ïî®/ÌëúÎ≥∏ Î∂ÄÏ°±'}
    m = panel_sub.groupby('TA_YM', as_index=False)['REVISIT_RATE'].mean()
    m = m.merge(wx_monthly[['TA_YM','RAIN_SUM']], on='TA_YM', how='inner')
    if m.empty or m['RAIN_SUM'].nunique() < 2:
        return {'metric':'REVISIT_RATE','effect':None,'ci':[None,None],'note':'ÏÉÅÍ¥Ä Ï∂îÏ†ï Î∂àÍ∞Ä'}
    corr = m['REVISIT_RATE'].corr(m['RAIN_SUM'])
    return {'metric':'REVISIT_RATE','effect':float(corr), 'ci':[None,None], 'note':'ÌîºÏñ¥Ïä® ÏÉÅÍ¥Ä(ÏõîÎã®ÏúÑ)'}

def agent1_pipeline(question, shinhan_dir=SHINHAN_DIR, external_dir=EXTERNAL_DIR):
    debug_block = {
        'input': {
            'original': _mask_debug_preview(question, limit=120),
            'flags': {
                'USE_LLM': USE_LLM,
                'DEBUG_MAX_PREVIEW': DEBUG_MAX_PREVIEW,
                'DEBUG_SHOW_RAW': DEBUG_SHOW_RAW,
            },
        },
        'errors': [],
    }

    merchants_df = load_set1(shinhan_dir)

    parse_t0 = tick()
    try:
        qinfo = parse_question(question)
    except Exception as exc:
        debug_block['errors'].append({'stage': 'parse', 'msg': str(exc)})
        debug_block['parse'] = {'elapsed_ms': to_ms(parse_t0)}
        raise
    parse_elapsed = to_ms(parse_t0)
    debug_block['parse'] = {
        'merchant_mask': qinfo.get('merchant_masked_name'),
        'mask_prefix': qinfo.get('merchant_mask_prefix'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'pattern_used': qinfo.get('merchant_pattern_used'),
        'elapsed_ms': parse_elapsed,
    }

    run_id = datetime.datetime.utcnow().isoformat()
    parse_log = {
        'original': qinfo.get('original_question'),
        'merchant_mask': qinfo.get('merchant_masked_name'),
        'mask_prefix': qinfo.get('merchant_mask_prefix'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'explicit_id': qinfo.get('merchant_explicit_id'),
    }
    print("üÜî agent1_run:", run_id)
    print("üßæ question_fields:", json.dumps(parse_log, ensure_ascii=False))

    merchant_match = None
    resolve_meta = {
        'candidates': [],
        'path': None,
        'notes': None,
        'suggestions': None,
        'llm': None,
    }

    resolve_stage = {
        'path': 'none',
        'candidates_top3': [],
        'resolved_merchant_id': None,
    }

    resolve_t0 = tick()
    try:
        explicit_id = qinfo.get('merchant_explicit_id')
        if explicit_id:
            lookup = merchants_df[merchants_df['ENCODED_MCT'] == explicit_id]
            print(
                "üè∑ explicit_id_lookup:",
                json.dumps({'explicit_id': explicit_id, 'row_count': int(len(lookup))}, ensure_ascii=False),
            )
            if not lookup.empty:
                row = lookup.iloc[0]
                merchant_match = {
                    'encoded_mct': str(row['ENCODED_MCT']),
                    'masked_name': row.get('MCT_NM'),
                    'address': row.get('ADDR_BASE'),
                    'sigungu': row.get('SIGUNGU'),
                    'category': row.get('CATEGORY'),
                    'score': None,
                }
                resolve_meta['path'] = 'user'

        if merchant_match is None:
            merchant_match, resolve_meta = resolve_merchant(
                qinfo.get('merchant_masked_name'),
                qinfo.get('merchant_mask_prefix'),
                qinfo.get('merchant_sigungu'),
                merchants_df,
                original_question=qinfo.get('normalized_question') or question,
                allow_llm=USE_LLM,
            )
    except Exception as exc:
        debug_block['errors'].append({'stage': 'resolve', 'msg': str(exc)})
        raise
    finally:
        resolve_stage['elapsed_ms'] = to_ms(resolve_t0)

    target_id = None
    if merchant_match and merchant_match.get('encoded_mct') is not None:
        target_id = str(merchant_match['encoded_mct'])
        merchant_match['encoded_mct'] = target_id

    if resolve_meta.get('path') is None:
        resolve_meta['path'] = 'llm' if resolve_meta.get('llm') else 'none'
    resolve_stage['path'] = resolve_meta.get('path') or 'none'
    resolve_stage['resolved_merchant_id'] = target_id

    candidate_payload = []
    for cand in resolve_meta.get('candidates') or []:
        cid = cand.get('ENCODED_MCT') or cand.get('encoded_mct')
        try:
            score_val = cand.get('score')
            score = round(float(score_val), 4) if score_val is not None else None
        except (TypeError, ValueError):
            score = None
        candidate_payload.append({
            'id': str(cid) if cid is not None else None,
            'name': cand.get('MCT_NM') or cand.get('masked_name'),
            'sigungu': cand.get('SIGUNGU') or cand.get('sigungu'),
            'score': score,
        })
    resolve_stage['candidates_top3'] = candidate_payload[:3]
    debug_block['resolve'] = resolve_stage

    llm_meta = resolve_meta.get('llm') or {}
    agent1_llm = {
        'used': bool(llm_meta.get('used')),
        'model': llm_meta.get('model'),
        'prompt_preview': llm_meta.get('prompt_preview', ''),
        'resp_bytes': llm_meta.get('resp_bytes'),
        'safety_blocked': bool(llm_meta.get('safety_blocked')),
        'elapsed_ms': llm_meta.get('elapsed_ms'),
    }
    debug_block['agent1_llm'] = agent1_llm

    panel_stage = {}
    panel_t0 = tick()
    try:
        panel, panel_stats = build_panel(shinhan_dir, merchants_df=merchants_df, target_id=target_id)
    except Exception as exc:
        panel_stage['elapsed_ms'] = to_ms(panel_t0)
        debug_block['errors'].append({'stage': 'panel', 'msg': str(exc)})
        debug_block['panel'] = panel_stage
        raise
    panel_elapsed = to_ms(panel_t0)
    sub = subset_period(panel, months=qinfo['months'])
    panel_stage.update({
        'rows_before': int(len(panel)),
        'rows_after': int(len(sub)),
        'latest_ta_ym': str(sub['TA_YM'].max()) if not sub.empty and 'TA_YM' in sub.columns else None,
        'elapsed_ms': panel_elapsed,
        'stats': panel_stats,
    })
    debug_block['panel'] = panel_stage

    wxm = None
    try:
        wxm = load_weather_monthly(external_dir)
    except Exception as exc:
        debug_block['errors'].append({'stage': 'weather', 'msg': str(exc)})
        wxm = None

    snapshot_t0 = tick()
    kpis, kpi_debug = kpi_summary(sub)
    snapshot_elapsed = to_ms(snapshot_t0)

    raw_snapshot = {}
    raw_source = (kpi_debug or {}).get('latest_raw_snapshot') or {}
    for key, value in raw_source.items():
        if key.endswith('_raw'):
            raw_snapshot[key[:-4]] = value
        else:
            raw_snapshot[key] = value
    sanitized_snapshot = (kpi_debug or {}).get('sanitized_snapshot') or {}
    debug_block['snapshot'] = {
        'raw': raw_snapshot,
        'sanitized': sanitized_snapshot,
        'elapsed_ms': snapshot_elapsed,
    }

    render_table = _build_debug_table(qinfo, merchant_match, sanitized_snapshot)
    debug_block['render'] = {
        'table_dict': render_table,
    }

    wfx = weather_effect(sub, wxm)

    notes = []
    quality = 'normal'
    if sub.empty:
        notes.append('ÏßàÎ¨∏ Ï°∞Í±¥ ÌëúÎ≥∏ Î∂ÄÏ°± ÎòêÎäî Í∏∞Í∞Ñ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå')
        quality = 'low'
    if wxm is None and qinfo['weather'] is not None:
        notes.append('ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞ Î∂ÄÏû¨: ÎÇ†Ïî® Í¥ÄÎ†® Ìö®Í≥ºÎäî Ï∂îÏ†ïÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        quality = 'low'
    if qinfo.get('merchant_masked_name') is None:
        notes.append('{ÏÉÅÌò∏} ÌòïÌÉúÏùò ÏûÖÎ†•Ïù¥ ÏóÜÏñ¥ Í∞ÄÎßπÏ†ê ÏãùÎ≥ÑÏùÑ ÏßÑÌñâÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        quality = 'low'
    if merchant_match is None:
        notes.append('ÏßàÎ¨∏Í≥º ÏùºÏπòÌïòÎäî Í∞ÄÎßπÏ†êÏùÑ Ï∞æÏßÄ Î™ªÌï¥ Ï†ÑÏ≤¥ ÌëúÎ≥∏ÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§.')
        quality = 'low'

    merchant_query = {
        'masked_name': qinfo.get('merchant_masked_name'),
        'mask_prefix': qinfo.get('merchant_mask_prefix'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'industry_label': qinfo.get('merchant_industry_label'),
    }

    merchants_covered = int(sub['_merchant_id'].nunique()) if not sub.empty else 0

    out = {
        'context': {
            'intent': question,
            'parsed': qinfo,
            'merchant_query': merchant_query,
            'run_id': run_id,
            'panel_stats': panel_stage.get('stats', {}),
            'merchant_candidates': resolve_meta.get('candidates'),
            'merchant_resolution_path': resolve_stage['path'],
        },
        'kpis': kpis,
        'weather_effect': wfx,
        'limits': notes,
        'quality': quality,
        'period': {
            'max_date': str(panel['_date'].max() if '_date' in panel.columns else None),
            'months': qinfo['months'],
            'weeks_requested': qinfo.get('weeks_requested'),
        },
        'sample': {
            'merchants_covered': merchants_covered
        },
        'debug': debug_block,
    }

    if merchant_match:
        out['context']['merchant'] = merchant_match
        out['context']['merchant_masked_name'] = merchant_match.get('masked_name')

    out_path = OUTPUT_DIR / 'agent1_output.json'
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print('‚úÖ Agent-1 JSON Ï†ÄÏû•:', out_path)
    return out

def build_agent2_prompt(agent1_json):
    try:
        schema, _ = load_actioncard_schema()
        schema_text = json.dumps(schema, ensure_ascii=False, indent=2)
    except Exception as e:
        schema_text = json.dumps({"schema_error": str(e)}, ensure_ascii=False, indent=2)

    rules = [
        "Agent-1 JSONÎßå Í∑ºÍ±∞Î°ú ÌôúÏö©ÌïòÍ≥† Ïô∏Î∂Ä Ï∂îÏ†ïÏùÄ Í∏àÏßÄÌï©ÎãàÎã§.",
        "Î™®Îì† Ïπ¥ÎìúÏóê ÌÉÄÍ≤ü ‚Üí Ï±ÑÎÑê ‚Üí Î∞©Î≤ï ‚Üí Ïπ¥Ìîº(2Í∞ú Ïù¥ÏÉÅ) ‚Üí KPI ‚Üí Î¶¨Ïä§ÌÅ¨/ÏôÑÌôî ‚Üí Í∑ºÍ±∞Î•º Ï±ÑÏõÅÎãàÎã§.",
        "Í∑ºÍ±∞ Î¨∏Ïû•ÏùÄ Î∞òÎìúÏãú Ïà´Ïûê+Ïª¨ÎüºÎ™Ö+Í∏∞Í∞Ñ ÌòïÏãùÏù¥Î©∞ Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ null ÎòêÎäî '‚Äî'Î°ú Îë°ÎãàÎã§.",
        "ÌíàÏßàÏù¥ ÎÇÆÍ±∞ÎÇò Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±ÌïòÎ©¥ ÎßàÏßÄÎßâ Ïπ¥ÎìúÏóê 'Îç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï Ï†úÏïà'ÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.",
        "ÏÉÅÌò∏Î™ÖÏùÄ Ìï≠ÏÉÅ ÎßàÏä§ÌÇπÎêú ÌòïÌÉúÎ°ú Ïú†ÏßÄÌï©ÎãàÎã§.",
        "KPI.expected_upliftÏôÄ rangeÎäî Í∑ºÍ±∞Í∞Ä ÏûàÏùÑ ÎïåÎßå Í∞íÏùÑ ÎÑ£Í≥†, ÏóÜÏúºÎ©¥ nullÏùÑ Ïú†ÏßÄÌï©ÎãàÎã§."
    ]

    rules_text = "- " + "\n- ".join(rules)

    guide = f"""ÎãπÏã†ÏùÄ ÌïúÍµ≠Ïñ¥ ÏÜåÏÉÅÍ≥µÏù∏ Ïª®ÏÑ§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÏïÑÎûò Agent-1 JSONÎßå Í∑ºÍ±∞Î°ú ÏÇ¨Ïö©ÌïòÏó¨,
Î∞òÎìúÏãú Ïï°ÏÖòÏπ¥Îìú Ïä§ÌÇ§Îßà(JSON)Î°úÎßå ÎãµÌïòÏÑ∏Ïöî. Î∂àÌôïÏã§Ìïú ÏàòÏπòÎäî null ÎòêÎäî '‚Äî'Î°ú ÎÇ®Í≤®ÎëêÏÑ∏Ïöî.

[Ï∂úÎ†• Í∑úÏπô]
{rules_text}

[Ïï°ÏÖòÏπ¥Îìú Ïä§ÌÇ§Îßà(JSON)]
{schema_text}

[Îç∞Ïù¥ÌÑ∞(JSON)]
{json.dumps(agent1_json, ensure_ascii=False, indent=2)}
"""
    return guide

def call_gemini_agent2(prompt_text, model_name='models/gemini-2.5-flash'):
    """
    Gemini 2.5 Flash Ï†ÑÏö© Ìò∏Ï∂ú:
    - google-generativeai==0.8.3 Í∏∞Ï§Ä
    - 2.5 flash Í∞ÄÏö©ÏÑ± ÏûêÎèô ÌôïÏù∏(list_models) ÌõÑ 2.5 flash Í≥ÑÏó¥Îßå ÏãúÎèÑ
    - response_mime_type Í∞ïÏ†ú Ìï¥Ï†ú(Îπà ÏùëÎãµ ÌöåÌîº), ÌÖçÏä§Ìä∏‚ÜíJSON Ï∂îÏ∂ú
    - ÏÑ∏Ïù¥ÌîÑÌã∞/ÎπàÏùëÎãµ/Í∞ÄÏö©ÏÑ± Ïù¥Ïäà Ïãú Ìè¥Î∞± Ïπ¥Îìú Î∞òÌôò (Ïï±ÏùÄ Í≥ÑÏÜç ÎèôÏûë)
    - ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: outputs/gemini_debug.json
    """
    import google.generativeai as genai
    import re, json, datetime

    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise RuntimeError('GEMINI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.')
    genai.configure(api_key=api_key)

    # 1) Í≥ÑÏ†ïÏóêÏÑú Ïã§Ï†ú Í∞ÄÎä•Ìïú 2.5-flash Í≥ÑÏó¥ Î™®Îç∏Îßå ÏàòÏßë
    candidates = []
    try:
        avail = []
        for m in genai.list_models():
            if "generateContent" in getattr(m, "supported_generation_methods", []):
                avail.append(m.name)  # Ïòà: 'models/gemini-2.5-flash'
        # 2.5 flash Í≥ÑÏó¥Îßå ÌïÑÌÑ∞
        for name in avail:
            tail = name.split("/")[-1]
            if "2.5" in tail and "flash" in tail:
                candidates.append(name)
    except Exception:
        pass

    # Í∞ÄÏö©ÏÑ± Ï°∞Ìöå Ïã§Ìå®/ÎπÑÏñ¥ÏûàÏúºÎ©¥ Ìï©Î¶¨Ï†Å ÌõÑÎ≥¥(2.5 flash Í≥ÑÏó¥)Îßå ÏãúÎèÑ
    if not candidates:
        candidates = [
            "models/gemini-2.5-flash",
            "models/gemini-2.5-flash-latest",
            "models/gemini-2.5-flash-001",
            "gemini-2.5-flash",
            "gemini-2.5-flash-latest",
            "gemini-2.5-flash-001",
        ]

    # ÏÇ¨Ïö©ÏûêÍ∞Ä Ïù∏ÏûêÎ•º Ï§¨Îã§Î©¥ Îß® ÏïûÏóê Îë†(Ïó≠Ïãú 2.5 flash Í≥ÑÏó¥Ïù¥Ïñ¥Ïïº Ìï®)
    if model_name and model_name not in candidates:
        candidates.insert(0, model_name)

    # ÏÑ∏Ïù¥ÌîÑÌã∞(ÏôÑÌôî) + ÌÜ†ÌÅ∞ Î≥¥ÏàòÏ†Å
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 32,
        "max_output_tokens": 900,
        # "response_mime_type": "application/json",  # Í∞ïÏ†úÌïòÏßÄ ÏïäÏùå(Îπà ÏùëÎãµ ÌöåÌîº)
    }

    def _extract_text(resp):
        text = ""
        try:
            t = getattr(resp, "text", None)
            if t: text = t
        except Exception:
            pass
        if (not text) and getattr(resp, "candidates", None):
            try:
                cand0 = resp.candidates[0]
                if cand0 and getattr(cand0, "content", None) and cand0.content.parts:
                    for p in cand0.content.parts:
                        pt = getattr(p, "text", "")
                        if pt: text += pt
            except Exception:
                pass
        return (text or "").strip()

    def _blocked_info(resp):
        info = {}
        try: info["prompt_feedback"] = getattr(resp, "prompt_feedback", None)
        except Exception: pass
        try:
            if getattr(resp, "candidates", None):
                info["candidate_safety"] = getattr(resp.candidates[0], "safety_ratings", None)
                info["finish_reason"] = getattr(resp.candidates[0], "finish_reason", None)
        except Exception: pass
        return info

    def _run_once(name: str):
        model = genai.GenerativeModel(
            model_name=name,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        resp = model.generate_content(prompt_text)
        text = _extract_text(resp)
        info = _blocked_info(resp)
        return text, info, name

    try:
        _, schema_validator = load_actioncard_schema()
        schema_error = None
    except Exception as e:
        schema_validator = None
        schema_error = str(e)

    all_attempt_logs = []
    last_error = schema_error

    for attempt in range(2):
        attempt_logs = []
        for mname in candidates:
            try:
                text, info, used = _run_once(mname)
                log_entry = {"model": used, "has_text": bool(text), "meta": info}
                if not text:
                    log_entry["error"] = "empty_text"
                    attempt_logs.append(log_entry)
                    last_error = "LLM ÏùëÎãµÏù¥ ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§."
                    continue

                core = None
                m = re.search(r"\{[\s\S]*\}", text)
                if m:
                    try:
                        core = json.loads(m.group(0))
                    except Exception as je:
                        log_entry["error"] = f"json_parse_error: {je}"
                        attempt_logs.append(log_entry)
                        last_error = f"JSON ÌååÏã± Ïã§Ìå®: {je}"
                        continue
                else:
                    log_entry["error"] = "json_not_found"
                    attempt_logs.append(log_entry)
                    last_error = "ÏùëÎãµÏóêÏÑú JSON Î∏îÎ°ùÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§."
                    continue

                if not isinstance(core, dict):
                    log_entry["error"] = "json_not_object"
                    attempt_logs.append(log_entry)
                    last_error = "Ï∂îÏ∂úÎêú JSONÏù¥ Í∞ùÏ≤¥ ÌòïÏãùÏù¥ ÏïÑÎãôÎãàÎã§."
                    continue

                validation_ok = True
                if schema_validator is not None:
                    try:
                        schema_validator.validate(core)
                    except ValidationError as ve:
                        validation_ok = False
                        log_entry["validation_error"] = ve.message
                        last_error = f"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïã§Ìå®: {ve.message}"

                if validation_ok:
                    attempt_logs.append(log_entry)
                    all_attempt_logs.append({"attempt": attempt + 1, "logs": attempt_logs})
                    # ÎîîÎ≤ÑÍ∑∏ Í∏∞Î°ù
                    try:
                        debug = {
                            "ts": datetime.datetime.utcnow().isoformat(),
                            "chosen_model": used,
                            "attempts": all_attempt_logs,
                            "preview_text": text[:400],
                            "schema_error": schema_error
                        }
                        with open(OUTPUT_DIR / "gemini_debug.json", "w", encoding="utf-8") as f:
                            json.dump(debug, f, ensure_ascii=False, indent=2)
                    except Exception:
                        pass

                    outp = OUTPUT_DIR / 'agent2_result.json'
                    with open(outp, 'w', encoding='utf-8') as f:
                        json.dump(core, f, ensure_ascii=False, indent=2)
                    print('‚úÖ Agent-2 Í≤∞Í≥º Ï†ÄÏû•:', outp)
                    return core

                attempt_logs.append(log_entry)
            except Exception as e:
                attempt_logs.append({"model": mname, "error": str(e)})
                last_error = str(e)

        all_attempt_logs.append({"attempt": attempt + 1, "logs": attempt_logs})
        if schema_validator is None:
            break

    # Î™®Îëê Ïã§Ìå® ‚Üí Ìè¥Î∞±(Ïï± Îã§Ïö¥ Î∞©ÏßÄ)
    fallback = {
        "recommendations": [{
            "title": "Îç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï Ï†úÏïà",
            "what": "Î™®Îç∏ Í∞ÄÏö©ÏÑ±/ÏÑ∏Ïù¥ÌîÑÌã∞/ÏøºÌÑ∞Î°ú Ïπ¥Îìú ÏÉùÏÑ±ÏùÑ Î≥¥Î•òÌï©ÎãàÎã§.",
            "when": "ÌôòÍ≤Ω ÌôïÏù∏ ÌõÑ Ïû¨ÏãúÎèÑ",
            "where": ["ÎåÄÏãúÎ≥¥Îìú"],
            "how": ["2.5 flash Í∞ÄÏö©ÏÑ± ÌôïÏù∏", "API ÌÇ§/ÏøºÌÑ∞ ÌôïÏù∏", "ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥ Ï∂ïÏÜå"],
            "copy": ["Îç∞Ïù¥ÌÑ∞Î•º Ï°∞Í∏àÎßå Îçî Ï£ºÏÑ∏Ïöî!"],
            "kpi": {"target": "revisit_rate", "expected_uplift": None, "range": [None, None]},
            "risks": ["LLM ÏïàÏ†Ñ ÌïÑÌÑ∞/ÏøºÌÑ∞/Î™®Îç∏ Í∞ÄÏö©ÏÑ±"],
            "checklist": ["App secrets ÌôïÏù∏", "list_models() Í≤∞Í≥ºÏóêÏÑú 2.5 flash Í≤ÄÏÉâ"],
            "evidence": [
                "gemini_debug.json Î°úÍ∑∏ Ï∞∏Ï°∞",
                f"ÏÇ¨Ïú†: {last_error or 'ÏõêÏù∏ ÎØ∏ÏÉÅ'}"
            ]
        }]
    }
    try:
        debug = {
            "ts": datetime.datetime.utcnow().isoformat(),
            "attempts": all_attempt_logs,
            "schema_error": schema_error,
            "last_error": last_error
        }
        with open(OUTPUT_DIR / "gemini_debug.json", "w", encoding="utf-8") as f:
            json.dump(debug, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

    outp = OUTPUT_DIR / 'agent2_result.json'
    with open(outp, 'w', encoding='utf-8') as f:
        json.dump(fallback, f, ensure_ascii=False, indent=2)
    print('‚ö†Ô∏è Agent-2: 2.5 flash Í∞ÄÏö©ÏÑ±/ÏÑ∏Ïù¥ÌîÑÌã∞ Î¨∏Ï†ú ‚Üí Ìè¥Î∞± Ïπ¥Îìú Î∞òÌôò')
    return fallback

def main():
    import argparse
    a1 = None; prompt_text = ''; a2 = None
    parser = argparse.ArgumentParser()
    parser.add_argument('--question', type=str, default=None)
    parser.add_argument('--model', type=str, default='gemini-2.5-flash')
    args, _ = parser.parse_known_args()
    q = args.question or os.getenv('QUESTION')
    if not q:
        try:
            q = input('ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî: ').strip()
        except Exception:
            q = None
    if not q:
        q = 'ÏÑ±ÎèôÍµ¨ {Í≥†Ìñ•***} Í∏∞Ï§ÄÏúºÎ°ú, Ïû¨Î∞©Î¨∏Ïú®ÏùÑ 4Ï£º ÏïàÏóê ÎÜíÏùº Ïã§ÌñâÏπ¥Îìú Ï†úÏãúÌï¥Ï§ò.'
        print('‚ÑπÔ∏è ÏßàÎ¨∏Ïù¥ ÏóÜÏñ¥ Í∏∞Î≥∏ ÏòàÏãúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§:', q)

    try:
        a1 = agent1_pipeline(q, SHINHAN_DIR, EXTERNAL_DIR)
        prompt_text = build_agent2_prompt(a1)
        print('\n==== Gemini Prompt Preview (ÏïûÎ∂ÄÎ∂Ñ) ====')
        print(prompt_text[:800] + ('\n... (ÏÉùÎûµ)' if len(prompt_text)>800 else ''))
        a2 = call_gemini_agent2(prompt_text, model_name=args.model)
        print('\n==== Agent-2 Í≤∞Í≥º (ÏïûÎ∂ÄÎ∂Ñ) ====')
        print(json.dumps(a2, ensure_ascii=False, indent=2)[:800] + '\n...')
    except FileNotFoundError as e:
        print('‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§:', e)
        print('Ïòà) /content/bigcon/shinhan/big_data_set1_f.csv, big_data_set2_f.csv, big_data_set3_f.csv')
        print('   /content/bigcon/external/weather.csv (ÏÑ†ÌÉù)')
    except Exception as e:
        print('‚ö†Ô∏è Ïã§Ìñâ Ï§ë Ïò§Î•ò:', e)

if __name__ == '__main__':
    main()
