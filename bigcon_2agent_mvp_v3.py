# -*- coding: utf-8 -*-
# BIGCON 2-Agent MVP (Colab, Gemini API) ‚Äî v3 (fits actual 3-dataset structure)
# %pip -q install google-generativeai pandas openpyxl

import os, json, re, random, sys, glob, datetime
import unicodedata
from pathlib import Path
import pandas as pd
import numpy as np
from jsonschema import Draft7Validator, ValidationError

APP_ROOT = Path(__file__).resolve().parent
DATA_DIR = APP_ROOT / 'data'
SHINHAN_DIR = DATA_DIR / 'shinhan'
EXTERNAL_DIR = DATA_DIR / 'external'
OUTPUT_DIR = DATA_DIR / 'outputs'
SCHEMA_PATH = APP_ROOT / 'schemas' / 'actioncard.schema.json'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

DEFAULT_MONTHS = 8
SEED = 42
random.seed(SEED); np.random.seed(SEED)

_SCHEMA_CACHE = None
_SCHEMA_VALIDATOR = None


def _normalize_str(value: str) -> str:
    if value is None:
        return ""
    text = unicodedata.normalize("NFKC", str(value))
    return re.sub(r"\s+", "", text)


def _wildcard_to_regex(masked: str | None) -> re.Pattern | None:
    if not masked:
        return None
    normalized = _normalize_str(masked)
    if not normalized:
        return None
    pattern = "".join(".*" if ch == "*" else re.escape(ch) for ch in normalized)
    try:
        return re.compile(f"^{pattern}")
    except re.error:
        return None

def load_actioncard_schema():
    global _SCHEMA_CACHE, _SCHEMA_VALIDATOR
    if _SCHEMA_CACHE is not None and _SCHEMA_VALIDATOR is not None:
        return _SCHEMA_CACHE, _SCHEMA_VALIDATOR
    if not SCHEMA_PATH.exists():
        raise FileNotFoundError(f"Ïä§ÌÇ§Îßà ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {SCHEMA_PATH}")
    with open(SCHEMA_PATH, 'r', encoding='utf-8') as f:
        _SCHEMA_CACHE = json.load(f)
    _SCHEMA_VALIDATOR = Draft7Validator(_SCHEMA_CACHE)
    return _SCHEMA_CACHE, _SCHEMA_VALIDATOR

def read_csv_smart(path):
    for enc in ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1']:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path, encoding='utf-8', errors='replace')


def normalize_rate_series(series: pd.Series | None) -> pd.Series | None:
    if series is None:
        return series
    cleaned = (
        series.astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("%", "", regex=False)
        .str.replace("‚àí", "-", regex=False)
    )
    cleaned = cleaned.str.replace(r"[^0-9\-.]", "", regex=True)
    numeric = pd.to_numeric(cleaned, errors='coerce')
    if numeric is None:
        return None
    scaled = numeric.copy()
    mask = scaled.notna() & (scaled.abs() <= 1)
    scaled.loc[mask] = scaled.loc[mask] * 100
    scaled.loc[(scaled < 0) | (scaled > 100)] = np.nan
    return scaled

def ym_to_date(ym_series):
    s = pd.to_datetime(ym_series.astype(str) + '01', format='%Y%m%d', errors='coerce')
    return s

def load_set1(shinhan_dir):
    p = shinhan_dir / 'big_data_set1_f.csv'
    df = read_csv_smart(p)
    ren = {}
    for c in df.columns:
        cu = str(c).upper()
        if cu == 'ENCODED_MCT': ren[c] = 'ENCODED_MCT'
        elif 'SIGUNGU' in cu:   ren[c] = 'SIGUNGU'
        elif 'BSE_AR' in cu:    ren[c] = 'ADDR_BASE'
        elif ('ZCD' in cu) or ('BZN' in cu) or ('ÏóÖÏ¢Ö' in cu): ren[c] = 'CATEGORY'
        elif cu == 'MCT_NM':    ren[c] = 'MCT_NM'
    df = df.rename(columns=ren)
    df = df.loc[:, ~df.columns.duplicated()]
    keep = ['ENCODED_MCT','MCT_NM','ADDR_BASE','SIGUNGU','CATEGORY']
    for k in keep:
        if k not in df.columns: df[k] = np.nan
    df = df[keep].drop_duplicates('ENCODED_MCT')
    if 'ENCODED_MCT' in df.columns:
        df['ENCODED_MCT'] = df['ENCODED_MCT'].apply(lambda v: str(v).strip() if pd.notna(v) else '')
    return df

def load_set2(shinhan_dir):
    p = shinhan_dir / 'big_data_set2_f.csv'
    df = read_csv_smart(p)
    df['TA_YM'] = df['TA_YM'].astype(str)
    df['_date'] = ym_to_date(df['TA_YM'])
    return df

def load_set3(shinhan_dir):
    p = shinhan_dir / 'big_data_set3_f.csv'
    df = read_csv_smart(p)
    df['TA_YM'] = df['TA_YM'].astype(str)
    df['_date'] = ym_to_date(df['TA_YM'])
    keep_cols = [
        'ENCODED_MCT','TA_YM','_date',
        'M12_MAL_1020_RAT','M12_MAL_30_RAT','M12_MAL_40_RAT','M12_MAL_50_RAT','M12_MAL_60_RAT',
        'M12_FME_1020_RAT','M12_FME_30_RAT','M12_FME_40_RAT','M12_FME_50_RAT','M12_FME_60_RAT',
        'MCT_UE_CLN_REU_RAT','MCT_UE_CLN_NEW_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT','RC_M1_SHC_WP_UE_CLN_RAT','RC_M1_SHC_FLP_UE_CLN_RAT'
    ]
    for c in keep_cols:
        if c not in df.columns:
            df[c] = np.nan
    df = df[keep_cols]
    rate_cols = [
        'M12_MAL_1020_RAT','M12_MAL_30_RAT','M12_MAL_40_RAT','M12_MAL_50_RAT','M12_MAL_60_RAT',
        'M12_FME_1020_RAT','M12_FME_30_RAT','M12_FME_40_RAT','M12_FME_50_RAT','M12_FME_60_RAT',
        'MCT_UE_CLN_REU_RAT','MCT_UE_CLN_NEW_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT','RC_M1_SHC_WP_UE_CLN_RAT','RC_M1_SHC_FLP_UE_CLN_RAT'
    ]
    for col in rate_cols:
        if col in df.columns:
            df[col] = normalize_rate_series(df[col])
    return df

def load_weather_monthly(external_dir):
    f = None
    for e in ('.csv','.parquet','.parq','.feather'):
        cand = list(external_dir.glob(f'**/*{e}'))
        if cand:
            f = cand[0]; break
    if not f:
        print('‚ö†Ô∏è Ïô∏Î∂Ä(ÎÇ†Ïî®) Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. ÎÇ†Ïî® Î∂ÑÏÑùÏùÄ Ï†úÌïúÎê©ÎãàÎã§.')
        return None
    if f.suffix.lower() == '.csv':
        wx = read_csv_smart(f)
    elif f.suffix.lower() in ('.parquet','.parq'):
        wx = pd.read_parquet(f)
    elif f.suffix.lower() == '.feather':
        wx = pd.read_feather(f)
    else:
        return None

    c_dt = None
    for c in wx.columns:
        cl = str(c).lower()
        if any(k in cl for k in ['date','ymd','dt','ÏùºÏûê','ÎÇ†Ïßú','yyyymm']):
            c_dt = c; break
    if c_dt is None:
        raise ValueError('ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞Ïóê ÎÇ†Ïßú(ÎòêÎäî YYYYMM) Ïª¨ÎüºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
    dt = pd.to_datetime(wx[c_dt].astype(str), errors='coerce')
    wx['_ym'] = dt.dt.strftime('%Y%m')
    c_rain = None
    for c in wx.columns:
        cl = c.lower()
        if any(k in cl for k in ['rain','precip','rn_mm','rainfall','rr','Í∞ïÏàò','Í∞ïÏàòÎüâ','ÎπÑ']):
            c_rain = c; break
    if c_rain is None:
        wx['_rain_val'] = 0.0
    else:
        wx['_rain_val'] = pd.to_numeric(wx[c_rain], errors='coerce').fillna(0.0)

    monthly = wx.groupby('_ym', as_index=False)['_rain_val'].sum().rename(columns={'_ym':'TA_YM','_rain_val':'RAIN_SUM'})
    monthly['TA_YM'] = monthly['TA_YM'].astype(str)
    monthly['_date'] = pd.to_datetime(monthly['TA_YM'] + '01', format='%Y%m%d', errors='coerce')
    return monthly[['TA_YM','_date','RAIN_SUM']]

def build_panel(shinhan_dir, merchants_df=None):
    s1 = merchants_df if merchants_df is not None else load_set1(shinhan_dir)
    s2 = load_set2(shinhan_dir)
    s3 = load_set3(shinhan_dir)
    m23 = pd.merge(s2, s3, on=['ENCODED_MCT','TA_YM','_date'], how='outer', suffixes=('_s2',''))
    panel = pd.merge(m23, s1, on='ENCODED_MCT', how='left')
    def nz(x):
        try: return pd.to_numeric(x, errors='coerce').fillna(0.0)
        except: return pd.Series([0.0]*len(x))
    if 'M12_MAL_1020_RAT' in panel.columns and 'M12_FME_1020_RAT' in panel.columns:
        panel['YOUTH_SHARE'] = nz(panel['M12_MAL_1020_RAT']) + nz(panel['M12_FME_1020_RAT'])
    else:
        panel['YOUTH_SHARE'] = np.nan
    panel['REVISIT_RATE'] = pd.to_numeric(panel.get('MCT_UE_CLN_REU_RAT', np.nan), errors='coerce')
    panel['NEW_RATE'] = pd.to_numeric(panel.get('MCT_UE_CLN_NEW_RAT', np.nan), errors='coerce')
    panel.rename(columns={'ENCODED_MCT':'_merchant_id'}, inplace=True)
    if '_merchant_id' in panel.columns:
        panel['_merchant_id'] = panel['_merchant_id'].astype(str)
    return panel


def resolve_merchant(masked_name: str | None, sigungu: str | None, industry_label: str | None, merchants_df: pd.DataFrame | None):
    if merchants_df is None or merchants_df.empty:
        return None

    df = merchants_df.copy()
    df['_norm_name'] = df['MCT_NM'].apply(_normalize_str)
    df['_norm_sigungu'] = df['SIGUNGU'].apply(_normalize_str)
    df['_norm_category'] = df['CATEGORY'].apply(_normalize_str)

    log_context = {
        'masked_name': masked_name,
        'sigungu': sigungu,
        'industry_label': industry_label,
    }

    candidates = df
    norm_sigungu = _normalize_str(sigungu) if sigungu else ""
    sigungu_filter_count = len(candidates)
    if norm_sigungu:
        exact = df[df['_norm_sigungu'] == norm_sigungu]
        if not exact.empty:
            candidates = exact
        else:
            partial = df[df['_norm_sigungu'].str.contains(norm_sigungu, na=False)]
            if not partial.empty:
                candidates = partial
        sigungu_filter_count = len(candidates)

    norm_industry = _normalize_str(industry_label) if industry_label else ""
    category_filter_count = len(candidates)
    if norm_industry:
        narrowed = candidates[candidates['_norm_category'].str.contains(norm_industry, na=False)]
        if narrowed.empty:
            tokens = [t for t in re.split(r'[-/,&]', norm_industry) if t]
            token_matches = []
            for token in tokens:
                sub = candidates[candidates['_norm_category'].str.contains(token, na=False)]
                if not sub.empty:
                    token_matches.append(sub)
            if token_matches:
                candidates = pd.concat(token_matches).drop_duplicates('ENCODED_MCT')
        else:
            candidates = narrowed
        category_filter_count = len(candidates)

    pattern = _wildcard_to_regex(masked_name)
    name_core = _normalize_str(masked_name.replace('*', '')) if masked_name else ""
    name_regex = pattern.pattern if pattern else None

    def _score(row):
        score = 0.0
        if norm_sigungu:
            if row['_norm_sigungu'] == norm_sigungu:
                score += 4.0
            elif norm_sigungu in row['_norm_sigungu']:
                score += 2.0
        if norm_industry:
            tokens = [t for t in re.split(r'[-/,&]', norm_industry) if t]
            for token in tokens:
                if token and token in row['_norm_category']:
                    score += 1.5
        if masked_name:
            name = row['_norm_name']
            if pattern and pattern.match(name):
                score += 10.0
            elif pattern and pattern.search(name):
                score += 6.0
            if name_core and name_core in name:
                score += 3.0
        return score

    scored = candidates.copy()
    scored['__score'] = scored.apply(_score, axis=1)
    if masked_name and scored['__score'].max() <= 0:
        scored = df.copy()
        scored['__score'] = scored.apply(_score, axis=1)
        sigungu_filter_count = len(df)
        category_filter_count = len(df)

    top = scored.sort_values(['__score', 'ENCODED_MCT'], ascending=[False, True]).head(1)
    name_match_count = 0
    if pattern is not None and '_norm_name' in scored:
        name_match_count = int(scored['_norm_name'].apply(lambda v: bool(pattern.match(v))).sum())

    candidate_preview = []
    for _, row in scored.sort_values('__score', ascending=False).head(3).iterrows():
        candidate_preview.append({
            'ENCODED_MCT': row['ENCODED_MCT'],
            'MCT_NM': row['MCT_NM'],
            'SIGUNGU': row['SIGUNGU'],
            'CATEGORY': row['CATEGORY'],
            'score': float(row['__score']) if pd.notna(row['__score']) else None,
        })

    print(
        "üß≠ resolve_phase:",
        json.dumps({
            'input': log_context,
            'sigungu_filter_count': sigungu_filter_count,
            'category_filter_count': category_filter_count,
            'name_regex': name_regex,
            'name_match_count': name_match_count,
            'candidates': candidate_preview,
        }, ensure_ascii=False)
    )

    if top.empty or top.iloc[0]['__score'] <= 0:
        print("‚ö†Ô∏è resolve_merchant: Í∞ÄÎßπÏ†ê ÎØ∏ÏùºÏπò ‚Äî Í∑úÏπô ÏôÑÌôî ÌïÑÏöî")
        return None
    best = top.iloc[0]
    resolved = {
        'encoded_mct': best['ENCODED_MCT'],
        'masked_name': best['MCT_NM'],
        'address': best.get('ADDR_BASE'),
        'sigungu': best.get('SIGUNGU'),
        'category': best.get('CATEGORY'),
        'score': float(best['__score']) if pd.notna(best['__score']) else None,
    }
    print("‚úÖ resolved_merchant_id:", resolved['encoded_mct'])
    return resolved

def parse_question(q):
    original = q or ''
    lower_q = original.lower()
    age_cond = None
    if '10ÎåÄ' in original or 'teen' in lower_q or re.search(r'\b1[0-9]\b', lower_q):
        age_cond = ('<=', 19)
    if '20ÎåÄ' in original or '20s' in lower_q or 'twenties' in lower_q:
        if ('Ïù¥Ìïò' in original) or ('under' in lower_q) or ('<=' in lower_q):
            age_cond = ('<=', 20)
        else:
            age_cond = ('range', (20,29))
    if 'Ï≤≠ÏÜåÎÖÑ' in original:
        age_cond = ('<=', 19)

    weather = None
    if ('ÎπÑ' in original) or ('Ïö∞Ï≤ú' in original) or ('rain' in lower_q):
        weather = 'rain'
    elif ('Îßë' in original) or ('sunny' in lower_q) or ('clear' in lower_q):
        weather = 'clear'
    elif ('Îàà' in original) or ('snow' in lower_q):
        weather = 'snow'

    months = DEFAULT_MONTHS
    weeks_requested = None
    week_match = re.search(r'(\d+)\s*Ï£º', original)
    if week_match:
        try:
            weeks_requested = int(week_match.group(1))
        except ValueError:
            weeks_requested = None
        if weeks_requested and weeks_requested > 0:
            months = max(1, round(weeks_requested / 4))
    if 'Ïù¥Î≤àÎã¨' in original or 'this month' in lower_q:
        months = 1
    elif ('ÌïúÎã¨' in original) or ('1Îã¨' in original) or ('month' in lower_q):
        months = 1
    elif 'Î∂ÑÍ∏∞' in original or 'quarter' in lower_q:
        months = 3

    industry = None
    if ('Ïπ¥Ìéò' in original) or ('Ïª§Ìîº' in original):
        industry = 'cafe'
    elif ('ÏöîÏãù' in original) or ('restaurant' in lower_q) or ('ÏãùÎãπ' in original):
        industry = 'restaurant'

    merchant_pattern = re.search(r'(?P<sigungu>[\w\sÍ∞Ä-Ìû£]+Íµ¨)\s+(?P<name>[\wÍ∞Ä-Ìû£\*]+)\s*\((?P<industry>[^\)]+)\)', original)
    merchant_info = {'masked_name': None, 'sigungu': None, 'industry_label': None}
    if merchant_pattern:
        merchant_info = {
            'masked_name': merchant_pattern.group('name').strip(),
            'sigungu': merchant_pattern.group('sigungu').strip(),
            'industry_label': merchant_pattern.group('industry').strip(),
        }

    explicit_id = None
    trimmed = original.strip()
    if re.fullmatch(r'[A-Z0-9]{10,12}', trimmed):
        explicit_id = trimmed

    return {
        'age_cond': age_cond,
        'weather': weather,
        'months': months,
        'weeks_requested': weeks_requested,
        'industry': industry,
        'merchant_masked_name': merchant_info['masked_name'],
        'merchant_sigungu': merchant_info['sigungu'],
        'merchant_industry_label': merchant_info['industry_label'],
        'merchant_explicit_id': explicit_id,
    }

def subset_period(panel, months=DEFAULT_MONTHS):
    if panel['_date'].isna().all():
        return panel.iloc[0:0]
    maxd = panel['_date'].max()
    thr = maxd - pd.Timedelta(days=31*months)
    return panel[panel['_date'] >= thr]

def kpi_summary(panel_sub):
    if panel_sub.empty:
        return {}
    latest_idx = panel_sub.groupby('_merchant_id')['_date'].idxmax()
    snap = panel_sub.loc[latest_idx].sort_values('_date', ascending=False)

    def _safe_float(val):
        try:
            f = float(val)
        except (TypeError, ValueError):
            return None
        return f

    def _clean_pct(val):
        if val is None or pd.isna(val):
            return None
        try:
            f = float(val)
        except (TypeError, ValueError):
            return None
        if f < 0 or f > 100:
            return None
        return round(f, 2)

    detail_row = snap.iloc[0]
    youth_latest = _safe_float(detail_row.get('YOUTH_SHARE'))
    revisit_latest = _safe_float(detail_row.get('REVISIT_RATE'))
    new_latest = _safe_float(detail_row.get('NEW_RATE'))

    age_labels = {
        '1020': 'Ï≤≠ÎÖÑ(10-20)',
        '30': '30ÎåÄ',
        '40': '40ÎåÄ',
        '50': '50ÎåÄ',
        '60': '60ÎåÄ',
    }
    age_distribution = []
    for code, label in age_labels.items():
        cols = [f'M12_MAL_{code}_RAT', f'M12_FME_{code}_RAT']
        vals = pd.to_numeric(pd.Series([detail_row.get(c) for c in cols]), errors='coerce')
        total = vals.sum(skipna=True)
        if pd.notna(total):
            cleaned = _clean_pct(total)
            if cleaned is not None:
                age_distribution.append({'code': code, 'label': label, 'value': cleaned})
    age_distribution.sort(key=lambda x: x['value'], reverse=True)

    customer_mix_map = [
        ('Ïú†Îèô', 'RC_M1_SHC_FLP_UE_CLN_RAT'),
        ('Í±∞Ï£º', 'RC_M1_SHC_RSD_UE_CLN_RAT'),
        ('ÏßÅÏû•', 'RC_M1_SHC_WP_UE_CLN_RAT'),
    ]
    customer_mix_detail = {}
    for label, col in customer_mix_map:
        customer_mix_detail[label] = _clean_pct(_safe_float(detail_row.get(col)))

    ticket_band_raw = detail_row.get('APV_CE_RAT')
    ticket_band = None
    if isinstance(ticket_band_raw, str):
        parts = ticket_band_raw.split('_', 1)
        ticket_band = parts[-1].strip() if parts else ticket_band_raw.strip()
    elif pd.notna(ticket_band_raw):
        ticket_band = str(ticket_band_raw)

    sanitized = {
        'youth_share_avg': _clean_pct(youth_latest),
        'revisit_rate_avg': _clean_pct(revisit_latest),
        'new_rate_avg': _clean_pct(new_latest),
        'age_distribution': age_distribution,
        'age_top_segments': age_distribution[:3],
        'customer_mix_detail': customer_mix_detail,
        'avg_ticket_band_label': ticket_band,
        'n_merchants': int(snap['_merchant_id'].nunique()),
    }

    raw_cols = [
        'MCT_UE_CLN_REU_RAT',
        'MCT_UE_CLN_NEW_RAT',
        'M12_MAL_1020_RAT',
        'M12_FME_1020_RAT',
        'RC_M1_SHC_FLP_UE_CLN_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT',
        'RC_M1_SHC_WP_UE_CLN_RAT',
    ]
    raw_snapshot = {col: detail_row.get(col) for col in raw_cols}
    raw_snapshot['TA_YM'] = detail_row.get('TA_YM')
    raw_snapshot['_date'] = str(detail_row.get('_date'))
    print("üóÇ KPI raw snapshot:", json.dumps(raw_snapshot, ensure_ascii=False))
    print("‚úÖ KPI sanitized:", json.dumps({
        'revisit_pct': sanitized['revisit_rate_avg'],
        'new_pct': sanitized['new_rate_avg'],
        'youth_pct': sanitized['youth_share_avg'],
        'customer_mix_detail': sanitized['customer_mix_detail'],
    }, ensure_ascii=False))

    return sanitized

def weather_effect(panel_sub, wx_monthly):
    if (wx_monthly is None) or panel_sub.empty or ('REVISIT_RATE' not in panel_sub):
        return {'metric':'REVISIT_RATE','effect':None,'ci':[None,None],'note':'ÎÇ†Ïî®/ÌëúÎ≥∏ Î∂ÄÏ°±'}
    m = panel_sub.groupby('TA_YM', as_index=False)['REVISIT_RATE'].mean()
    m = m.merge(wx_monthly[['TA_YM','RAIN_SUM']], on='TA_YM', how='inner')
    if m.empty or m['RAIN_SUM'].nunique() < 2:
        return {'metric':'REVISIT_RATE','effect':None,'ci':[None,None],'note':'ÏÉÅÍ¥Ä Ï∂îÏ†ï Î∂àÍ∞Ä'}
    corr = m['REVISIT_RATE'].corr(m['RAIN_SUM'])
    return {'metric':'REVISIT_RATE','effect':float(corr), 'ci':[None,None], 'note':'ÌîºÏñ¥Ïä® ÏÉÅÍ¥Ä(ÏõîÎã®ÏúÑ)'}

def agent1_pipeline(question, shinhan_dir=SHINHAN_DIR, external_dir=EXTERNAL_DIR):
    merchants_df = load_set1(shinhan_dir)
    panel = build_panel(shinhan_dir, merchants_df=merchants_df)
    qinfo = parse_question(question)

    run_id = datetime.datetime.utcnow().isoformat()
    parse_log = {
        'merchant_mask': qinfo.get('merchant_masked_name'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'industry_label': qinfo.get('merchant_industry_label'),
        'explicit_id': qinfo.get('merchant_explicit_id'),
    }
    print("üÜî agent1_run:", run_id)
    print("üßæ question_fields:", json.dumps(parse_log, ensure_ascii=False))

    merchant_match = None
    explicit_id = qinfo.get('merchant_explicit_id')
    if explicit_id:
        lookup = merchants_df[merchants_df['ENCODED_MCT'] == explicit_id]
        print(
            "üè∑ explicit_id_lookup:",
            json.dumps({'explicit_id': explicit_id, 'row_count': int(len(lookup))}, ensure_ascii=False),
        )
        if not lookup.empty:
            row = lookup.iloc[0]
            merchant_match = {
                'encoded_mct': str(row['ENCODED_MCT']),
                'masked_name': row.get('MCT_NM'),
                'address': row.get('ADDR_BASE'),
                'sigungu': row.get('SIGUNGU'),
                'category': row.get('CATEGORY'),
                'score': None,
            }

    if merchant_match is None:
        merchant_match = resolve_merchant(
            qinfo.get('merchant_masked_name'),
            qinfo.get('merchant_sigungu'),
            qinfo.get('merchant_industry_label'),
            merchants_df,
        )

    if merchant_match and merchant_match.get('encoded_mct') is not None:
        merchant_match['encoded_mct'] = str(merchant_match['encoded_mct'])

    merchants_covered_before = int(panel['_merchant_id'].nunique()) if '_merchant_id' in panel.columns else 0
    panel_focus = panel
    target_id = None
    if merchant_match and merchant_match.get('encoded_mct'):
        target_id = merchant_match['encoded_mct']
        panel_focus = panel[panel['_merchant_id'] == target_id]
    merchants_covered_after = int(panel_focus['_merchant_id'].nunique()) if '_merchant_id' in panel_focus.columns else 0
    print(
        "üì¶ panel_filter:",
        json.dumps({
            'before': merchants_covered_before,
            'after': merchants_covered_after,
            'target_id': target_id,
        }, ensure_ascii=False),
    )
    print("üèÅ merchant_match:", json.dumps(merchant_match, ensure_ascii=False))

    sub = subset_period(panel_focus, months=qinfo['months'])

    wxm = None
    try:
        wxm = load_weather_monthly(external_dir)
    except Exception:
        wxm = None

    kpis = kpi_summary(sub)
    wfx  = weather_effect(sub, wxm)

    notes = []
    quality = 'normal'
    if sub.empty:
        notes.append('ÏßàÎ¨∏ Ï°∞Í±¥ ÌëúÎ≥∏ Î∂ÄÏ°± ÎòêÎäî Í∏∞Í∞Ñ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå')
        quality = 'low'
    if wxm is None and qinfo['weather'] is not None:
        notes.append('ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞ Î∂ÄÏû¨: ÎÇ†Ïî® Í¥ÄÎ†® Ìö®Í≥ºÎäî Ï∂îÏ†ïÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        quality = 'low'
    if merchant_match is None:
        notes.append('ÏßàÎ¨∏Í≥º ÏùºÏπòÌïòÎäî Í∞ÄÎßπÏ†êÏùÑ Ï∞æÏßÄ Î™ªÌï¥ Ï†ÑÏ≤¥ ÌëúÎ≥∏ÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§.')
        quality = 'low'

    merchant_query = {
        'masked_name': qinfo.get('merchant_masked_name'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'industry_label': qinfo.get('merchant_industry_label'),
    }

    out = {
        'context': {
            'intent': question,
            'parsed': qinfo,
            'merchant_query': merchant_query,
            'run_id': run_id,
        },
        'kpis': kpis,
        'weather_effect': wfx,
        'limits': notes,
        'quality': quality,
        'period': {
            'max_date': str(panel['_date'].max() if '_date' in panel.columns else None),
            'months': qinfo['months'],
            'weeks_requested': qinfo.get('weeks_requested'),
        },
        'sample': {
            'merchants_covered': int(sub['_merchant_id'].nunique()) if not sub.empty else 0
        }
    }

    if merchant_match:
        out['context']['merchant'] = merchant_match
        out['context']['merchant_masked_name'] = merchant_match.get('masked_name')

    out_path = OUTPUT_DIR / 'agent1_output.json'
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print('‚úÖ Agent-1 JSON Ï†ÄÏû•:', out_path)
    return out

def build_agent2_prompt(agent1_json):
    try:
        schema, _ = load_actioncard_schema()
        schema_text = json.dumps(schema, ensure_ascii=False, indent=2)
    except Exception as e:
        schema_text = json.dumps({"schema_error": str(e)}, ensure_ascii=False, indent=2)

    rules = [
        "Agent-1 JSONÎßå Í∑ºÍ±∞Î°ú ÌôúÏö©ÌïòÍ≥† Ïô∏Î∂Ä Ï∂îÏ†ïÏùÄ Í∏àÏßÄÌï©ÎãàÎã§.",
        "Î™®Îì† Ïπ¥ÎìúÏóê ÌÉÄÍ≤ü ‚Üí Ï±ÑÎÑê ‚Üí Î∞©Î≤ï ‚Üí Ïπ¥Ìîº(2Í∞ú Ïù¥ÏÉÅ) ‚Üí KPI ‚Üí Î¶¨Ïä§ÌÅ¨/ÏôÑÌôî ‚Üí Í∑ºÍ±∞Î•º Ï±ÑÏõÅÎãàÎã§.",
        "Í∑ºÍ±∞ Î¨∏Ïû•ÏùÄ Î∞òÎìúÏãú Ïà´Ïûê+Ïª¨ÎüºÎ™Ö+Í∏∞Í∞Ñ ÌòïÏãùÏù¥Î©∞ Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ null ÎòêÎäî '‚Äî'Î°ú Îë°ÎãàÎã§.",
        "ÌíàÏßàÏù¥ ÎÇÆÍ±∞ÎÇò Îç∞Ïù¥ÌÑ∞Í∞Ä Î∂ÄÏ°±ÌïòÎ©¥ ÎßàÏßÄÎßâ Ïπ¥ÎìúÏóê 'Îç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï Ï†úÏïà'ÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.",
        "ÏÉÅÌò∏Î™ÖÏùÄ Ìï≠ÏÉÅ ÎßàÏä§ÌÇπÎêú ÌòïÌÉúÎ°ú Ïú†ÏßÄÌï©ÎãàÎã§.",
        "KPI.expected_upliftÏôÄ rangeÎäî Í∑ºÍ±∞Í∞Ä ÏûàÏùÑ ÎïåÎßå Í∞íÏùÑ ÎÑ£Í≥†, ÏóÜÏúºÎ©¥ nullÏùÑ Ïú†ÏßÄÌï©ÎãàÎã§."
    ]

    rules_text = "- " + "\n- ".join(rules)

    guide = f"""ÎãπÏã†ÏùÄ ÌïúÍµ≠Ïñ¥ ÏÜåÏÉÅÍ≥µÏù∏ Ïª®ÏÑ§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÏïÑÎûò Agent-1 JSONÎßå Í∑ºÍ±∞Î°ú ÏÇ¨Ïö©ÌïòÏó¨,
Î∞òÎìúÏãú Ïï°ÏÖòÏπ¥Îìú Ïä§ÌÇ§Îßà(JSON)Î°úÎßå ÎãµÌïòÏÑ∏Ïöî. Î∂àÌôïÏã§Ìïú ÏàòÏπòÎäî null ÎòêÎäî '‚Äî'Î°ú ÎÇ®Í≤®ÎëêÏÑ∏Ïöî.

[Ï∂úÎ†• Í∑úÏπô]
{rules_text}

[Ïï°ÏÖòÏπ¥Îìú Ïä§ÌÇ§Îßà(JSON)]
{schema_text}

[Îç∞Ïù¥ÌÑ∞(JSON)]
{json.dumps(agent1_json, ensure_ascii=False, indent=2)}
"""
    return guide

def call_gemini_agent2(prompt_text, model_name='models/gemini-2.5-flash'):
    """
    Gemini 2.5 Flash Ï†ÑÏö© Ìò∏Ï∂ú:
    - google-generativeai==0.8.3 Í∏∞Ï§Ä
    - 2.5 flash Í∞ÄÏö©ÏÑ± ÏûêÎèô ÌôïÏù∏(list_models) ÌõÑ 2.5 flash Í≥ÑÏó¥Îßå ÏãúÎèÑ
    - response_mime_type Í∞ïÏ†ú Ìï¥Ï†ú(Îπà ÏùëÎãµ ÌöåÌîº), ÌÖçÏä§Ìä∏‚ÜíJSON Ï∂îÏ∂ú
    - ÏÑ∏Ïù¥ÌîÑÌã∞/ÎπàÏùëÎãµ/Í∞ÄÏö©ÏÑ± Ïù¥Ïäà Ïãú Ìè¥Î∞± Ïπ¥Îìú Î∞òÌôò (Ïï±ÏùÄ Í≥ÑÏÜç ÎèôÏûë)
    - ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏: outputs/gemini_debug.json
    """
    import google.generativeai as genai
    import re, json, datetime

    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise RuntimeError('GEMINI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.')
    genai.configure(api_key=api_key)

    # 1) Í≥ÑÏ†ïÏóêÏÑú Ïã§Ï†ú Í∞ÄÎä•Ìïú 2.5-flash Í≥ÑÏó¥ Î™®Îç∏Îßå ÏàòÏßë
    candidates = []
    try:
        avail = []
        for m in genai.list_models():
            if "generateContent" in getattr(m, "supported_generation_methods", []):
                avail.append(m.name)  # Ïòà: 'models/gemini-2.5-flash'
        # 2.5 flash Í≥ÑÏó¥Îßå ÌïÑÌÑ∞
        for name in avail:
            tail = name.split("/")[-1]
            if "2.5" in tail and "flash" in tail:
                candidates.append(name)
    except Exception:
        pass

    # Í∞ÄÏö©ÏÑ± Ï°∞Ìöå Ïã§Ìå®/ÎπÑÏñ¥ÏûàÏúºÎ©¥ Ìï©Î¶¨Ï†Å ÌõÑÎ≥¥(2.5 flash Í≥ÑÏó¥)Îßå ÏãúÎèÑ
    if not candidates:
        candidates = [
            "models/gemini-2.5-flash",
            "models/gemini-2.5-flash-latest",
            "models/gemini-2.5-flash-001",
            "gemini-2.5-flash",
            "gemini-2.5-flash-latest",
            "gemini-2.5-flash-001",
        ]

    # ÏÇ¨Ïö©ÏûêÍ∞Ä Ïù∏ÏûêÎ•º Ï§¨Îã§Î©¥ Îß® ÏïûÏóê Îë†(Ïó≠Ïãú 2.5 flash Í≥ÑÏó¥Ïù¥Ïñ¥Ïïº Ìï®)
    if model_name and model_name not in candidates:
        candidates.insert(0, model_name)

    # ÏÑ∏Ïù¥ÌîÑÌã∞(ÏôÑÌôî) + ÌÜ†ÌÅ∞ Î≥¥ÏàòÏ†Å
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {
        "temperature": 0.2,
        "top_p": 0.9,
        "top_k": 32,
        "max_output_tokens": 900,
        # "response_mime_type": "application/json",  # Í∞ïÏ†úÌïòÏßÄ ÏïäÏùå(Îπà ÏùëÎãµ ÌöåÌîº)
    }

    def _extract_text(resp):
        text = ""
        try:
            t = getattr(resp, "text", None)
            if t: text = t
        except Exception:
            pass
        if (not text) and getattr(resp, "candidates", None):
            try:
                cand0 = resp.candidates[0]
                if cand0 and getattr(cand0, "content", None) and cand0.content.parts:
                    for p in cand0.content.parts:
                        pt = getattr(p, "text", "")
                        if pt: text += pt
            except Exception:
                pass
        return (text or "").strip()

    def _blocked_info(resp):
        info = {}
        try: info["prompt_feedback"] = getattr(resp, "prompt_feedback", None)
        except Exception: pass
        try:
            if getattr(resp, "candidates", None):
                info["candidate_safety"] = getattr(resp.candidates[0], "safety_ratings", None)
                info["finish_reason"] = getattr(resp.candidates[0], "finish_reason", None)
        except Exception: pass
        return info

    def _run_once(name: str):
        model = genai.GenerativeModel(
            model_name=name,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
        resp = model.generate_content(prompt_text)
        text = _extract_text(resp)
        info = _blocked_info(resp)
        return text, info, name

    try:
        _, schema_validator = load_actioncard_schema()
        schema_error = None
    except Exception as e:
        schema_validator = None
        schema_error = str(e)

    all_attempt_logs = []
    last_error = schema_error

    for attempt in range(2):
        attempt_logs = []
        for mname in candidates:
            try:
                text, info, used = _run_once(mname)
                log_entry = {"model": used, "has_text": bool(text), "meta": info}
                if not text:
                    log_entry["error"] = "empty_text"
                    attempt_logs.append(log_entry)
                    last_error = "LLM ÏùëÎãµÏù¥ ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§."
                    continue

                core = None
                m = re.search(r"\{[\s\S]*\}", text)
                if m:
                    try:
                        core = json.loads(m.group(0))
                    except Exception as je:
                        log_entry["error"] = f"json_parse_error: {je}"
                        attempt_logs.append(log_entry)
                        last_error = f"JSON ÌååÏã± Ïã§Ìå®: {je}"
                        continue
                else:
                    log_entry["error"] = "json_not_found"
                    attempt_logs.append(log_entry)
                    last_error = "ÏùëÎãµÏóêÏÑú JSON Î∏îÎ°ùÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§."
                    continue

                if not isinstance(core, dict):
                    log_entry["error"] = "json_not_object"
                    attempt_logs.append(log_entry)
                    last_error = "Ï∂îÏ∂úÎêú JSONÏù¥ Í∞ùÏ≤¥ ÌòïÏãùÏù¥ ÏïÑÎãôÎãàÎã§."
                    continue

                validation_ok = True
                if schema_validator is not None:
                    try:
                        schema_validator.validate(core)
                    except ValidationError as ve:
                        validation_ok = False
                        log_entry["validation_error"] = ve.message
                        last_error = f"Ïä§ÌÇ§Îßà Í≤ÄÏ¶ù Ïã§Ìå®: {ve.message}"

                if validation_ok:
                    attempt_logs.append(log_entry)
                    all_attempt_logs.append({"attempt": attempt + 1, "logs": attempt_logs})
                    # ÎîîÎ≤ÑÍ∑∏ Í∏∞Î°ù
                    try:
                        debug = {
                            "ts": datetime.datetime.utcnow().isoformat(),
                            "chosen_model": used,
                            "attempts": all_attempt_logs,
                            "preview_text": text[:400],
                            "schema_error": schema_error
                        }
                        with open(OUTPUT_DIR / "gemini_debug.json", "w", encoding="utf-8") as f:
                            json.dump(debug, f, ensure_ascii=False, indent=2)
                    except Exception:
                        pass

                    outp = OUTPUT_DIR / 'agent2_result.json'
                    with open(outp, 'w', encoding='utf-8') as f:
                        json.dump(core, f, ensure_ascii=False, indent=2)
                    print('‚úÖ Agent-2 Í≤∞Í≥º Ï†ÄÏû•:', outp)
                    return core

                attempt_logs.append(log_entry)
            except Exception as e:
                attempt_logs.append({"model": mname, "error": str(e)})
                last_error = str(e)

        all_attempt_logs.append({"attempt": attempt + 1, "logs": attempt_logs})
        if schema_validator is None:
            break

    # Î™®Îëê Ïã§Ìå® ‚Üí Ìè¥Î∞±(Ïï± Îã§Ïö¥ Î∞©ÏßÄ)
    fallback = {
        "recommendations": [{
            "title": "Îç∞Ïù¥ÌÑ∞ Î≥¥Í∞ï Ï†úÏïà",
            "what": "Î™®Îç∏ Í∞ÄÏö©ÏÑ±/ÏÑ∏Ïù¥ÌîÑÌã∞/ÏøºÌÑ∞Î°ú Ïπ¥Îìú ÏÉùÏÑ±ÏùÑ Î≥¥Î•òÌï©ÎãàÎã§.",
            "when": "ÌôòÍ≤Ω ÌôïÏù∏ ÌõÑ Ïû¨ÏãúÎèÑ",
            "where": ["ÎåÄÏãúÎ≥¥Îìú"],
            "how": ["2.5 flash Í∞ÄÏö©ÏÑ± ÌôïÏù∏", "API ÌÇ§/ÏøºÌÑ∞ ÌôïÏù∏", "ÌîÑÎ°¨ÌîÑÌä∏ Í∏∏Ïù¥ Ï∂ïÏÜå"],
            "copy": ["Îç∞Ïù¥ÌÑ∞Î•º Ï°∞Í∏àÎßå Îçî Ï£ºÏÑ∏Ïöî!"],
            "kpi": {"target": "revisit_rate", "expected_uplift": None, "range": [None, None]},
            "risks": ["LLM ÏïàÏ†Ñ ÌïÑÌÑ∞/ÏøºÌÑ∞/Î™®Îç∏ Í∞ÄÏö©ÏÑ±"],
            "checklist": ["App secrets ÌôïÏù∏", "list_models() Í≤∞Í≥ºÏóêÏÑú 2.5 flash Í≤ÄÏÉâ"],
            "evidence": [
                "gemini_debug.json Î°úÍ∑∏ Ï∞∏Ï°∞",
                f"ÏÇ¨Ïú†: {last_error or 'ÏõêÏù∏ ÎØ∏ÏÉÅ'}"
            ]
        }]
    }
    try:
        debug = {
            "ts": datetime.datetime.utcnow().isoformat(),
            "attempts": all_attempt_logs,
            "schema_error": schema_error,
            "last_error": last_error
        }
        with open(OUTPUT_DIR / "gemini_debug.json", "w", encoding="utf-8") as f:
            json.dump(debug, f, ensure_ascii=False, indent=2)
    except Exception:
        pass

    outp = OUTPUT_DIR / 'agent2_result.json'
    with open(outp, 'w', encoding='utf-8') as f:
        json.dump(fallback, f, ensure_ascii=False, indent=2)
    print('‚ö†Ô∏è Agent-2: 2.5 flash Í∞ÄÏö©ÏÑ±/ÏÑ∏Ïù¥ÌîÑÌã∞ Î¨∏Ï†ú ‚Üí Ìè¥Î∞± Ïπ¥Îìú Î∞òÌôò')
    return fallback

def main():
    import argparse
    a1 = None; prompt_text = ''; a2 = None
    parser = argparse.ArgumentParser()
    parser.add_argument('--question', type=str, default=None)
    parser.add_argument('--model', type=str, default='gemini-2.5-flash')
    args, _ = parser.parse_known_args()
    q = args.question or os.getenv('QUESTION')
    if not q:
        try:
            q = input('ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî: ').strip()
        except Exception:
            q = None
    if not q:
        q = 'ÏÑ±ÎèôÍµ¨ Í≥†Ìñ•*** (ÌïúÏãù-Ï∞åÍ∞ú/Ï†ÑÍ≥®) Í∞ÄÎßπÏ†ê Í∏∞Ï§ÄÏúºÎ°ú, Ïû¨Î∞©Î¨∏Ïú®ÏùÑ 4Ï£º ÏïàÏóê ÎÜíÏùº Ïã§ÌñâÏπ¥Îìú Ï†úÏãúÌï¥Ï§ò.'
        print('‚ÑπÔ∏è ÏßàÎ¨∏Ïù¥ ÏóÜÏñ¥ Í∏∞Î≥∏ ÏòàÏãúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§:', q)

    try:
        a1 = agent1_pipeline(q, SHINHAN_DIR, EXTERNAL_DIR)
        prompt_text = build_agent2_prompt(a1)
        print('\n==== Gemini Prompt Preview (ÏïûÎ∂ÄÎ∂Ñ) ====')
        print(prompt_text[:800] + ('\n... (ÏÉùÎûµ)' if len(prompt_text)>800 else ''))
        a2 = call_gemini_agent2(prompt_text, model_name=args.model)
        print('\n==== Agent-2 Í≤∞Í≥º (ÏïûÎ∂ÄÎ∂Ñ) ====')
        print(json.dumps(a2, ensure_ascii=False, indent=2)[:800] + '\n...')
    except FileNotFoundError as e:
        print('‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§:', e)
        print('Ïòà) /content/bigcon/shinhan/big_data_set1_f.csv, big_data_set2_f.csv, big_data_set3_f.csv')
        print('   /content/bigcon/external/weather.csv (ÏÑ†ÌÉù)')
    except Exception as e:
        print('‚ö†Ô∏è Ïã§Ìñâ Ï§ë Ïò§Î•ò:', e)

if __name__ == '__main__':
    main()
