# -*- coding: utf-8 -*-
# BIGCON 2-Agent MVP (Colab, Gemini API) ‚Äî v3 (fits actual 3-dataset structure)
# %pip -q install google-generativeai pandas openpyxl

import ast
import datetime
import json
import os
import random
import re
from time import perf_counter
import unicodedata
from difflib import SequenceMatcher
from pathlib import Path
import pandas as pd
import numpy as np
from jsonschema import Draft7Validator

from app_core.formatters import merge_age_buckets, to_float_pct

__all__ = [
    "agent1_pipeline",
    "build_agent2_prompt",
    "build_agent2_prompt_overhauled",
    "call_gemini_agent2",
    "call_gemini_agent2_overhauled",
    "infer_question_type",
    "load_actioncard_schema",
    "load_actioncard_schema_current",
    "AGENT2_PROMPT_TRACE",
    "AGENT2_RESPONSE_TRACE",
    "llm_json_safe_parse",
]

APP_ROOT = Path(__file__).resolve().parent
DATA_DIR = APP_ROOT / 'data'
SHINHAN_DIR = DATA_DIR / 'shinhan'
EXTERNAL_DIR = DATA_DIR / 'external'
OUTPUT_DIR = DATA_DIR / 'outputs'
SCHEMA_PATH = APP_ROOT / 'schemas' / 'actioncard.schema.json'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

DEFAULT_MONTHS = 8
SEED = 42
random.seed(SEED); np.random.seed(SEED)

_SCHEMA_CACHE = None
_SCHEMA_VALIDATOR = None
AGENT2_PROMPT_TRACE: dict = {}
AGENT2_RESPONSE_TRACE: dict = {}


def tick():
    return perf_counter()


def to_ms(t0):
    return int((perf_counter() - t0) * 1000)


def _env_flag(name: str, default: str) -> str:
    value = os.getenv(name)
    return value if value is not None else default


def _strip_code_fences(text: str) -> str:
    if not text:
        return ""
    stripped = text.strip()
    if stripped.startswith("```"):
        lines = stripped.splitlines()
        if lines:
            lines = lines[1:]
        if lines and lines[-1].strip().startswith("```"):
            lines = lines[:-1]
        stripped = "\n".join(lines)
    return stripped.strip()


def _json_error_window(text: str, exc: Exception, radius: int = 120) -> str:
    pos = getattr(exc, "pos", None)
    if pos is None and isinstance(exc, json.JSONDecodeError):
        pos = exc.pos
    if pos is None:
        return ""
    start = max(pos - radius, 0)
    end = min(len(text), pos + radius)
    return text[start:end]


def _extract_json_candidate(text: str) -> tuple[str | None, int | None]:
    if not text:
        return None, None
    cleaned = _strip_code_fences(text)
    start = cleaned.find("{")
    while start != -1:
        depth = 0
        for idx in range(start, len(cleaned)):
            ch = cleaned[idx]
            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    snippet = cleaned[start : idx + 1]
                    return snippet, start
        start = cleaned.find("{", start + 1)
    return None, None


def _apply_json_repairs(text: str) -> list[str]:
    candidates: list[str] = []
    if not text:
        return candidates
    base = text.strip()
    seen = set()

    def _push(value: str):
        if value and value not in seen:
            seen.add(value)
            candidates.append(value)

    _push(base)
    repaired = re.sub(r",\s*(\]|\})", r"\1", base)
    _push(repaired)
    repaired = re.sub(r"'", '"', repaired)
    repaired = re.sub(r"\bNaN\b", "null", repaired)
    repaired = re.sub(r"\bInfinity\b", "null", repaired)
    repaired = re.sub(r"\b-?inf\b", "null", repaired, flags=re.IGNORECASE)
    _push(repaired)
    return candidates


def _attempt_literal_eval(text: str):
    try:
        value = ast.literal_eval(text)
    except Exception:
        return None
    try:
        return json.loads(json.dumps(value))
    except Exception:
        return None


def llm_json_safe_parse(text: str, schema_validator: Draft7Validator | None = None):
    logs: list[dict] = []
    if text is None:
        logs.append({"pass": "strict", "success": False, "error": "empty_response"})
        return None, logs

    cleaned = text.strip()
    if not cleaned:
        logs.append({"pass": "strict", "success": False, "error": "empty_response"})
        return None, logs

    def _validate(obj):
        if schema_validator is not None:
            schema_validator.validate(obj)

    # Strict pass ---------------------------------------------------------
    try:
        parsed = json.loads(cleaned)
        _validate(parsed)
        logs.append({"pass": "strict", "success": True})
        return parsed, logs
    except Exception as exc:
        entry = {"pass": "strict", "success": False, "error": str(exc)}
        if isinstance(exc, json.JSONDecodeError):
            entry.update({"line": exc.lineno, "col": exc.colno, "excerpt": _json_error_window(cleaned, exc)})
        logs.append(entry)

    # Extract pass --------------------------------------------------------
    snippet, offset = _extract_json_candidate(cleaned)
    if snippet:
        try:
            parsed = json.loads(snippet)
            _validate(parsed)
            logs.append({"pass": "extract", "success": True, "offset": offset})
            return parsed, logs
        except Exception as exc:
            entry = {"pass": "extract", "success": False, "error": str(exc), "offset": offset}
            if isinstance(exc, json.JSONDecodeError):
                entry.update({"line": exc.lineno, "col": exc.colno, "excerpt": _json_error_window(snippet, exc)})
            logs.append(entry)
    else:
        logs.append({"pass": "extract", "success": False, "error": "candidate_not_found"})

    candidate = snippet or cleaned

    # Repair pass ---------------------------------------------------------
    for idx, variant in enumerate(_apply_json_repairs(candidate)):
        try:
            parsed = json.loads(variant)
            _validate(parsed)
            logs.append({"pass": "repair", "success": True, "variant": idx})
            return parsed, logs
        except Exception as exc:
            entry = {"pass": "repair", "success": False, "variant": idx, "error": str(exc)}
            if isinstance(exc, json.JSONDecodeError):
                entry.update({"line": exc.lineno, "col": exc.colno, "excerpt": _json_error_window(variant, exc)})
            logs.append(entry)

    literal_candidate = _attempt_literal_eval(candidate)
    if literal_candidate is not None:
        try:
            _validate(literal_candidate)
            logs.append({"pass": "literal_eval", "success": True})
            return literal_candidate, logs
        except Exception as exc:
            logs.append({"pass": "literal_eval", "success": False, "error": str(exc)})

    return None, logs


def _structured_value(agent1_json: dict | None, key: str) -> tuple[float | None, str | None]:
    agent1_json = agent1_json or {}
    debug = agent1_json.get("debug") or {}
    snapshot = debug.get("snapshot") or {}
    sanitized = snapshot.get("sanitized") or {}
    kpis = agent1_json.get("kpis") or {}
    raw_value = sanitized.get(key, kpis.get(key))
    value, _ = to_float_pct(raw_value)
    return value, sanitized.get("latest_ta_ym") or debug.get("panel", {}).get("latest_ta_ym")


def _structured_evidence_entry(
    key: str,
    value: float | None,
    *,
    period: str | None,
    snippet: str | None = None,
) -> dict:
    if value is None:
        return {
            "source": "STRUCTURED",
            "key": key or "Í∑ºÍ±∞ ÏóÜÏùå",
            "value": "‚Äî",
            "period": period or "‚Äî",
            "snippet": snippet or "Í∑ºÍ±∞ ÏóÜÏùå",
            "doc_id": None,
            "chunk_id": None,
            "score": None,
        }
    display_value = f"{value:.1f}%" if isinstance(value, (int, float)) else value
    return {
        "source": "STRUCTURED",
        "key": key,
        "value": display_value,
        "period": period or "recent_8m",
        "snippet": snippet or display_value,
        "doc_id": None,
        "chunk_id": None,
        "score": None,
    }


def _structured_fallback_cards(agent1_json: dict | None, question_type: str | None) -> dict:
    question_type = question_type or "GENERIC"
    age_records = merge_age_buckets(agent1_json or {})
    top_age = age_records[0] if age_records else None
    top_age_label = top_age.get("label") if top_age else "ÌïµÏã¨ Í≥†Í∞ùÏ∏µ"
    top_age_value = top_age.get("value") if top_age else None
    top_age_key = top_age.get("key") if top_age else "age_distribution"

    flow_pct, period_hint = _structured_value(agent1_json, "flow_pct")
    resident_pct, _ = _structured_value(agent1_json, "resident_pct")
    work_pct, _ = _structured_value(agent1_json, "work_pct")
    new_pct, _ = _structured_value(agent1_json, "new_pct")
    revisit_pct, _ = _structured_value(agent1_json, "revisit_pct")

    if period_hint is None:
        panel = ((agent1_json or {}).get("debug") or {}).get("panel") or {}
        period_hint = panel.get("latest_ta_ym") or "recent_8m"

    def _make_card(
        idea_title: str,
        audience: str,
        channels: list[str],
        execution: list[str],
        copy_samples: list[str],
        measurement: list[str],
        evidence_specs: list[dict],
    ) -> dict:
        evidence = [item for item in evidence_specs if item]
        if not evidence:
            evidence = [
                _structured_evidence_entry(
                    "Í∑ºÍ±∞ ÏóÜÏùå",
                    None,
                    period=period_hint,
                    snippet="Í∑ºÍ±∞ ÏóÜÏùå",
                )
            ]
        return {
            "idea_title": idea_title,
            "audience": audience,
            "channels": channels,
            "execution": execution,
            "copy_samples": copy_samples,
            "measurement": measurement,
            "evidence": evidence,
        }

    age_evidence = _structured_evidence_entry(
        f"age_distribution.{top_age_key}",
        top_age_value,
        period=period_hint,
        snippet=f"{top_age_label} {top_age_value:.1f}%" if isinstance(top_age_value, (int, float)) else None,
    ) if top_age_value is not None else None
    flow_evidence = _structured_evidence_entry(
        "customer_mix_detail.Ïú†Îèô",
        flow_pct,
        period=period_hint,
        snippet=f"Ïú†Îèô {flow_pct:.1f}%" if isinstance(flow_pct, (int, float)) else None,
    ) if flow_pct is not None else None
    resident_evidence = _structured_evidence_entry(
        "customer_mix_detail.Í±∞Ï£º",
        resident_pct,
        period=period_hint,
        snippet=f"Í±∞Ï£º {resident_pct:.1f}%" if isinstance(resident_pct, (int, float)) else None,
    ) if resident_pct is not None else None
    work_evidence = _structured_evidence_entry(
        "customer_mix_detail.ÏßÅÏû•",
        work_pct,
        period=period_hint,
        snippet=f"ÏßÅÏû• {work_pct:.1f}%" if isinstance(work_pct, (int, float)) else None,
    ) if work_pct is not None else None
    new_evidence = _structured_evidence_entry(
        "new_pct",
        new_pct,
        period=period_hint,
        snippet=f"Ïã†Í∑ú {new_pct:.1f}%" if isinstance(new_pct, (int, float)) else None,
    ) if new_pct is not None else None
    revisit_evidence = _structured_evidence_entry(
        "revisit_pct",
        revisit_pct,
        period=period_hint,
        snippet=f"Ïû¨Î∞©Î¨∏ {revisit_pct:.1f}%" if isinstance(revisit_pct, (int, float)) else None,
    ) if revisit_pct is not None else None

    cards: list[dict] = []

    if question_type == "Q2_LOW_RETENTION":
        cards.append(
            _make_card(
                "Îã®Í≥® ÌöåÏàò Î©îÏãúÏßÄ Ï∫†ÌéòÏù∏",
                top_age_label,
                ["Ïπ¥Ïπ¥Ïò§ÌÜ° Ï±ÑÎÑê", "SMS"],
                [
                    "Ïû¨Î∞©Î¨∏ Í≥†Í∞ùÏóêÍ≤å Ïù¥ÌÉà Í∏∞Í∞ÑÎ≥Ñ Î¶¨ÎßàÏù∏Îìú Î©îÏãúÏßÄÎ•º Î∞úÏÜ°Ìï©ÎãàÎã§.",
                    "Î©§Î≤ÑÏã≠ Ïø†Ìè∞ÏùÑ Ïû¨Î∞úÍ∏âÌï¥ Ïû¨Î∞©Î¨∏ Ïú†Ïù∏ÏùÑ ÎßåÎì≠ÎãàÎã§.",
                ],
                [
                    f"Îã§Ïãú Ï∞æÏïÑÏ£ºÏãúÎ©¥ {top_age_label} Ï†ÑÏö© ÌòúÌÉùÏùÑ Ï§ÄÎπÑÌñàÏñ¥Ïöî!",
                    "Ïù¥Î≤à Ï£º ÏïàÏóê Î∞©Î¨∏ Ïãú Ï∂îÍ∞Ä Î¶¨ÌïÑÏùÑ ÎìúÎ¶ΩÎãàÎã§.",
                ],
                ["Ïû¨Î∞©Î¨∏ Í≥†Í∞ù Ïàò", "Ïø†Ìè∞ Ïû¨ÏÇ¨Ïö©Î•†"],
                [revisit_evidence or new_evidence, age_evidence],
            )
        )
        cards.append(
            _make_card(
                "Ïä§ÌÉ¨ÌîÑ Ï†ÅÎ¶Ω ÌîÑÎ°úÎ™®ÏÖò",
                "Ïû¨Î∞©Î¨∏ Ïú†ÎèÑ Í≥†Í∞ù",
                ["ÌòÑÏû• POP", "Î©§Î≤ÑÏã≠ Ïï±"],
                [
                    "ÌòÑÏû• Ïä§ÌÉ¨ÌîÑÌåêÏùÑ ÎπÑÏπòÌïòÍ≥† 3Ìöå Î∞©Î¨∏ Ïãú Î¨¥Î£å Î©îÎâ¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§.",
                    "Ïï±ÏóêÏÑú Ïã§ÏãúÍ∞Ñ Ï†ÅÎ¶Ω ÌòÑÌô©ÏùÑ Î≥¥Ïó¨Ï§çÎãàÎã§.",
                ],
                [
                    "Ïä§ÌÉ¨ÌîÑ 3Í∞ú Î™®ÏúºÎ©¥ ÏïÑÎ©îÎ¶¨Ïπ¥ÎÖ∏ Î¨¥Î£å!",
                    "Ïù¥Î≤à Ï£º Î©§Î≤ÑÏã≠ Ï†ÅÎ¶ΩÎ•† TOP Í≥†Í∞ù Í≥µÏßÄ",
                ],
                ["Ïä§ÌÉ¨ÌîÑ Ï†ÅÎ¶ΩÎ•†", "ÌòúÌÉù ÍµêÌôò Í±¥Ïàò"],
                [new_evidence, revisit_evidence],
            )
        )
        cards.append(
            _make_card(
                "ÌòÑÏû• Ï≤¥Î•ò Í≤ΩÌóò Í∞ïÌôî",
                "Ï†êÏã¨/Ìá¥Í∑º Í≥†Í∞ù",
                ["Ïò§ÌîÑÎùºÏù∏ Ïù¥Î≤§Ìä∏", "ÏßÄÏó≠ Ïª§ÎÆ§ÎãàÌã∞"],
                [
                    "Ï†êÏã¨ÏãúÍ∞Ñ ÌïúÏ†ï ÏãúÏãù/ÏãúÏó∞ Ïù¥Î≤§Ìä∏Î•º Í∏∞ÌöçÌï©ÎãàÎã§.",
                    "ÏßÄÏó≠ Ïª§ÎÆ§ÎãàÌã∞Ïóê Î¶¨Î∑∞ Ïù∏Ï¶ù Ïãú ÌòúÌÉùÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.",
                ],
                [
                    "Ï†êÏã¨ Ïù∏Ï¶ùÏÉ∑ ÏóÖÎ°úÎìú Ïãú ÎîîÏ†ÄÌä∏ Ï¶ùÏ†ï",
                    "Ìá¥Í∑ºÍ∏∏Ïóê Îì§Î•¥Î©¥ ÎßàÍ∞ê Ìï†Ïù∏ Ï†úÍ≥µ",
                ],
                ["Ïù¥Î≤§Ìä∏ Ï∞∏Ïó¨Ïûê Ïàò", "Î¶¨Î∑∞ Ï¶ùÍ∞ÄÎüâ"],
                [flow_evidence, resident_evidence or work_evidence],
            )
        )
    elif question_type == "Q3_FOOD_ISSUE":
        cards.append(
            _make_card(
                "ÏãúÍ∑∏ÎãàÏ≤ò Î©îÎâ¥ ÏßëÏ§ë ÌôçÎ≥¥",
                top_age_label,
                ["SNS", "ÏßÄÎèÑ Î¶¨Î∑∞"],
                [
                    "ÎåÄÌëú Î©îÎâ¥ Ï°∞Î¶¨ Í≥ºÏ†ïÏùÑ ÏßßÏùÄ ÏòÅÏÉÅÏúºÎ°ú Ï†úÏûëÌï©ÎãàÎã§.",
                    "ÏßÄÎèÑ Î¶¨Î∑∞Ïóê Îßõ/ÏúÑÏÉù ÌÇ§ÏõåÎìú ÏùëÎãµÏùÑ ÏÉÅÏãú Í¥ÄÎ¶¨Ìï©ÎãàÎã§.",
                ],
                [
                    f"{top_age_label} ÏÜêÎãòÏù¥ Ï¢ãÏïÑÌïú ÏãúÍ∑∏ÎãàÏ≤ò Î†àÏãúÌîº Í≥µÍ∞ú",
                    "Ïò§Îäò ÎßåÎì† Ïã†ÏÑ†Ìïú ÏõêÎëê/Ïû¨Î£å Í∞ïÏ°∞",
                ],
                ["ÏòÅÏÉÅ Ï°∞ÌöåÏàò", "Í∏çÏ†ï Î¶¨Î∑∞ ÎπÑÏ§ë"],
                [age_evidence, flow_evidence or resident_evidence],
            )
        )
        cards.append(
            _make_card(
                "ÌîºÌÅ¨ÌÉÄÏûÑ ÌöåÏ†ÑÏú® Í∞úÏÑ†",
                "Ï†êÏã¨ ÌîºÌÅ¨ Í≥†Í∞ù",
                ["ÌòÑÏû• ÏïàÎÇ¥", "ÌÖåÏù¥Î∏î Ïò§Îçî"],
                [
                    "ÌîºÌÅ¨ ÏãúÍ∞Ñ ÏÇ¨Ï†Ñ Ï£ºÎ¨∏/ÌîΩÏóÖ ÎùºÏù∏ÏùÑ Ïö¥ÏòÅÌï©ÎãàÎã§.",
                    "ÌòºÏû°ÎèÑÎ•º ÌòÑÏû• ÏïàÎÇ¥ÌåêÏúºÎ°ú Í≥µÏßÄÌï©ÎãàÎã§.",
                ],
                [
                    "Ï†êÏã¨ Ï†Ñ ÎØ∏Î¶¨ Ï£ºÎ¨∏ÌïòÎ©¥ ÎåÄÍ∏∞ ÏóÜÏù¥ ÌîΩÏóÖ!",
                    "ÌÖåÏù¥Î∏îÏóêÏÑú QR Ï£ºÎ¨∏ÌïòÍ≥† Î∞îÎ°ú Î∞õÏïÑÍ∞ÄÏÑ∏Ïöî",
                ],
                ["ÌÖåÏù¥Î∏î ÌöåÏ†Ñ ÏãúÍ∞Ñ", "ÌîΩÏóÖ ÏÇ¨Ï†Ñ Ï£ºÎ¨∏ ÎπÑÏ§ë"],
                [work_evidence, flow_evidence],
            )
        )
        cards.append(
            _make_card(
                "Î¶¨Î∑∞ Í∏∞Î∞ò ÌíàÏßà Î≥¥ÏôÑ",
                "Ïû¨Î∞©Î¨∏ Ïû†Ïû¨ Í≥†Í∞ù",
                ["Î¶¨Î∑∞ DM", "Î©§Î≤ÑÏã≠"],
                [
                    "Î¶¨Î∑∞ÏóêÏÑú Ï†úÍ∏∞Îêú Îßõ/ÏÑúÎπÑÏä§ Ïù¥ÏäàÎ•º Ï†ïÎ¶¨ÌïòÍ≥† Í∞úÏÑ† Í≥µÏßÄÎ•º Î≥¥ÎÉÖÎãàÎã§.",
                    "Í∞úÏÑ† ÌõÑ Ïû¨Î∞©Î¨∏ Í≥†Í∞ùÏóêÍ≤å Î≥¥ÏÉÅ Ïø†Ìè∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.",
                ],
                [
                    "Î¶¨Î∑∞ ÎÇ®Í≤®Ï£ºÏã† ÏùòÍ≤¨ÏùÑ Î∞òÏòÅÌï¥ Î†àÏãúÌîºÎ•º ÏÉàÎ°ú ÏÜêÎ¥§ÏäµÎãàÎã§.",
                    "Ïû¨Î∞©Î¨∏ Ïãú Î¨¥Î£å ÌÜ†ÌïëÏùÑ ÎìúÎ¶ΩÎãàÎã§.",
                ],
                ["Î¶¨Î∑∞ ÏùëÎãµÎ•†", "Ïû¨Î∞©Î¨∏ Í≥†Í∞ù Ïàò"],
                [revisit_evidence or new_evidence, resident_evidence],
            )
        )
    else:  # Q1_CAFE_CHANNELS or generic
        cards.append(
            _make_card(
                "ÌïµÏã¨ Ïó∞Î†πÎåÄ SNS ÏßëÏ§ë Í¥ëÍ≥†",
                top_age_label,
                ["Ïù∏Ïä§ÌÉÄÍ∑∏Îû®", "ÎÑ§Ïù¥Î≤Ñ ÌîåÎ†àÏù¥Ïä§"],
                [
                    "Ïó∞Î†πÎåÄ Í¥ÄÏã¨ÏÇ¨Ïóê ÎßûÏ∂ò ÏàèÌèº ÏΩòÌÖêÏ∏†Î•º Ï£º 3Ìöå Í≤åÏãúÌï©ÎãàÎã§.",
                    "ÌîåÎ†àÏù¥Ïä§ Î¶¨Î∑∞ ÏÉÅÎã®Ïóê ÏãúÏ¶å ÌïúÏ†ï Î©îÎâ¥Î•º ÎÖ∏Ï∂úÌï©ÎãàÎã§.",
                ],
                [
                    f"#{top_age_label} Ï∑®Ìñ• Ï†ÄÍ≤© Ïã†Î©îÎâ¥ Í≥µÍ∞ú",
                    "Î∞©Î¨∏ Ïù∏Ï¶ù Ïãú ÏùåÎ£å 1+1",
                ],
                ["SNS Ï∞∏Ïó¨Ïú®", "ÌîåÎ†àÏù¥Ïä§ ÌÅ¥Î¶≠Ïàò"],
                [age_evidence, flow_evidence],
            )
        )
        cards.append(
            _make_card(
                "Ïò§ÌîºÏä§ ÌÉÄÍ≤ü Ïø†Ìè∞ Ï†úÌú¥",
                "ÏßÅÏû•Ïù∏ Í≥†Í∞ù",
                ["ÌöåÏÇ¨ Ï†úÌú¥", "Î∞∞Îã¨Ïï± Í¥ëÍ≥†"],
                [
                    "Í∑ºÏ≤ò Ïò§ÌîºÏä§ÏôÄ Ï†úÌú¥Ìï¥ Ï†êÏã¨ ÏãúÍ∞Ñ Ìï†Ïù∏ Ïø†Ìè∞ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.",
                    "Î∞∞Îã¨Ïï± Ìôà Î∞∞ÎÑàÏóê Ïò§ÌîºÏä§ ÌÉÄÍ≤ü Ï†ÑÏö© Î©îÎâ¥Î•º ÎÖ∏Ï∂úÌï©ÎãàÎã§.",
                ],
                [
                    "Ï†êÏã¨ 11-14Ïãú Ï†ÑÏö© 2,000Ïõê Ìï†Ïù∏ Ïø†Ìè∞",
                    "ÌöåÏÇ¨ Ïù¥Î©îÏùº Ïù∏Ï¶ù Ïãú Ï∂îÍ∞Ä Ï†ÅÎ¶Ω",
                ],
                ["Ïø†Ìè∞ ÏÇ¨Ïö©Î•†", "Ï†êÏã¨ Îß§Ï∂ú"],
                [work_evidence, resident_evidence or flow_evidence],
            )
        )
        cards.append(
            _make_card(
                "Îã®Í≥® ÌôïÎ≥¥ Î©§Î≤ÑÏã≠ Î©îÏãúÏßÄ",
                "Ïû¨Î∞©Î¨∏ Ïû†Ïû¨ Í≥†Í∞ù",
                ["Î©§Î≤ÑÏã≠ Ïï±", "Ïπ¥Ïπ¥Ïò§ÌÜ° Ï±ÑÎÑê"],
                [
                    "Ïû¨Î∞©Î¨∏Ïú®Ïù¥ ÎÇÆÏùÄ Í≥†Í∞ùÏóêÍ≤å Ï£ºÎßê ÌïúÏ†ï ÌòúÌÉùÏùÑ Î≥¥ÎÉÖÎãàÎã§.",
                    "Î©§Î≤ÑÏã≠ Ìè¨Ïù∏Ìä∏Î°ú ÏãúÏ¶å ÍµøÏ¶à ÍµêÌôò Ïù¥Î≤§Ìä∏Î•º ÏïåÎ¶ΩÎãàÎã§.",
                ],
                [
                    "Ïù¥Î≤à Ï£ºÎßê Ïû¨Î∞©Î¨∏ Ïãú Ìè¨Ïù∏Ìä∏ 2Î∞∞ Ï†ÅÎ¶Ω",
                    "ÍµøÏ¶à ÌïúÏ†ï ÏàòÎüâ ÏòàÏïΩ ÎßÅÌÅ¨",
                ],
                ["Ïû¨Î∞©Î¨∏ Í≥†Í∞ù Ïàò", "Î©§Î≤ÑÏã≠ ÌôúÏÑ±ÌôîÏú®"],
                [revisit_evidence, new_evidence],
            )
        )

    return {"answers": cards[:4]}

USE_LLM = _env_flag("AGENT1_USE_LLM", "true").lower() not in {"0", "false", "no"}
DEBUG_MAX_PREVIEW = int(_env_flag("DEBUG_MAX_PREVIEW", "200") or 200)
DEBUG_SHOW_RAW = _env_flag("DEBUG_SHOW_RAW", "true").lower() in {"1", "true", "yes"}


def _mask_debug_preview(text: str | None, limit: int = DEBUG_MAX_PREVIEW) -> str:
    if not text:
        return ""
    masked = re.sub(r"\{[^{}]*\}", "{***}", str(text))
    masked = re.sub(r"([A-Za-z0-9]{4})[A-Za-z0-9]{4,}", r"\1***", masked)
    return masked[:limit]


def _normalize_str(value: str) -> str:
    if value is None:
        return ""
    text = unicodedata.normalize("NFKC", str(value))
    return re.sub(r"\s+", "", text)


def _normalize_compare(value: str | None) -> str:
    if value is None:
        return ""
    text = unicodedata.normalize("NFKC", str(value))
    text = re.sub(r"\s+", "", text)
    return text.upper()


def _wildcard_to_regex(masked: str | None) -> re.Pattern | None:
    if not masked:
        return None
    normalized = _normalize_str(masked)
    if not normalized:
        return None
    pattern = "".join(".*" if ch == "*" else re.escape(ch) for ch in normalized)
    try:
        return re.compile(f"^{pattern}")
    except re.error:
        return None

def load_actioncard_schema_current():
    global _SCHEMA_CACHE, _SCHEMA_VALIDATOR
    if _SCHEMA_CACHE is not None and _SCHEMA_VALIDATOR is not None:
        return _SCHEMA_CACHE, _SCHEMA_VALIDATOR
    if not SCHEMA_PATH.exists():
        raise FileNotFoundError(f"Ïä§ÌÇ§Îßà ÌååÏùºÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {SCHEMA_PATH}")
    with open(SCHEMA_PATH, 'r', encoding='utf-8') as f:
        _SCHEMA_CACHE = json.load(f)

    if isinstance(_SCHEMA_CACHE, dict) and "schemas" in _SCHEMA_CACHE:
        validators: dict[str, Draft7Validator] = {}
        for key, schema_obj in _SCHEMA_CACHE.get("schemas", {}).items():
            try:
                validators[key] = Draft7Validator(schema_obj)
            except Exception:
                continue
        _SCHEMA_VALIDATOR = validators
    else:
        _SCHEMA_VALIDATOR = Draft7Validator(_SCHEMA_CACHE)
    return _SCHEMA_CACHE, _SCHEMA_VALIDATOR

def read_csv_smart(path):
    for enc in ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1']:
        try:
            return pd.read_csv(path, encoding=enc)
        except Exception:
            continue
    return pd.read_csv(path, encoding='utf-8', errors='replace')


def normalize_rate_series(series: pd.Series | None) -> pd.Series | None:
    if series is None:
        return series
    cleaned = (
        series.astype(str)
        .str.replace(",", "", regex=False)
        .str.replace("%", "", regex=False)
        .str.replace("‚àí", "-", regex=False)
    )
    cleaned = cleaned.str.replace(r"[^0-9\-.]", "", regex=True)
    numeric = pd.to_numeric(cleaned, errors='coerce')
    if numeric is None:
        return None
    scaled = numeric.copy()
    mask = scaled.notna() & (scaled.abs() <= 1)
    scaled.loc[mask] = scaled.loc[mask] * 100
    scaled.loc[(scaled < 0) | (scaled > 100)] = np.nan
    return scaled

def ym_to_date(ym_series):
    s = pd.to_datetime(ym_series.astype(str) + '01', format='%Y%m%d', errors='coerce')
    return s

def load_set1(shinhan_dir):
    p = shinhan_dir / 'big_data_set1_f.csv'
    df = read_csv_smart(p)
    ren = {}
    for c in df.columns:
        cu = str(c).upper()
        if cu == 'ENCODED_MCT': ren[c] = 'ENCODED_MCT'
        elif 'SIGUNGU' in cu:   ren[c] = 'SIGUNGU'
        elif 'BSE_AR' in cu:    ren[c] = 'ADDR_BASE'
        elif ('ZCD' in cu) or ('BZN' in cu) or ('ÏóÖÏ¢Ö' in cu): ren[c] = 'CATEGORY'
        elif cu == 'MCT_NM':    ren[c] = 'MCT_NM'
    df = df.rename(columns=ren)
    df = df.loc[:, ~df.columns.duplicated()]
    keep = ['ENCODED_MCT','MCT_NM','ADDR_BASE','SIGUNGU','CATEGORY']
    for k in keep:
        if k not in df.columns: df[k] = np.nan
    df = df[keep].drop_duplicates('ENCODED_MCT')
    if 'ENCODED_MCT' in df.columns:
        df['ENCODED_MCT'] = df['ENCODED_MCT'].apply(lambda v: str(v).strip() if pd.notna(v) else '')
    return df

def load_set2(shinhan_dir):
    p = shinhan_dir / 'big_data_set2_f.csv'
    df = read_csv_smart(p)
    df['TA_YM'] = df['TA_YM'].astype(str)
    df['_date'] = ym_to_date(df['TA_YM'])
    return df

def load_set3(shinhan_dir):
    p = shinhan_dir / 'big_data_set3_f.csv'
    df = read_csv_smart(p)
    df['TA_YM'] = df['TA_YM'].astype(str)
    df['_date'] = ym_to_date(df['TA_YM'])
    keep_cols = [
        'ENCODED_MCT','TA_YM','_date',
        'M12_MAL_1020_RAT','M12_MAL_30_RAT','M12_MAL_40_RAT','M12_MAL_50_RAT','M12_MAL_60_RAT',
        'M12_FME_1020_RAT','M12_FME_30_RAT','M12_FME_40_RAT','M12_FME_50_RAT','M12_FME_60_RAT',
        'MCT_UE_CLN_REU_RAT','MCT_UE_CLN_NEW_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT','RC_M1_SHC_WP_UE_CLN_RAT','RC_M1_SHC_FLP_UE_CLN_RAT',
        'APV_CE_RAT'
    ]
    for c in keep_cols:
        if c not in df.columns:
            df[c] = np.nan
    df = df[keep_cols]
    rate_cols = [
        'M12_MAL_1020_RAT','M12_MAL_30_RAT','M12_MAL_40_RAT','M12_MAL_50_RAT','M12_MAL_60_RAT',
        'M12_FME_1020_RAT','M12_FME_30_RAT','M12_FME_40_RAT','M12_FME_50_RAT','M12_FME_60_RAT',
        'MCT_UE_CLN_REU_RAT','MCT_UE_CLN_NEW_RAT',
        'RC_M1_SHC_RSD_UE_CLN_RAT','RC_M1_SHC_WP_UE_CLN_RAT','RC_M1_SHC_FLP_UE_CLN_RAT'
    ]
    for col in rate_cols:
        if col in df.columns:
            original = df[col].copy()
            df[f'{col}_raw'] = original
            df[col] = normalize_rate_series(original)
    return df

def load_weather_monthly(external_dir):
    f = None
    for e in ('.csv','.parquet','.parq','.feather'):
        cand = list(external_dir.glob(f'**/*{e}'))
        if cand:
            f = cand[0]; break
    if not f:
        print('‚ö†Ô∏è Ïô∏Î∂Ä(ÎÇ†Ïî®) Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§. ÎÇ†Ïî® Î∂ÑÏÑùÏùÄ Ï†úÌïúÎê©ÎãàÎã§.')
        return None
    if f.suffix.lower() == '.csv':
        wx = read_csv_smart(f)
    elif f.suffix.lower() in ('.parquet','.parq'):
        wx = pd.read_parquet(f)
    elif f.suffix.lower() == '.feather':
        wx = pd.read_feather(f)
    else:
        return None

    c_dt = None
    for c in wx.columns:
        cl = str(c).lower()
        if any(k in cl for k in ['date','ymd','dt','ÏùºÏûê','ÎÇ†Ïßú','yyyymm']):
            c_dt = c; break
    if c_dt is None:
        raise ValueError('ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞Ïóê ÎÇ†Ïßú(ÎòêÎäî YYYYMM) Ïª¨ÎüºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
    dt = pd.to_datetime(wx[c_dt].astype(str), errors='coerce')
    wx['_ym'] = dt.dt.strftime('%Y%m')
    c_rain = None
    for c in wx.columns:
        cl = c.lower()
        if any(k in cl for k in ['rain','precip','rn_mm','rainfall','rr','Í∞ïÏàò','Í∞ïÏàòÎüâ','ÎπÑ']):
            c_rain = c; break
    if c_rain is None:
        wx['_rain_val'] = 0.0
    else:
        wx['_rain_val'] = pd.to_numeric(wx[c_rain], errors='coerce').fillna(0.0)

    monthly = wx.groupby('_ym', as_index=False)['_rain_val'].sum().rename(columns={'_ym':'TA_YM','_rain_val':'RAIN_SUM'})
    monthly['TA_YM'] = monthly['TA_YM'].astype(str)
    monthly['_date'] = pd.to_datetime(monthly['TA_YM'] + '01', format='%Y%m%d', errors='coerce')
    return monthly[['TA_YM','_date','RAIN_SUM']]


def _format_percent_debug(value):
    if value is None:
        return None
    try:
        num = float(value)
    except (TypeError, ValueError):
        return None
    if num < 0 or num > 100:
        return None
    return round(num, 2)


def _format_percent_text(value):
    pct = _format_percent_debug(value)
    if pct is None:
        return '‚Äî'
    return f"{pct:.1f}%"


def _format_customer_mix_debug(detail):
    if not isinstance(detail, dict):
        return '‚Äî'
    ordered_labels = ['Ïú†Îèô', 'Í±∞Ï£º', 'ÏßÅÏû•']
    parts = []
    for label in ordered_labels:
        pct = _format_percent_text(detail.get(label))
        if pct != '‚Äî':
            parts.append(f"{label} {pct}")
    for label, value in detail.items():
        if label in ordered_labels:
            continue
        pct = _format_percent_text(value)
        if pct != '‚Äî':
            parts.append(f"{label} {pct}")
    return ', '.join(parts[:3]) if parts else '‚Äî'


def _format_age_segments_debug(segments):
    if not isinstance(segments, (list, tuple)):
        return '‚Äî'
    formatted = []
    for seg in segments:
        if not isinstance(seg, dict):
            continue
        label = seg.get('label') or seg.get('code')
        value = _format_percent_text(seg.get('value'))
        if label and value != '‚Äî':
            formatted.append(f"{label} {value}")
    return ', '.join(formatted[:3]) if formatted else '‚Äî'


def _build_debug_table(qinfo, merchant_match, sanitized_snapshot):
    industry_candidate = None
    if merchant_match:
        industry_candidate = merchant_match.get('category')
    if not industry_candidate:
        industry_candidate = qinfo.get('merchant_industry_label') or qinfo.get('industry')
    industry_labels = {
        'cafe': 'Ïπ¥Ìéò',
        'restaurant': 'ÏùåÏãùÏ†ê',
        'retail': 'ÏÜåÎß§',
    }
    industry = industry_labels.get(industry_candidate, industry_candidate or '‚Äî')

    address = '‚Äî'
    if merchant_match:
        addr = merchant_match.get('address')
        if isinstance(addr, (list, tuple)):
            addr = ' / '.join([str(v) for v in addr if v])
        if addr:
            address = str(addr)

    revisit = _format_percent_text((sanitized_snapshot or {}).get('revisit_pct'))
    new = _format_percent_text((sanitized_snapshot or {}).get('new_pct'))
    revisit_block = '‚Äî'
    if revisit != '‚Äî' or new != '‚Äî':
        revisit_block = f"Ïã†Í∑ú {new} / Ïû¨Î∞©Î¨∏ {revisit}"

    table = {
        'ÏóÖÏ¢Ö': industry,
        'Ï£ºÏÜå': address,
        'Ï£ºÏöî Í≥†Í∞ùÏ∏µ': _format_age_segments_debug((sanitized_snapshot or {}).get('age_top_segments')),
        'Í≥†Í∞ù Ïú†Ìòï': _format_customer_mix_debug((sanitized_snapshot or {}).get('customer_mix_detail')),
        'Ïã†Í∑ú/Ïû¨Î∞©Î¨∏': revisit_block,
        'Í∞ùÎã®Í∞Ä Íµ¨Í∞Ñ': (sanitized_snapshot or {}).get('avg_ticket_band_label') or '‚Äî',
    }
    return table

def build_panel(shinhan_dir, merchants_df=None, target_id=None):
    s1 = merchants_df if merchants_df is not None else load_set1(shinhan_dir)
    s2_all = load_set2(shinhan_dir)
    s3_all = load_set3(shinhan_dir)

    stats = {
        'set2_merchants_before': int(s2_all['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s2_all.columns else 0,
        'set3_merchants_before': int(s3_all['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s3_all.columns else 0,
    }

    s2 = s2_all
    s3 = s3_all
    if target_id:
        tid = str(target_id)
        s2 = s2_all[s2_all['ENCODED_MCT'].astype(str) == tid]
        s3 = s3_all[s3_all['ENCODED_MCT'].astype(str) == tid]

    stats['set2_rows_after'] = int(len(s2))
    stats['set3_rows_after'] = int(len(s3))

    m23 = pd.merge(s2, s3, on=['ENCODED_MCT','TA_YM','_date'], how='outer', suffixes=('_s2',''))
    panel = pd.merge(m23, s1, on='ENCODED_MCT', how='left')
    def nz(x):
        try:
            return pd.to_numeric(x, errors='coerce').fillna(0.0)
        except Exception:
            return pd.Series([0.0] * len(x))
    if 'M12_MAL_1020_RAT' in panel.columns and 'M12_FME_1020_RAT' in panel.columns:
        panel['YOUTH_SHARE'] = nz(panel['M12_MAL_1020_RAT']) + nz(panel['M12_FME_1020_RAT'])
    else:
        panel['YOUTH_SHARE'] = np.nan
    panel['REVISIT_RATE'] = pd.to_numeric(panel.get('MCT_UE_CLN_REU_RAT', np.nan), errors='coerce')
    panel['NEW_RATE'] = pd.to_numeric(panel.get('MCT_UE_CLN_NEW_RAT', np.nan), errors='coerce')
    panel.rename(columns={'ENCODED_MCT':'_merchant_id'}, inplace=True)
    if '_merchant_id' in panel.columns:
        panel['_merchant_id'] = panel['_merchant_id'].astype(str)
    stats['merchants_after'] = int(panel['_merchant_id'].nunique()) if '_merchant_id' in panel.columns else 0
    stats['set2_merchants_after'] = int(s2['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s2.columns else 0
    stats['set3_merchants_after'] = int(s3['ENCODED_MCT'].astype(str).nunique()) if 'ENCODED_MCT' in s3.columns else 0
    return panel, stats


def call_llm_for_mask(original_question: str | None, merchant_mask: str | None, sigungu: str | None):
    meta = {
        'used': False,
        'model': 'models/gemini-2.5-flash',
        'prompt_preview': '',
        'resp_bytes': 0,
        'safety_blocked': False,
        'elapsed_ms': 0,
        'error': None,
    }

    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print('‚ö†Ô∏è GEMINI_API_KEY ÎØ∏ÏÑ§Ï†ïÏúºÎ°ú LLM Î≥¥Ï°∞ Îß§Ïπ≠ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.')
        meta['error'] = 'missing_api_key'
        return None, meta
    try:
        import google.generativeai as genai
    except ImportError:
        print('‚ö†Ô∏è google-generativeai ÎØ∏ÏÑ§ÏπòÎ°ú LLM Î≥¥Ï°∞ Îß§Ïπ≠ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.')
        meta['error'] = 'missing_dependency'
        return None, meta

    genai.configure(api_key=api_key)
    model_name = meta['model']
    model = genai.GenerativeModel(
        model_name=model_name,
        generation_config={
            'temperature': 0.1,
            'top_p': 0.8,
            'max_output_tokens': 128,
        },
    )

    prompt = f"""ÎãπÏã†ÏùÄ ÌÖçÏä§Ìä∏ÏóêÏÑú ÎßàÏä§ÌÇπÎêú Í∞ÄÎßπÏ†ê Îã®ÏÑúÎ•º Ï†ïÎ¶¨ÌïòÎäî ÌïúÍµ≠Ïñ¥ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.
ÏßàÎ¨∏ÏóêÏÑú ÌôïÏù∏ Í∞ÄÎä•Ìïú ÏÉÅÌò∏ ÎßàÏä§ÌÅ¨ÏôÄ ÏãúÍµ∞Íµ¨Îßå JSONÏúºÎ°ú Îã§Ïãú Ïç®Ï£ºÏÑ∏Ïöî.
Ï∂îÏ†ïÏù¥ÎÇò ÏÉùÏÑ±ÏùÄ Í∏àÏßÄÌïòÎ©∞, Ï†ïÎ≥¥Í∞Ä ÏóÜÏúºÎ©¥ nullÏùÑ ÎÑ£ÏäµÎãàÎã§.

ÏßàÎ¨∏: {original_question}
ÌòÑÏû¨ Ï∂îÏ∂ú: merchant_mask={merchant_mask}, sigungu={sigungu}

JSON ÌòïÏãù:
{{"merchant_mask":"Î¨∏ÏûêÏó¥ ÎòêÎäî null","sigungu":"Î¨∏ÏûêÏó¥ ÎòêÎäî null","notes":"Í∞ÑÎã® Î©îÎ™®"}}
"""

    meta['prompt_preview'] = _mask_debug_preview(prompt)
    t0 = tick()

    try:
        response = model.generate_content(prompt)
    except Exception as exc:
        meta['elapsed_ms'] = to_ms(t0)
        meta['error'] = str(exc)
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ Ìò∏Ï∂ú Ïã§Ìå®:', exc)
        return None, meta

    def _response_text(resp):
        parts: list[str] = []

        def _append_text(value):
            if value:
                parts.append(str(value))

        for part in getattr(resp, 'parts', []) or []:
            _append_text(getattr(part, 'text', None))

        # google.generativeai ÏùëÎãµÏùÄ candidates[*].content.parts ÏóêÎèÑ ÌÖçÏä§Ìä∏Í∞Ä Îã¥Í∏∏ Ïàò ÏûàÎã§.
        for candidate in getattr(resp, 'candidates', []) or []:
            content = getattr(candidate, 'content', None)
            if content is None:
                continue
            for part in getattr(content, 'parts', []) or []:
                _append_text(getattr(part, 'text', None))

        if hasattr(resp, 'text'):
            try:
                quick_text = resp.text
            except ValueError:
                quick_text = None
            _append_text(quick_text)

        return '\n'.join([p for p in parts if p])

    text = _response_text(response)
    meta['elapsed_ms'] = to_ms(t0)

    prompt_feedback = getattr(response, 'prompt_feedback', None)
    block_reason = None
    if prompt_feedback is not None:
        block_reason = getattr(prompt_feedback, 'block_reason', None)
    safety_blocked = bool(block_reason and str(block_reason).lower() != 'block_none')

    if not safety_blocked:
        # ÌõÑÎ≥¥Ïùò finish_reason Ïù¥ ÏïàÏ†Ñ Ï∞®Îã®ÏùÑ ÎÇòÌÉÄÎÇ¥Î©¥ ÏïàÏ†Ñ Ï∞®Îã®ÏúºÎ°ú Í∞ÑÏ£ºÌïúÎã§.
        for candidate in getattr(response, 'candidates', []) or []:
            finish_reason = getattr(candidate, 'finish_reason', None)
            if finish_reason is None:
                continue
            fr_text = str(finish_reason).lower()
            if 'safety' in fr_text or 'blocked' in fr_text or fr_text in {'block_safety', '2'}:
                safety_blocked = True
                break

    meta['safety_blocked'] = safety_blocked

    if not text:
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ ÏùëÎãµÏù¥ ÎπÑÏóàÏäµÎãàÎã§.')
        meta['used'] = True
        meta['error'] = 'empty_text'
        return None, meta

    meta['used'] = True
    meta['resp_bytes'] = len(text.encode('utf-8'))

    match = re.search(r'\{[\s\S]*\}', text)
    if not match:
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ÏóêÏÑú JSONÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        meta['error'] = 'json_not_found'
        return None, meta

    try:
        data = json.loads(match.group(0))
    except Exception as exc:
        print('‚ö†Ô∏è LLM Î≥¥Ï°∞ Îß§Ïπ≠ JSON ÌååÏã± Ïã§Ìå®:', exc)
        meta['error'] = f'json_parse_error: {exc}'
        return None, meta

    return (data if isinstance(data, dict) else None), meta


def resolve_merchant(
    masked_name: str | None,
    mask_prefix: str | None,
    sigungu: str | None,
    merchants_df: pd.DataFrame | None,
    original_question: str | None = None,
    allow_llm: bool = True,
):
    debug_info = {
        'candidates': [],
        'path': None,
        'notes': None,
        'suggestions': None,
        'llm': None,
    }

    if merchants_df is None or merchants_df.empty:
        return None, debug_info

    if not masked_name:
        debug_info['notes'] = 'ÎßàÏä§ÌÇπ ÏÉÅÌò∏ ÎØ∏Ï†úÍ≥µ'
        print("‚ö†Ô∏è resolve_merchant: ÏûÖÎ†•Îêú ÎßàÏä§ÌÇπ ÏÉÅÌò∏Í∞Ä ÏóÜÏñ¥ Îß§Ïπ≠ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§.")
        return None, debug_info

    df = merchants_df.copy()
    df['_norm_name'] = df['MCT_NM'].apply(_normalize_compare)
    df['_norm_sigungu'] = df['SIGUNGU'].apply(_normalize_compare)
    df['_norm_category'] = df['CATEGORY'].apply(_normalize_compare)

    norm_sigungu = _normalize_compare(sigungu)
    prefix_norm = _normalize_compare(mask_prefix) if mask_prefix else ''

    base = df
    if norm_sigungu:
        exact = base[base['_norm_sigungu'] == norm_sigungu]
        if exact.empty:
            base = base[base['_norm_sigungu'].str.contains(norm_sigungu, na=False)]
        else:
            base = exact
    sigungu_filter_count = int(len(base))

    def _preview_candidates(frame: pd.DataFrame) -> list[dict]:
        preview = []
        for _, row in frame.head(3).iterrows():
            preview.append({
                'ENCODED_MCT': row['ENCODED_MCT'],
                'MCT_NM': row['MCT_NM'],
                'SIGUNGU': row['SIGUNGU'],
                'CATEGORY': row['CATEGORY'],
                'score': float(row.get('__score')) if '__score' in row else None,
            })
        return preview

    if base.empty:
        debug_payload = {
            'input': {'masked_name': masked_name, 'mask_prefix': mask_prefix, 'sigungu': sigungu},
            'sigungu_filter_count': sigungu_filter_count,
            'rule': 'rule1',
            'candidates': [],
        }
        print("üß≠ resolve_phase:", json.dumps(debug_payload, ensure_ascii=False))
        print(f"‚ö†Ô∏è Í∞ÄÎßπÏ†ê ÎØ∏ÏùºÏπò ‚Äì {masked_name}¬∑{sigungu}Î•º ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.")
        debug_info['notes'] = 'sigungu_filter_empty'
        return None, debug_info

    # Rule-1 strict: startswith
    if prefix_norm:
        rule1 = base[base['_norm_name'].str.startswith(prefix_norm, na=False)]
    else:
        rule1 = base.iloc[0:0]
    rule1_count = int(len(rule1))

    if rule1_count == 1:
        row = rule1.iloc[0]
        resolved = {
            'encoded_mct': str(row['ENCODED_MCT']),
            'masked_name': row.get('MCT_NM'),
            'address': row.get('ADDR_BASE'),
            'sigungu': row.get('SIGUNGU'),
            'category': row.get('CATEGORY'),
            'score': 1.0,
        }
        debug_info['candidates'] = _preview_candidates(rule1.assign(__score=1.0))
        debug_info['path'] = 'rule1'
        print(
            "üß≠ resolve_phase:",
            json.dumps({
                'input': {'masked_name': masked_name, 'mask_prefix': mask_prefix, 'sigungu': sigungu},
                'rule': 'rule1',
                'sigungu_filter_count': sigungu_filter_count,
                'rule1_count': rule1_count,
                'candidates': debug_info['candidates'],
            }, ensure_ascii=False),
        )
        print("‚úÖ resolved_merchant_id:", resolved['encoded_mct'])
        return resolved, debug_info

    def _score_rows(frame: pd.DataFrame) -> pd.DataFrame:
        scored = frame.copy()
        scores = []
        for _, r in scored.iterrows():
            name_norm = r['_norm_name'] or ''
            base_score = 0.0
            if prefix_norm:
                if name_norm.startswith(prefix_norm):
                    base_score = 1.0
                elif prefix_norm in name_norm:
                    base_score = 0.8
                else:
                    base_score = SequenceMatcher(None, prefix_norm, name_norm).ratio()
            fuzzy = SequenceMatcher(None, prefix_norm, name_norm).ratio() if prefix_norm else 0.0
            base_val = max(base_score, fuzzy)
            length_bonus = 0.05 if prefix_norm and len(name_norm) > len(prefix_norm) else 0.0
            scores.append(round(min(base_val + length_bonus, 1.05), 4))
        scored['__score'] = scores
        scored['__name_len'] = scored['_norm_name'].str.len().fillna(0)
        return scored

    # Rule-2 fallback if strict fails
    rule2_base = base if rule1_count == 0 else rule1
    rule2 = _score_rows(rule2_base)
    top = rule2.sort_values(['__score', '__name_len', 'ENCODED_MCT'], ascending=[False, False, True])
    debug_candidates = _preview_candidates(top)
    debug_info['candidates'] = debug_candidates

    chosen = None
    path = 'rule2' if rule1_count == 0 else 'rule1'
    if not top.empty and float(top.iloc[0]['__score']) >= 0.85:
        chosen = top.iloc[0]
        debug_info['path'] = path
    elif not top.empty and rule1_count > 1:
        chosen = top.iloc[0]
        debug_info['path'] = 'rule1'

    print(
        "üß≠ resolve_phase:",
        json.dumps({
            'input': {'masked_name': masked_name, 'mask_prefix': mask_prefix, 'sigungu': sigungu},
            'rule': path,
            'sigungu_filter_count': sigungu_filter_count,
            'rule1_count': rule1_count,
            'candidates': debug_candidates,
        }, ensure_ascii=False),
    )

    if chosen is not None:
        resolved = {
            'encoded_mct': str(chosen['ENCODED_MCT']),
            'masked_name': chosen.get('MCT_NM'),
            'address': chosen.get('ADDR_BASE'),
            'sigungu': chosen.get('SIGUNGU'),
            'category': chosen.get('CATEGORY'),
            'score': float(chosen.get('__score')) if pd.notna(chosen.get('__score')) else None,
        }
        print("‚úÖ resolved_merchant_id:", resolved['encoded_mct'])
        return resolved, debug_info

    # Rule-2 failed ‚Üí optional LLM assist
    if allow_llm:
        llm_result, llm_meta = call_llm_for_mask(original_question, masked_name, sigungu)
        debug_info['notes'] = 'llm_invoked'
        if llm_meta:
            debug_info['llm'] = {'parsed': llm_result, **llm_meta}
        if llm_result:
            new_mask = llm_result.get('merchant_mask') or masked_name
            new_prefix = (new_mask.split('*', 1)[0].strip() if new_mask else mask_prefix)
            new_sigungu = llm_result.get('sigungu') or sigungu
            if (new_mask, new_sigungu) != (masked_name, sigungu):
                match, nested_debug = resolve_merchant(
                    new_mask,
                    new_prefix,
                    new_sigungu,
                    merchants_df,
                    original_question=original_question,
                    allow_llm=False,
                )
                if isinstance(nested_debug, dict):
                    if llm_meta:
                        nested_debug.setdefault('llm', {'parsed': llm_result, **llm_meta})
                    nested_debug['notes'] = nested_debug.get('notes') or 'llm_invoked'
                    if not nested_debug.get('path'):
                        nested_debug['path'] = 'llm'
                return match, nested_debug

    # No match ‚Üí surface suggestions
    base_scores = []
    if prefix_norm:
        for _, row in base.iterrows():
            ratio = SequenceMatcher(None, prefix_norm, row['_norm_name'] or '').ratio()
            base_scores.append((ratio, row))
        base_scores.sort(key=lambda x: x[0], reverse=True)
        suggestions = [
            {
                'ENCODED_MCT': r['ENCODED_MCT'],
                'MCT_NM': r['MCT_NM'],
                'SIGUNGU': r['SIGUNGU'],
                'CATEGORY': r['CATEGORY'],
            }
            for ratio, r in base_scores[:3]
            if ratio > 0
        ]
    else:
        suggestions = []

    debug_info['suggestions'] = suggestions
    print(f"‚ö†Ô∏è Í∞ÄÎßπÏ†ê ÎØ∏ÏùºÏπò ‚Äì {masked_name}¬∑{sigungu}Î•º ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.")
    if suggestions:
        print("üîç Ïú†ÏÇ¨ ÌõÑÎ≥¥:", json.dumps(suggestions, ensure_ascii=False))
    return None, debug_info

def parse_question(q):
    original = q or ''
    normalized = unicodedata.normalize('NFKC', original)
    normalized = re.sub(r"\s+", " ", normalized).strip()
    lower_q = normalized.lower()
    age_cond = None
    if '10ÎåÄ' in original or 'teen' in lower_q or re.search(r'\b1[0-9]\b', lower_q):
        age_cond = ('<=', 19)
    if '20ÎåÄ' in original or '20s' in lower_q or 'twenties' in lower_q:
        if ('Ïù¥Ìïò' in original) or ('under' in lower_q) or ('<=' in lower_q):
            age_cond = ('<=', 20)
        else:
            age_cond = ('range', (20,29))
    if 'Ï≤≠ÏÜåÎÖÑ' in original:
        age_cond = ('<=', 19)

    weather = None
    if ('ÎπÑ' in original) or ('Ïö∞Ï≤ú' in original) or ('rain' in lower_q):
        weather = 'rain'
    elif ('Îßë' in original) or ('sunny' in lower_q) or ('clear' in lower_q):
        weather = 'clear'
    elif ('Îàà' in original) or ('snow' in lower_q):
        weather = 'snow'

    months = DEFAULT_MONTHS
    weeks_requested = None
    week_match = re.search(r'(\d+)\s*Ï£º', original)
    if week_match:
        try:
            weeks_requested = int(week_match.group(1))
        except ValueError:
            weeks_requested = None
        if weeks_requested and weeks_requested > 0:
            months = max(1, round(weeks_requested / 4))
    if 'Ïù¥Î≤àÎã¨' in original or 'this month' in lower_q:
        months = 1
    elif ('ÌïúÎã¨' in original) or ('1Îã¨' in original) or ('month' in lower_q):
        months = 1
    elif 'Î∂ÑÍ∏∞' in original or 'quarter' in lower_q:
        months = 3

    industry = None
    if ('Ïπ¥Ìéò' in original) or ('Ïª§Ìîº' in original):
        industry = 'cafe'
    elif ('ÏöîÏãù' in original) or ('restaurant' in lower_q) or ('ÏãùÎãπ' in original):
        industry = 'restaurant'

    merchant_mask = None
    pattern_used = 'none'
    brace_match = re.search(r'\{([^{}]+)\}', normalized)
    if brace_match:
        merchant_mask = brace_match.group(1).strip()
        pattern_used = 'curly_brace'

    mask_prefix = None
    if merchant_mask:
        mask_prefix = merchant_mask.split('*', 1)[0].strip()

    sigungu_pattern = 'hangul_gu_regex'
    sigungu_match = re.search(r'(?P<sigungu>[Í∞Ä-Ìû£]{2,}Íµ¨)', normalized)
    if sigungu_match:
        merchant_sigungu = sigungu_match.group('sigungu')
    else:
        merchant_sigungu = 'ÏÑ±ÎèôÍµ¨'
        sigungu_pattern = 'default_sigungu'

    merchant_info = {
        'masked_name': merchant_mask,
        'mask_prefix': mask_prefix,
        'sigungu': merchant_sigungu,
        'industry_label': None,
    }

    explicit_id = None
    trimmed = original.strip()
    if re.fullmatch(r'[A-Z0-9]{10,12}', trimmed):
        explicit_id = trimmed

    return {
        'original_question': original,
        'age_cond': age_cond,
        'weather': weather,
        'months': months,
        'weeks_requested': weeks_requested,
        'industry': industry,
        'normalized_question': normalized,
        'merchant_masked_name': merchant_info['masked_name'],
        'merchant_mask_prefix': merchant_info['mask_prefix'],
        'merchant_sigungu': merchant_info['sigungu'],
        'merchant_industry_label': merchant_info['industry_label'],
        'merchant_explicit_id': explicit_id,
        'merchant_pattern_used': pattern_used,
        'merchant_sigungu_pattern': sigungu_pattern,
    }

def subset_period(panel, months=DEFAULT_MONTHS):
    if panel['_date'].isna().all():
        return panel.iloc[0:0]
    maxd = panel['_date'].max()
    thr = maxd - pd.Timedelta(days=31*months)
    return panel[panel['_date'] >= thr]

def kpi_summary(panel_sub):
    if panel_sub.empty:
        return {}, {'latest_raw_snapshot': None, 'sanitized_snapshot': None}
    latest_idx = panel_sub.groupby('_merchant_id')['_date'].idxmax()
    snap = panel_sub.loc[latest_idx].sort_values('_date', ascending=False)

    def _safe_float(val):
        try:
            f = float(val)
        except (TypeError, ValueError):
            return None
        return f

    def _clean_pct(val):
        if val is None or pd.isna(val):
            return None
        try:
            f = float(val)
        except (TypeError, ValueError):
            return None
        if f < 0 or f > 100:
            return None
        return round(f, 1)

    detail_row = snap.iloc[0]
    youth_latest = _safe_float(detail_row.get('YOUTH_SHARE'))
    revisit_latest = _safe_float(detail_row.get('REVISIT_RATE'))
    new_latest = _safe_float(detail_row.get('NEW_RATE'))

    age_labels = {
        '1020': 'Ï≤≠ÎÖÑ(10-20)',
        '30': '30ÎåÄ',
        '40': '40ÎåÄ',
        '50': '50ÎåÄ',
        '60': '60ÎåÄ',
    }
    age_distribution = []
    age_allowlist = []
    for code, label in age_labels.items():
        cols = [f'M12_MAL_{code}_RAT', f'M12_FME_{code}_RAT']
        raw_vals = pd.Series([detail_row.get(c) for c in cols])
        numeric_vals = pd.to_numeric(raw_vals, errors='coerce')
        if numeric_vals.notna().sum() == 0:
            continue
        total = numeric_vals.sum(skipna=True)
        cleaned = _clean_pct(total)
        if cleaned is not None:
            age_distribution.append({'code': code, 'label': label, 'value': cleaned})
            age_allowlist.append(code)
    age_distribution.sort(key=lambda x: x['value'], reverse=True)

    customer_mix_map = [
        ('Ïú†Îèô', 'RC_M1_SHC_FLP_UE_CLN_RAT'),
        ('Í±∞Ï£º', 'RC_M1_SHC_RSD_UE_CLN_RAT'),
        ('ÏßÅÏû•', 'RC_M1_SHC_WP_UE_CLN_RAT'),
    ]
    customer_mix_detail = {}
    for label, col in customer_mix_map:
        customer_mix_detail[label] = _clean_pct(_safe_float(detail_row.get(col)))

    ticket_band_raw = detail_row.get('APV_CE_RAT')
    ticket_band = None
    if isinstance(ticket_band_raw, str):
        parts = ticket_band_raw.split('_', 1)
        ticket_band = parts[-1].strip() if parts else ticket_band_raw.strip()
    elif pd.notna(ticket_band_raw):
        ticket_band = str(ticket_band_raw)

    sanitized = {
        'youth_share_avg': _clean_pct(youth_latest),
        'revisit_rate_avg': _clean_pct(revisit_latest),
        'new_rate_avg': _clean_pct(new_latest),
        'age_distribution': age_distribution,
        'age_top_segments': age_distribution[:3],
        'age_allowlist': age_allowlist,
        'customer_mix_detail': customer_mix_detail,
        'avg_ticket_band_label': ticket_band,
        'n_merchants': int(snap['_merchant_id'].nunique()),
    }

    raw_snapshot = {
        'TA_YM': detail_row.get('TA_YM'),
        '_date': str(detail_row.get('_date')),
        'MCT_UE_CLN_REU_RAT_raw': detail_row.get('MCT_UE_CLN_REU_RAT_raw'),
        'MCT_UE_CLN_NEW_RAT_raw': detail_row.get('MCT_UE_CLN_NEW_RAT_raw'),
        'M12_MAL_1020_RAT_raw': detail_row.get('M12_MAL_1020_RAT_raw'),
        'M12_FME_1020_RAT_raw': detail_row.get('M12_FME_1020_RAT_raw'),
        'RC_M1_SHC_FLP_UE_CLN_RAT_raw': detail_row.get('RC_M1_SHC_FLP_UE_CLN_RAT_raw'),
        'RC_M1_SHC_RSD_UE_CLN_RAT_raw': detail_row.get('RC_M1_SHC_RSD_UE_CLN_RAT_raw'),
        'RC_M1_SHC_WP_UE_CLN_RAT_raw': detail_row.get('RC_M1_SHC_WP_UE_CLN_RAT_raw'),
    }

    sanitized_snapshot = {
        'revisit_pct': sanitized['revisit_rate_avg'],
        'new_pct': sanitized['new_rate_avg'],
        'youth_pct': sanitized['youth_share_avg'],
        'customer_mix_detail': sanitized['customer_mix_detail'],
        'age_top_segments': sanitized['age_top_segments'],
        'age_allowlist': sanitized['age_allowlist'],
        'avg_ticket_band_label': sanitized['avg_ticket_band_label'],
    }

    print("üóÇ KPI raw snapshot:", json.dumps(raw_snapshot, ensure_ascii=False))
    print("‚úÖ KPI sanitized:", json.dumps(sanitized_snapshot, ensure_ascii=False))

    return sanitized, {'latest_raw_snapshot': raw_snapshot, 'sanitized_snapshot': sanitized_snapshot}


    sanitized_snapshot = {
        'revisit_pct': sanitized['revisit_rate_avg'],
        'new_pct': sanitized['new_rate_avg'],
        'youth_pct': sanitized['youth_share_avg'],
        'customer_mix_detail': sanitized['customer_mix_detail'],
        'age_top_segments': sanitized['age_top_segments'],
        'avg_ticket_band_label': sanitized['avg_ticket_band_label'],
    }

    print("üóÇ KPI raw snapshot:", json.dumps(raw_snapshot, ensure_ascii=False))
    print("‚úÖ KPI sanitized:", json.dumps(sanitized_snapshot, ensure_ascii=False))

    return sanitized, {'latest_raw_snapshot': raw_snapshot, 'sanitized_snapshot': sanitized_snapshot}


def weather_effect(panel_sub, wx_monthly):
    if (wx_monthly is None) or panel_sub.empty or ('REVISIT_RATE' not in panel_sub):
        return {'metric':'REVISIT_RATE','effect':None,'ci':[None,None],'note':'ÎÇ†Ïî®/ÌëúÎ≥∏ Î∂ÄÏ°±'}
    m = panel_sub.groupby('TA_YM', as_index=False)['REVISIT_RATE'].mean()
    m = m.merge(wx_monthly[['TA_YM','RAIN_SUM']], on='TA_YM', how='inner')
    if m.empty or m['RAIN_SUM'].nunique() < 2:
        return {'metric':'REVISIT_RATE','effect':None,'ci':[None,None],'note':'ÏÉÅÍ¥Ä Ï∂îÏ†ï Î∂àÍ∞Ä'}
    corr = m['REVISIT_RATE'].corr(m['RAIN_SUM'])
    return {'metric':'REVISIT_RATE','effect':float(corr), 'ci':[None,None], 'note':'ÌîºÏñ¥Ïä® ÏÉÅÍ¥Ä(ÏõîÎã®ÏúÑ)'}

def agent1_pipeline(question, shinhan_dir=SHINHAN_DIR, external_dir=EXTERNAL_DIR):
    debug_block = {
        'input': {
            'original': _mask_debug_preview(question, limit=120),
            'flags': {
                'USE_LLM': USE_LLM,
                'DEBUG_MAX_PREVIEW': DEBUG_MAX_PREVIEW,
                'DEBUG_SHOW_RAW': DEBUG_SHOW_RAW,
            },
        },
        'errors': [],
    }

    merchants_df = load_set1(shinhan_dir)

    parse_t0 = tick()
    try:
        qinfo = parse_question(question)
    except Exception as exc:
        debug_block['errors'].append({'stage': 'parse', 'msg': str(exc)})
        debug_block['parse'] = {'elapsed_ms': to_ms(parse_t0)}
        raise
    parse_elapsed = to_ms(parse_t0)
    debug_block['parse'] = {
        'merchant_mask': qinfo.get('merchant_masked_name'),
        'mask_prefix': qinfo.get('merchant_mask_prefix'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'pattern_used': qinfo.get('merchant_pattern_used'),
        'elapsed_ms': parse_elapsed,
    }

    run_id = datetime.datetime.utcnow().isoformat()
    parse_log = {
        'original': qinfo.get('original_question'),
        'merchant_mask': qinfo.get('merchant_masked_name'),
        'mask_prefix': qinfo.get('merchant_mask_prefix'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'explicit_id': qinfo.get('merchant_explicit_id'),
    }
    print("üÜî agent1_run:", run_id)
    print("üßæ question_fields:", json.dumps(parse_log, ensure_ascii=False))

    merchant_match = None
    resolve_meta = {
        'candidates': [],
        'path': None,
        'notes': None,
        'suggestions': None,
        'llm': None,
    }

    resolve_stage = {
        'path': 'none',
        'candidates_top3': [],
        'resolved_merchant_id': None,
    }

    resolve_t0 = tick()
    try:
        explicit_id = qinfo.get('merchant_explicit_id')
        if explicit_id:
            lookup = merchants_df[merchants_df['ENCODED_MCT'] == explicit_id]
            print(
                "üè∑ explicit_id_lookup:",
                json.dumps({'explicit_id': explicit_id, 'row_count': int(len(lookup))}, ensure_ascii=False),
            )
            if not lookup.empty:
                row = lookup.iloc[0]
                merchant_match = {
                    'encoded_mct': str(row['ENCODED_MCT']),
                    'masked_name': row.get('MCT_NM'),
                    'address': row.get('ADDR_BASE'),
                    'sigungu': row.get('SIGUNGU'),
                    'category': row.get('CATEGORY'),
                    'score': None,
                }
                resolve_meta['path'] = 'user'

        if merchant_match is None:
            merchant_match, resolve_meta = resolve_merchant(
                qinfo.get('merchant_masked_name'),
                qinfo.get('merchant_mask_prefix'),
                qinfo.get('merchant_sigungu'),
                merchants_df,
                original_question=qinfo.get('normalized_question') or question,
                allow_llm=USE_LLM,
            )
    except Exception as exc:
        debug_block['errors'].append({'stage': 'resolve', 'msg': str(exc)})
        raise
    finally:
        resolve_stage['elapsed_ms'] = to_ms(resolve_t0)

    target_id = None
    if merchant_match and merchant_match.get('encoded_mct') is not None:
        target_id = str(merchant_match['encoded_mct'])
        merchant_match['encoded_mct'] = target_id

    if resolve_meta.get('path') is None:
        resolve_meta['path'] = 'llm' if resolve_meta.get('llm') else 'none'
    resolve_stage['path'] = resolve_meta.get('path') or 'none'
    resolve_stage['resolved_merchant_id'] = target_id

    candidate_payload = []
    for cand in resolve_meta.get('candidates') or []:
        cid = cand.get('ENCODED_MCT') or cand.get('encoded_mct')
        try:
            score_val = cand.get('score')
            score = round(float(score_val), 4) if score_val is not None else None
        except (TypeError, ValueError):
            score = None
        candidate_payload.append({
            'id': str(cid) if cid is not None else None,
            'name': cand.get('MCT_NM') or cand.get('masked_name'),
            'sigungu': cand.get('SIGUNGU') or cand.get('sigungu'),
            'score': score,
        })
    resolve_stage['candidates_top3'] = candidate_payload[:3]
    debug_block['resolve'] = resolve_stage

    llm_meta = resolve_meta.get('llm') or {}
    agent1_llm = {
        'used': bool(llm_meta.get('used')),
        'model': llm_meta.get('model'),
        'prompt_preview': llm_meta.get('prompt_preview', ''),
        'resp_bytes': llm_meta.get('resp_bytes'),
        'safety_blocked': bool(llm_meta.get('safety_blocked')),
        'elapsed_ms': llm_meta.get('elapsed_ms'),
    }
    debug_block['agent1_llm'] = agent1_llm

    panel_stage = {}
    panel_t0 = tick()
    try:
        panel, panel_stats = build_panel(shinhan_dir, merchants_df=merchants_df, target_id=target_id)
    except Exception as exc:
        panel_stage['elapsed_ms'] = to_ms(panel_t0)
        debug_block['errors'].append({'stage': 'panel', 'msg': str(exc)})
        debug_block['panel'] = panel_stage
        raise
    panel_elapsed = to_ms(panel_t0)
    sub = subset_period(panel, months=qinfo['months'])
    panel_stage.update({
        'rows_before': int(len(panel)),
        'rows_after': int(len(sub)),
        'latest_ta_ym': str(sub['TA_YM'].max()) if not sub.empty and 'TA_YM' in sub.columns else None,
        'elapsed_ms': panel_elapsed,
        'stats': panel_stats,
    })
    debug_block['panel'] = panel_stage

    wxm = None
    try:
        wxm = load_weather_monthly(external_dir)
    except Exception as exc:
        debug_block['errors'].append({'stage': 'weather', 'msg': str(exc)})
        wxm = None

    snapshot_t0 = tick()
    kpis, kpi_debug = kpi_summary(sub)
    snapshot_elapsed = to_ms(snapshot_t0)

    raw_snapshot = {}
    raw_source = (kpi_debug or {}).get('latest_raw_snapshot') or {}
    for key, value in raw_source.items():
        if key.endswith('_raw'):
            raw_snapshot[key[:-4]] = value
        else:
            raw_snapshot[key] = value
    sanitized_snapshot = (kpi_debug or {}).get('sanitized_snapshot') or {}
    debug_block['snapshot'] = {
        'raw': raw_snapshot,
        'sanitized': sanitized_snapshot,
        'elapsed_ms': snapshot_elapsed,
    }

    render_table = _build_debug_table(qinfo, merchant_match, sanitized_snapshot)
    debug_block['render'] = {
        'table_dict': render_table,
    }

    wfx = weather_effect(sub, wxm)

    notes = []
    quality = 'normal'
    if sub.empty:
        notes.append('ÏßàÎ¨∏ Ï°∞Í±¥ ÌëúÎ≥∏ Î∂ÄÏ°± ÎòêÎäî Í∏∞Í∞Ñ Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå')
        quality = 'low'
    if wxm is None and qinfo['weather'] is not None:
        notes.append('ÎÇ†Ïî® Îç∞Ïù¥ÌÑ∞ Î∂ÄÏû¨: ÎÇ†Ïî® Í¥ÄÎ†® Ìö®Í≥ºÎäî Ï∂îÏ†ïÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        quality = 'low'
    if qinfo.get('merchant_masked_name') is None:
        notes.append('{ÏÉÅÌò∏} ÌòïÌÉúÏùò ÏûÖÎ†•Ïù¥ ÏóÜÏñ¥ Í∞ÄÎßπÏ†ê ÏãùÎ≥ÑÏùÑ ÏßÑÌñâÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.')
        quality = 'low'
    if merchant_match is None:
        notes.append('ÏßàÎ¨∏Í≥º ÏùºÏπòÌïòÎäî Í∞ÄÎßπÏ†êÏùÑ Ï∞æÏßÄ Î™ªÌï¥ Ï†ÑÏ≤¥ ÌëúÎ≥∏ÏùÑ ÏÇ¨Ïö©ÌñàÏäµÎãàÎã§.')
        quality = 'low'

    merchant_query = {
        'masked_name': qinfo.get('merchant_masked_name'),
        'mask_prefix': qinfo.get('merchant_mask_prefix'),
        'sigungu': qinfo.get('merchant_sigungu'),
        'industry_label': qinfo.get('merchant_industry_label'),
    }

    merchants_covered = int(sub['_merchant_id'].nunique()) if not sub.empty else 0

    out = {
        'context': {
            'intent': question,
            'parsed': qinfo,
            'merchant_query': merchant_query,
            'run_id': run_id,
            'panel_stats': panel_stage.get('stats', {}),
            'merchant_candidates': resolve_meta.get('candidates'),
            'merchant_resolution_path': resolve_stage['path'],
        },
        'kpis': kpis,
        'weather_effect': wfx,
        'limits': notes,
        'quality': quality,
        'period': {
            'max_date': str(panel['_date'].max() if '_date' in panel.columns else None),
            'months': qinfo['months'],
            'weeks_requested': qinfo.get('weeks_requested'),
        },
        'sample': {
            'merchants_covered': merchants_covered
        },
        'debug': debug_block,
    }

    if merchant_match:
        out['context']['merchant'] = merchant_match
        out['context']['merchant_masked_name'] = merchant_match.get('masked_name')

    out_path = OUTPUT_DIR / 'agent1_output.json'
    with open(out_path, 'w', encoding='utf-8') as f:
        json.dump(out, f, ensure_ascii=False, indent=2)
    print('‚úÖ Agent-1 JSON Ï†ÄÏû•:', out_path)
    return out

QUESTION_TYPE_INFO = {
    "Q1_CAFE_CHANNELS": {
        "label": "Ï£ºÏöî Î∞©Î¨∏ Í≥†Í∞ù ÌäπÏÑ±Ïóê Îî∞Î•∏ Ï±ÑÎÑê Ï∂îÏ≤ú Î∞è ÌôçÎ≥¥Ïïà",
        "instructions": [
            "Ïó∞Î†π/ÏÑ±Î≥Ñ¬∑Ïú†Îèô/Í±∞Ï£º Íµ¨ÏÑ± ÎπÑÏ§ëÏùÑ ÌôúÏö©Ìï¥ Ï±ÑÎÑêÍ≥º Î©îÏãúÏßÄÎ•º Ï†úÏãúÌï©ÎãàÎã§.",
            "Ïò®¬∑Ïò§ÌîÑÎùºÏù∏ 3~4Í∞ú ÌôçÎ≥¥ ÏïÑÏù¥ÎîîÏñ¥Î•º Í∞ÑÍ≤∞ÌïòÍ≤å ÏûëÏÑ±Ìï©ÎãàÎã§.",
            "Í∞Å ÏïÑÏù¥ÎîîÏñ¥Îäî Í≥†Í∞ùÍµ∞ ‚Üí Ï±ÑÎÑê ‚Üí Ïã§Ìñâ ÏöîÏïΩÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.",
        ],
    },
    "Q2_LOW_RETENTION": {
        "label": "Ïû¨Î∞©Î¨∏Î•† 30% Ïù¥Ìïò Í∞úÏÑ† ÏïÑÏù¥ÎîîÏñ¥",
        "instructions": [
            "Ïû¨Î∞©Î¨∏¬∑Ïã†Í∑ú ÎπÑÏ§ëÏùÑ Í∑ºÍ±∞Î°ú Ïû¨Î∞©Î¨∏ Ï¥âÏßÑ Ïï°ÏÖòÏùÑ Ï†úÏãúÌï©ÎãàÎã§.",
            "3~4Í∞úÏùò ÌîÑÎ°úÎ™®ÏÖò/Î©§Î≤ÑÏã≠/CRM ÏïÑÏù¥ÎîîÏñ¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§.",
            "Í∞Å ÏïÑÏù¥ÎîîÏñ¥Îäî ÌÉÄÍπÉ Í≥†Í∞ùÍ≥º Ïã§Ìñâ Îã®Í≥ÑÎ•º Î∂ÑÎ™ÖÌûà Ìï©ÎãàÎã§.",
        ],
    },
    "Q3_FOOD_ISSUE": {
        "label": "ÏöîÏãùÏóÖÏùò Í∞ÄÏû• ÌÅ∞ Î¨∏Ï†ú Í∞ÄÏÑ§ + Î≥¥ÏôÑ ÏïÑÏù¥ÎîîÏñ¥",
        "instructions": [
            "ÏãùÏùåÏóÖ ÌäπÏÑ±(Î∞©Î¨∏ Í≥†Í∞ù/ÏãúÍ∞ÑÎåÄ/Ïú†Îèô)ÏùÑ Í∑ºÍ±∞Î°ú Î¨∏Ï†ú Í∞ÄÏÑ§ÏùÑ ÏÑ∏ÏõÅÎãàÎã§.",
            "3~4Í∞úÏùò Í∞úÏÑ† ÏïÑÏù¥ÎîîÏñ¥Î•º Ï†úÏãúÌïòÍ≥† Ïã§Ìñâ Îã®Í≥ÑÎ•º ÎÇòÏó¥Ìï©ÎãàÎã§.",
            "Î¨∏Ï†ú Í∞ÄÏÑ§Í≥º Ìï¥Í≤∞ ÏïÑÏù¥ÎîîÏñ¥Î•º Ìïú ÏÑ∏Ìä∏Î°ú ÏÑúÏà†Ìï©ÎãàÎã§.",
        ],
    },
    "GENERIC": {
        "label": "ÏùºÎ∞ò Ïª®ÏÑ§ÌåÖ ÏßàÎ¨∏",
        "instructions": [
            "ÌïµÏã¨ Í≥†Í∞ù¬∑ÏÑ±Í≥º Îç∞Ïù¥ÌÑ∞Î•º Í∑ºÍ±∞Î°ú 3~4Í∞úÏùò Ïã§Ìñâ ÏïÑÏù¥ÎîîÏñ¥Î•º Ï†úÍ≥µÌï©ÎãàÎã§.",
            "Í∞Å ÏïÑÏù¥ÎîîÏñ¥Îäî ÎåÄÏÉÅ, Ï±ÑÎÑê, Ïã§Ìñâ Îã®Í≥Ñ, Ï∏°Ï†ï ÏßÄÌëúÎ•º Ìè¨Ìï®Ìï©ÎãàÎã§.",
        ],
    },
}


ORGANIZER_QUESTION_TYPES = {
    "Q1_CAFE_CHANNELS",
    "Q2_LOW_RETENTION",
    "Q3_FOOD_ISSUE",
}


def _schema_key_for_question(question_type: str | None) -> str:
    if question_type in ORGANIZER_QUESTION_TYPES:
        return "organizer"
    return "legacy"


def _resolve_schema_for_question(
    question_type: str | None,
) -> tuple[dict, Draft7Validator | None, str]:
    schema_bundle, validator_bundle = load_actioncard_schema_current()
    key = _schema_key_for_question(question_type)
    if isinstance(schema_bundle, dict) and "schemas" in schema_bundle:
        schema_obj = schema_bundle.get("schemas", {}).get(key) or {}
    else:
        schema_obj = schema_bundle if isinstance(schema_bundle, dict) else {}
        key = "legacy"

    validator = None
    if isinstance(validator_bundle, dict):
        validator = validator_bundle.get(key)
    else:
        validator = validator_bundle
    return schema_obj, validator, key


def infer_question_type(question_text: str | None) -> str:
    text = (question_text or "").lower()
    if not text:
        return "GENERIC"
    if any(keyword in text for keyword in ["Ï±ÑÎÑê", "ÌôçÎ≥¥", "sns", "Ï∫†ÌéòÏù∏"]):
        return "Q1_CAFE_CHANNELS"
    if any(keyword in text for keyword in ["Ïû¨Î∞©Î¨∏", "retention", "Ïû¨Íµ¨Îß§", "Îã®Í≥®"]):
        return "Q2_LOW_RETENTION"
    if any(keyword in text for keyword in ["ÏöîÏãù", "ÏãùÎãπ", "food", "ÎßõÏßë"]):
        return "Q3_FOOD_ISSUE"
    return "GENERIC"


def _summarise_rag_context(rag_context: dict | None) -> tuple[str, str]:
    if not isinstance(rag_context, dict):
        return ("", "RAG ÎπÑÌôúÏÑ±Ìôî: Ïª®ÌÖçÏä§Ìä∏Í∞Ä Ï†ÑÎã¨ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")

    enabled = bool(rag_context.get("enabled"))
    threshold = rag_context.get("threshold")
    max_score = rag_context.get("max_score")
    raw_hits = rag_context.get("hits")
    chunks_for_hits = rag_context.get("chunks") if isinstance(rag_context.get("chunks"), list) else []
    hits = int(raw_hits if raw_hits is not None else len(chunks_for_hits))
    selected_docs = rag_context.get("selected_doc_ids") or []
    mode = rag_context.get("mode") or "auto"
    reason_lines: list[str] = []

    if not enabled:
        if rag_context.get("selection_missing"):
            reason_lines.append("RAG ÏöîÏ≤≠Îê®Ïù¥ÎÇò ÏÑ†ÌÉùÎêú Î¨∏ÏÑúÍ∞Ä ÏóÜÏäµÎãàÎã§.")
        elif rag_context.get("requested") and rag_context.get("error"):
            reason_lines.append(f"Ïò§Î•ò: {rag_context['error']}")
        else:
            reason_lines.append("UI ÌÜ†Í∏Ä ÎòêÎäî Î™®ÎìúÎ°ú Ïù∏Ìï¥ ÎπÑÌôúÏÑ±ÌôîÎêòÏóàÏäµÎãàÎã§.")
    else:
        reason_lines.append(f"Î™®Îìú={mode}, ÏÑ†ÌÉù Î¨∏ÏÑú={selected_docs or 'ÏóÜÏùå'}")
        if max_score is None:
            reason_lines.append("ÏµúÍ≥† Ï†êÏàòÎ•º Í≥ÑÏÇ∞ÌïòÏßÄ Î™ªÌñàÏäµÎãàÎã§.")
        elif threshold is not None and max_score < threshold and mode != "always":
            reason_lines.append(f"ÏµúÍ≥† Ï†êÏàò {max_score:.2f} < ÏûÑÍ≥ÑÍ∞í {threshold:.2f}")

    include_rag = bool(
        enabled
        and hits > 0
        and (mode == "always" or threshold is None or (max_score is not None and max_score >= threshold))
    )

    prompt_block = ""
    if include_rag:
        snippets = []
        for chunk in (rag_context.get("chunks") or [])[: hits or 5]:
            text = str(chunk.get("text") or "").strip()
            if len(text) > 220:
                text = text[:220].rstrip() + "‚Ä¶"
            snippets.append(
                {
                    "doc_id": chunk.get("doc_id"),
                    "chunk_id": chunk.get("chunk_id"),
                    "score": float(chunk.get("score") or 0.0),
                    "snippet": text,
                }
            )
        rag_payload = json.dumps(snippets, ensure_ascii=False, indent=2)
        summary = f"RAG Ìè¨Ìï®: hits={hits}, max_score={max_score}, threshold={threshold}, mode={mode}"
        prompt_block = f"{summary}\n{rag_payload}"
        rag_context['prompt_note'] = summary
    else:
        reason = " ; ".join(reason_lines) if reason_lines else "Í∑ºÍ±∞ ÏóÜÏùå"
        rag_context['prompt_note'] = f"RAG Ï†úÏô∏: {reason}"

    return prompt_block, "\n- ".join(reason_lines)


def build_agent2_prompt_overhauled(
    agent1_json,
    *,
    question_text: str | None = None,
    question_type: str | None = None,
    rag_context: dict | None = None,
):
    inferred_type = question_type or infer_question_type(question_text)
    info = QUESTION_TYPE_INFO.get(inferred_type, QUESTION_TYPE_INFO["GENERIC"])
    schema_obj = {}
    try:
        schema_obj, _, _ = _resolve_schema_for_question(inferred_type)
    except Exception as exc:
        schema_obj = {"schema_error": str(exc)}
    schema_text = json.dumps(schema_obj, ensure_ascii=False, indent=2)

    snapshot = (agent1_json or {}).get("debug", {}).get("snapshot", {})
    if isinstance(snapshot, dict):
        sanitized_snapshot = snapshot.get("sanitized") or {}
    else:
        sanitized_snapshot = {}
    age_allowlist = []
    if isinstance(sanitized_snapshot, dict):
        allowlist_candidate = sanitized_snapshot.get("age_allowlist")
        if isinstance(allowlist_candidate, list):
            age_allowlist = [str(code) for code in allowlist_candidate if str(code)]
        elif isinstance(allowlist_candidate, (tuple, set)):
            age_allowlist = [str(code) for code in allowlist_candidate if str(code)]
        if not age_allowlist:
            distribution = sanitized_snapshot.get("age_distribution") or []
            if isinstance(distribution, list):
                for item in distribution:
                    code = str(item.get("code")) if isinstance(item, dict) else None
                    if code:
                        age_allowlist.append(code)
    age_allowlist = list(dict.fromkeys(age_allowlist))

    base_rules = [
        "ÏßàÎ¨∏Ïóê ÏßÅÏ†ë ÎãµÌïòÏã≠ÏãúÏò§. Î™©ÌëúÏπòÎÇò Íµ¨Í∞ÑÏùÑ ÏûÑÏùòÎ°ú Ï∂îÏ†ïÌïòÏßÄ ÎßàÏã≠ÏãúÏò§(Ï†úÍ≥µÎêú Í≤ΩÏö∞ÏóêÎßå ÏÇ¨Ïö©).",
        "Ï£ºÏöî Í∑ºÍ±∞Îäî Agent-1 JSONÏù¥Î©∞, RAGÍ∞Ä ÌôúÏÑ±ÌôîÎêòÍ≥† Ïú†Ìö®Ìï† ÎïåÎßå RAG Ïä§ÎãàÌé´ÏùÑ Í∑ºÍ±∞Î°ú Ìè¨Ìï®Ìï©ÎãàÎã§.",
        "Î™®Îì† ÏïÑÏù¥ÎîîÏñ¥Ïóê ÏµúÏÜå 1Í∞ú Ïù¥ÏÉÅÏùò Í∑ºÍ±∞Î•º Î∂ôÏù¥Í≥†, ÏóÜÏúºÎ©¥ 'Í∑ºÍ±∞ ÏóÜÏùå'ÏúºÎ°ú Î™ÖÏãúÌï©ÎãàÎã§.",
        "Í∑ºÍ±∞ÏóêÎäî Ï∂úÏ≤ò(STRUCTURED/RAG)ÏôÄ ÌïµÏã¨ ÏàòÏπò ÎòêÎäî Ïä§ÎãàÌé´ÏùÑ Ìï®Íªò Ï†úÏãúÌï©ÎãàÎã§.",
        "ÏÉÅÌò∏Î™ÖÏùÄ Ìï≠ÏÉÅ ÎßàÏä§ÌÇπ ÏÉÅÌÉúÎ•º Ïú†ÏßÄÌï©ÎãàÎã§.",
        "answers Î∞∞Ïó¥Ïóê 3~4Í∞úÏùò Í∞ÑÍ≤∞Ìïú ÏïÑÏù¥ÎîîÏñ¥Î•º ÏûëÏÑ±Ìï©ÎãàÎã§.",
        "age cohort Í∑úÏπô: Agent-1 JSONÏóê Ï°¥Ïû¨ÌïòÎäî Ïó∞Î†πÎåÄÎßå ÏÇ¨Ïö©ÌïòÍ≥†, 0~100% Î≤îÏúÑÎ•º Î≤óÏñ¥ÎÇòÎ©¥ '‚Äî'Î°ú ÌëúÍ∏∞Ìï©ÎãàÎã§.",
    ]
    if age_allowlist:
        base_rules.append(f"ÏÇ¨Ïö© Í∞ÄÎä•Ìïú Ïó∞Î†πÎåÄ ÏΩîÎìú: {', '.join(age_allowlist)}")

    type_rules = info.get("instructions", [])

    rag_block, rag_reason = _summarise_rag_context(rag_context)

    sections = [
        "ÎãπÏã†ÏùÄ ÌïúÍµ≠Ïñ¥ ÏÜåÏÉÅÍ≥µÏù∏ Ïª®ÏÑ§ÌÑ¥Ìä∏ÏûÖÎãàÎã§.",
        f"question_type={inferred_type}",
        f"ÏßàÎ¨∏ Ïú†Ìòï ÏÑ§Î™Ö: {info['label']}",
        f"ÏßàÎ¨∏ ÏõêÎ¨∏: {question_text or '‚Äî'}",
        "[Ï∂úÎ†• Í∑úÏπô]",
        "- " + "\n- ".join(base_rules),
    ]

    if type_rules:
        sections.append("[ÏßàÎ¨∏ Ïú†ÌòïÎ≥Ñ ÏßÄÏπ®]")
        sections.append("- " + "\n- ".join(type_rules))

    sections.append("[Ï∂úÎ†• Ïä§ÌÇ§Îßà(JSON)]")
    sections.append(schema_text)
    sections.append("[Îç∞Ïù¥ÌÑ∞(JSON)]")
    sections.append(json.dumps(agent1_json, ensure_ascii=False, indent=2))

    if rag_block:
        sections.append("[RAG_CONTEXT]")
        sections.append(rag_block)
    elif rag_reason:
        sections.append(f"[RAG Ï∞∏Í≥† Î©îÎ™®]\n- {rag_reason}")

    sections.append("[ÏùëÎãµ ÌòïÏãù]")
    sections.append("JSONÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî. ÎßàÌÅ¨Îã§Ïö¥/ÏÑ§Î™Ö/ÏΩîÎìúÎ∏îÎü≠ Í∏àÏßÄ. Î¨∏ÏûêÏó¥ ÎÇ¥ Ï§ÑÎ∞îÍøàÏùÄ \\nÏúºÎ°ú ÌëúÍ∏∞ÌïòÏÑ∏Ïöî.")

    guide = "\n\n".join(sections)
    schema_keys = []
    if isinstance(schema_obj, dict) and isinstance(schema_obj.get("properties"), dict):
        schema_keys = sorted(schema_obj["properties"].keys())
    rag_doc_ids = []
    rag_threshold = None
    rag_max_score = None
    rag_mode = None
    if isinstance(rag_context, dict):
        rag_doc_ids = list(rag_context.get("selected_doc_ids") or [])
        rag_threshold = rag_context.get("threshold")
        rag_max_score = rag_context.get("max_score")
        rag_mode = rag_context.get("mode")
    global AGENT2_PROMPT_TRACE
    AGENT2_PROMPT_TRACE = {
        "question_type": inferred_type,
        "organizer_mode": inferred_type in ORGANIZER_QUESTION_TYPES,
        "schema_keys": schema_keys,
        "rag_included": bool(rag_block),
        "rag_reason": rag_reason,
        "rag_context_doc_ids": rag_doc_ids,
        "rag_threshold": rag_threshold,
        "rag_max_score": rag_max_score,
        "rag_mode": rag_mode,
    }
    return guide

def call_gemini_agent2_overhauled(
    prompt_text,
    model_name: str = 'models/gemini-2.5-flash',
    *,
    question_type: str | None = None,
    agent1_json: dict | None = None,
    **kwargs,
):
    """Call Gemini for Agent-2 with robust JSON parsing and fallback cards."""

    import google.generativeai as genai
    import datetime

    if kwargs:
        _ = ", ".join(sorted(kwargs.keys()))  # noqa: F841 - reserved for debugging

    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        raise RuntimeError('GEMINI_API_KEYÍ∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.')
    genai.configure(api_key=api_key)

    candidates: list[str] = []
    try:
        available = []
        for model in genai.list_models():
            if "generateContent" in getattr(model, "supported_generation_methods", []):
                available.append(model.name)
        for name in available:
            tail = name.split('/')[-1]
            if '2.5' in tail and 'flash' in tail:
                candidates.append(name)
    except Exception:
        pass

    if not candidates:
        candidates = [
            "models/gemini-2.5-flash",
            "models/gemini-2.5-flash-latest",
            "models/gemini-2.5-flash-001",
            "gemini-2.5-flash",
            "gemini-2.5-flash-latest",
            "gemini-2.5-flash-001",
        ]

    if model_name and model_name not in candidates:
        candidates.insert(0, model_name)

    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    generation_config = {
        "temperature": 0.2,
        "top_p": 0.1,
        "top_k": 32,
        "max_output_tokens": 2048,
    }

    prompt_payload = (prompt_text or "").rstrip()
    if "JSONÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî" not in prompt_payload:
        prompt_payload = (
            prompt_payload
            + "\n\nJSONÎßå Ï∂úÎ†•ÌïòÏÑ∏Ïöî. ÎßàÌÅ¨Îã§Ïö¥/ÏÑ§Î™Ö/ÏΩîÎìúÎ∏îÎü≠ Í∏àÏßÄ. Î¨∏ÏûêÏó¥ ÎÇ¥ Ï§ÑÎ∞îÍøàÏùÄ \\nÏúºÎ°ú ÌëúÍ∏∞ÌïòÏÑ∏Ïöî."
        )

    try:
        _, schema_validator, schema_key = _resolve_schema_for_question(question_type)
        schema_error = None
    except Exception as exc:
        schema_validator = None
        schema_key = "unknown"
        schema_error = str(exc)

    global AGENT2_RESPONSE_TRACE
    AGENT2_RESPONSE_TRACE = {
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "question_type": question_type,
        "requested_model": model_name,
        "model_candidates": list(dict.fromkeys(candidates)),
        "generation_config": dict(generation_config),
        "schema_key": schema_key,
        "schema_error": schema_error,
        "prompt_length": len(prompt_payload),
        "attempts": [],
    }

    def _extract_text(resp) -> str:
        text_value = ''
        try:
            payload = getattr(resp, 'text', None)
            if payload:
                text_value = payload
        except Exception:
            pass
        if (not text_value) and getattr(resp, 'candidates', None):
            try:
                cand0 = resp.candidates[0]
                if cand0 and getattr(cand0, 'content', None) and cand0.content.parts:
                    for part in cand0.content.parts:
                        piece = getattr(part, 'text', '')
                        if piece:
                            text_value += piece
            except Exception:
                pass
        return (text_value or '').strip()

    def _blocked_info(resp):
        info = {}
        try:
            info['prompt_feedback'] = getattr(resp, 'prompt_feedback', None)
        except Exception:
            pass
        try:
            if getattr(resp, 'candidates', None):
                info['candidate_safety'] = getattr(resp.candidates[0], 'safety_ratings', None)
                info['finish_reason'] = getattr(resp.candidates[0], 'finish_reason', None)
        except Exception:
            pass
        return info

    def _run_once(name: str, prompt_body: str):
        model = genai.GenerativeModel(
            model_name=name,
            generation_config=generation_config,
            safety_settings=safety_settings,
        )
        response = model.generate_content(
            prompt_body,
            response_mime_type="application/json",
        )
        text_value = _extract_text(response)
        info = _blocked_info(response)
        return text_value, info, name

    def _repair_with_llm(name: str, failing_text: str):
        snippet = _strip_code_fences(failing_text or "").strip()
        if len(snippet) > 4000:
            snippet = snippet[:4000]
        repair_prompt = (
            "ÏïÑÎûò JSONÏùÑ Ïä§ÌÇ§ÎßàÏóê ÎßûÍ≤å Í≥†Ï≥ê JSONÎßå Î∞òÌôòÌïòÏÑ∏Ïöî. Í≥†Ïπ† Ïàò ÏóÜÏúºÎ©¥ {\"answers\": []}Î•º Î∞òÌôòÌïòÏÑ∏Ïöî."
            "\n\n=== JSON ÌõÑÎ≥¥ ===\n" + snippet
        )
        repair_config = dict(generation_config)
        repair_config.update({"temperature": 0.1, "top_p": 0.05, "max_output_tokens": 1024})
        try:
            model = genai.GenerativeModel(
                model_name=name,
                generation_config=repair_config,
                safety_settings=safety_settings,
            )
            response = model.generate_content(
                repair_prompt,
                response_mime_type="application/json",
            )
            repaired_text = _extract_text(response)
            info = _blocked_info(response)
            return repaired_text, {"meta": info}
        except Exception as exc:  # pragma: no cover - network guard
            return None, {"error": str(exc)}

    chosen_payload = None
    chosen_model = None
    last_error = schema_error or "unknown"

    for attempt in range(2):
        for model_candidate in candidates:
            attempt_record = {
                "attempt": attempt + 1,
                "model": model_candidate,
            }
            try:
                text_value, info, used_model = _run_once(model_candidate, prompt_payload)
            except Exception as exc:  # pragma: no cover - network guard
                attempt_record["error"] = str(exc)
                last_error = str(exc)
                AGENT2_RESPONSE_TRACE["attempts"].append(attempt_record)
                continue

            attempt_record["raw_preview"] = (text_value or "")[:600]
            attempt_record["raw_length"] = len(text_value or "")
            attempt_record["meta"] = info

            parsed, parse_logs = llm_json_safe_parse(text_value, schema_validator)
            attempt_record["parse_logs"] = parse_logs

            if parsed is not None:
                attempt_record["status"] = "parsed"
                chosen_payload = parsed
                chosen_model = used_model
                AGENT2_RESPONSE_TRACE["attempts"].append(attempt_record)
                break

            attempt_record["status"] = "parse_failed"
            if parse_logs:
                last_error = parse_logs[-1].get("error") or last_error

            repaired_text, repair_meta = _repair_with_llm(model_candidate, text_value)
            attempt_record["repair_preview"] = (repaired_text or "")[:600]
            attempt_record["repair_meta"] = repair_meta
            if repaired_text:
                repaired, repair_logs = llm_json_safe_parse(repaired_text, schema_validator)
                attempt_record["repair_logs"] = repair_logs
                if repaired is not None:
                    attempt_record["status"] = "repaired"
                    chosen_payload = repaired
                    chosen_model = model_candidate
                    AGENT2_RESPONSE_TRACE["attempts"].append(attempt_record)
                    break
                if repair_logs:
                    last_error = repair_logs[-1].get("error") or last_error

            AGENT2_RESPONSE_TRACE["attempts"].append(attempt_record)
        if chosen_payload is not None:
            break

    if chosen_payload is not None:
        AGENT2_RESPONSE_TRACE.update({"status": "success", "chosen_model": chosen_model})
        try:
            debug_payload = {
                "ts": datetime.datetime.utcnow().isoformat(),
                "chosen_model": chosen_model,
                "attempts": AGENT2_RESPONSE_TRACE["attempts"],
                "schema_key": schema_key,
            }
            (OUTPUT_DIR / 'gemini_debug.json').write_text(
                json.dumps(debug_payload, ensure_ascii=False, indent=2),
                encoding='utf-8',
            )
        except Exception:
            pass
        (OUTPUT_DIR / 'agent2_result.json').write_text(
            json.dumps(chosen_payload, ensure_ascii=False, indent=2),
            encoding='utf-8',
        )
        return chosen_payload

    AGENT2_RESPONSE_TRACE.update({
        "status": "fallback",
        "last_error": last_error,
    })
    fallback = _structured_fallback_cards(agent1_json, question_type)
    AGENT2_RESPONSE_TRACE["fallback_answers"] = len(fallback.get("answers", []))
    try:
        if schema_validator is not None:
            schema_validator.validate(fallback)
    except Exception:
        pass
    (OUTPUT_DIR / 'agent2_result.json').write_text(
        json.dumps(fallback, ensure_ascii=False, indent=2),
        encoding='utf-8',
    )
    return fallback


# ---------------------------------------------------------------------------
# Backward-compatibility shims
# ---------------------------------------------------------------------------

def load_actioncard_schema(*args, **kwargs):
    """Backward-compatible wrapper for legacy imports."""
    return load_actioncard_schema_current(*args, **kwargs)


def build_agent2_prompt(*args, **kwargs):
    """Backward-compatible wrapper for the overhauled Agent-2 prompt builder."""
    return build_agent2_prompt_overhauled(*args, **kwargs)


def call_gemini_agent2(*args, **kwargs):
    """Backward-compatible wrapper that delegates to the updated Gemini caller."""
    return call_gemini_agent2_overhauled(*args, **kwargs)


def main():
    import argparse
    a1 = None; prompt_text = ''; a2 = None
    parser = argparse.ArgumentParser()
    parser.add_argument('--question', type=str, default=None)
    parser.add_argument('--model', type=str, default='gemini-2.5-flash')
    args, _ = parser.parse_known_args()
    q = args.question or os.getenv('QUESTION')
    if not q:
        try:
            q = input('ÏßàÎ¨∏ÏùÑ ÏûÖÎ†•ÌïòÏÑ∏Ïöî: ').strip()
        except Exception:
            q = None
    if not q:
        q = 'ÏÑ±ÎèôÍµ¨ {Í≥†Ìñ•***} Í∏∞Ï§ÄÏúºÎ°ú, Ïû¨Î∞©Î¨∏Ïú®ÏùÑ 4Ï£º ÏïàÏóê ÎÜíÏùº Ïã§ÌñâÏπ¥Îìú Ï†úÏãúÌï¥Ï§ò.'
        print('‚ÑπÔ∏è ÏßàÎ¨∏Ïù¥ ÏóÜÏñ¥ Í∏∞Î≥∏ ÏòàÏãúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§:', q)

    try:
        a1 = agent1_pipeline(q, SHINHAN_DIR, EXTERNAL_DIR)
        prompt_text = build_agent2_prompt_overhauled(a1, question_text=q)
        print('\n==== Gemini Prompt Preview (ÏïûÎ∂ÄÎ∂Ñ) ====')
        print(prompt_text[:800] + ('\n... (ÏÉùÎûµ)' if len(prompt_text)>800 else ''))
        a2 = call_gemini_agent2_overhauled(
            prompt_text,
            model_name=args.model,
            agent1_json=a1,
            question_type=infer_question_type(q),
        )
        print('\n==== Agent-2 Í≤∞Í≥º (ÏïûÎ∂ÄÎ∂Ñ) ====')
        print(json.dumps(a2, ensure_ascii=False, indent=2)[:800] + '\n...')
    except FileNotFoundError as e:
        print('‚ö†Ô∏è Îç∞Ïù¥ÌÑ∞ ÌååÏùºÏùÑ Ï∞æÏßÄ Î™ªÌñàÏäµÎãàÎã§:', e)
        print('Ïòà) /content/bigcon/shinhan/big_data_set1_f.csv, big_data_set2_f.csv, big_data_set3_f.csv')
        print('   /content/bigcon/external/weather.csv (ÏÑ†ÌÉù)')
    except Exception as e:
        print('‚ö†Ô∏è Ïã§Ìñâ Ï§ë Ïò§Î•ò:', e)

if __name__ == '__main__':
    main()
