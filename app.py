import os, json, traceback, re, hashlib
from pathlib import Path
import streamlit as st
import pandas as pd
from diagnostics import (
    build_catalog,
    export_reports,
    export_access,
    load_set1,
    run_access_audit,
    summarize_access,
    summarize_catalog,
)

# ===== í˜ì´ì§€ ê¸°ë³¸ =====
st.set_page_config(page_title="ì„±ë™êµ¬ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì‚¬ (MVP)", page_icon="ğŸ’¬", layout="wide")
st.title("ì„±ë™êµ¬ ì†Œìƒê³µì¸ ë¹„ë°€ìƒë‹´ì‚¬ (MVP)")
st.caption("Agent-1: ë°ì´í„° ì§‘ê³„/ìš”ì•½ â†’ Agent-2: ì‹¤í–‰ì¹´ë“œ(JSON) ìƒì„±")
show_debug = st.checkbox("ğŸ” ë””ë²„ê·¸ ë³´ê¸°", value=True)


def _env_flag(name: str, default: str) -> str:
    value = os.getenv(name)
    return value if value is not None else default


DEBUG_SHOW_RAW = _env_flag("DEBUG_SHOW_RAW", "true").lower() in {"1", "true", "yes"}


@st.cache_data(show_spinner=False)
def _load_set1_cached(path: str = "data/shinhan/big_data_set1_f.csv") -> pd.DataFrame:
    """Cached helper that wraps :func:`diagnostics.load_set1`."""

    return load_set1(path)


def _hash_dataframe(df: pd.DataFrame) -> str:
    """Create a stable hash signature for a dataframe's current content."""

    if df is None or df.empty:
        return "empty"
    csv_bytes = df.to_csv(index=False).encode("utf-8")
    return hashlib.md5(csv_bytes).hexdigest()


def _get_debug_section(agent1_json: dict | None) -> dict:
    debug = (agent1_json or {}).get("debug")
    return debug if isinstance(debug, dict) else {}


def _get_debug_snapshot(agent1_json: dict | None) -> dict:
    debug = _get_debug_section(agent1_json)
    snap = debug.get("snapshot")
    if isinstance(snap, dict):
        sanitized = snap.get("sanitized")
        if isinstance(sanitized, dict):
            return sanitized
    legacy = debug.get("sanitized_snapshot")
    return legacy if isinstance(legacy, dict) else {}


def _get_debug_raw_snapshot(agent1_json: dict | None) -> dict:
    debug = _get_debug_section(agent1_json)
    snap = debug.get("snapshot")
    if isinstance(snap, dict):
        raw = snap.get("raw")
        if isinstance(raw, dict):
            return raw
    legacy = debug.get("latest_raw_snapshot")
    return legacy if isinstance(legacy, dict) else {}


def render_debug_view(agent1_json: dict | None, show_raw: bool = DEBUG_SHOW_RAW) -> None:
    debug = _get_debug_section(agent1_json)
    if not debug:
        st.info("ë””ë²„ê·¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    def _flatten_rows(obj: dict) -> pd.DataFrame:
        rows = []
        for key, value in (obj or {}).items():
            if isinstance(value, (dict, list)):
                try:
                    text = json.dumps(value, ensure_ascii=False)
                except TypeError:
                    text = str(value)
                rows.append({"í•­ëª©": key, "ê°’": text})
            else:
                rows.append({"í•­ëª©": key, "ê°’": value})
        return pd.DataFrame(rows)

    def _numeric_values(data):
        if isinstance(data, dict):
            for val in data.values():
                yield from _numeric_values(val)
        elif isinstance(data, (list, tuple)):
            for item in data:
                yield from _numeric_values(item)
        elif isinstance(data, (int, float)):
            yield data

    errors = debug.get("errors", [])
    resolve_info = debug.get("resolve", {}) or {}
    panel_info = debug.get("panel", {}) or {}
    snapshot_info = (debug.get("snapshot", {}) or {})
    sanitized_snapshot = snapshot_info.get("sanitized") or {}
    agent1_llm = debug.get("agent1_llm", {}) or {}

    warnings = []
    if resolve_info.get("resolved_merchant_id") is None:
        warnings.append("ê°€ë§¹ì  ë¯¸í™•ì •: ì „í‘œë³¸ ìš”ì•½ìœ¼ë¡œ ë–¨ì–´ì§ˆ ìœ„í—˜")
    rows_after = panel_info.get("rows_after")
    if rows_after is not None and rows_after != 1:
        warnings.append("ë‹¨ì¼ ìƒì  íŒ¨ë„ ì•„ë‹˜")
    for num in _numeric_values(sanitized_snapshot):
        try:
            val = float(num)
        except (TypeError, ValueError):
            continue
        if val < 0 or val > 100:
            warnings.append("ì •ê·œí™” ì‹¤íŒ¨ ì˜ì‹¬")
            break
    if agent1_llm.get("safety_blocked"):
        warnings.append("LLM ì•ˆì „ì„± ì°¨ë‹¨")

    for err in errors:
        stage = err.get('stage', 'unknown')
        msg = err.get('msg', '')
        st.error(f"[{stage}] {msg}")
    for warn in warnings:
        st.error(warn)

    input_info = debug.get("input", {}) or {}
    st.markdown("#### ì…ë ¥/í”Œë˜ê·¸")
    st.write("ì›ë¬¸:", input_info.get("original") or "â€”")
    flags = input_info.get("flags") or {}
    if flags:
        st.write({k: flags.get(k) for k in sorted(flags)})

    parse_info = debug.get("parse", {}) or {}
    st.markdown("#### íŒŒì‹± ê²°ê³¼")
    st.write({
        "merchant_mask": parse_info.get("merchant_mask"),
        "mask_prefix": parse_info.get("mask_prefix"),
        "sigungu": parse_info.get("sigungu"),
        "pattern_used": parse_info.get("pattern_used"),
        "elapsed_ms": parse_info.get("elapsed_ms"),
    })

    st.markdown("#### ê°€ë§¹ì  ë§¤ì¹­")
    st.write({
        "path": resolve_info.get("path"),
        "resolved_merchant_id": resolve_info.get("resolved_merchant_id"),
        "elapsed_ms": resolve_info.get("elapsed_ms"),
    })
    candidates = resolve_info.get("candidates_top3") or []
    if candidates:
        st.table(pd.DataFrame(candidates))
    else:
        st.write("í›„ë³´ ì—†ìŒ")

    st.markdown("#### íŒ¨ë„ í•„í„°")
    st.write({
        "rows_before": panel_info.get("rows_before"),
        "rows_after": panel_info.get("rows_after"),
        "latest_ta_ym": panel_info.get("latest_ta_ym"),
        "elapsed_ms": panel_info.get("elapsed_ms"),
    })

    st.markdown("#### ìŠ¤ëƒ…ìƒ·")
    if show_raw:
        raw_df = _flatten_rows(snapshot_info.get("raw") or {})
        if not raw_df.empty:
            st.caption("ì›ë³¸(raw)")
            st.table(raw_df)
    sanitized_df = _flatten_rows(sanitized_snapshot)
    if not sanitized_df.empty:
        st.caption("ì •ê·œí™”(sanitized)")
        st.table(sanitized_df)

    render_info = debug.get("render", {}) or {}
    table_dict = render_info.get("table_dict")
    if isinstance(table_dict, dict) and table_dict:
        st.markdown("#### ë Œë” í…Œì´ë¸”")
        st.table(pd.DataFrame([table_dict]))

    st.markdown("#### Agent-1 LLM")
    st.write({
        "used": agent1_llm.get("used"),
        "model": agent1_llm.get("model"),
        "resp_bytes": agent1_llm.get("resp_bytes"),
        "safety_blocked": agent1_llm.get("safety_blocked"),
        "elapsed_ms": agent1_llm.get("elapsed_ms"),
    })
    preview = agent1_llm.get("prompt_preview")
    if preview:
        st.caption("í”„ë¡¬í”„íŠ¸ í”„ë¦¬ë·°")
        st.code(preview)


def _mask_name(raw: str) -> str:
    if not raw:
        return "â€”"
    name = str(raw)
    if "*" in name:
        return name
    trimmed = name.strip()
    if len(trimmed) <= 1:
        return trimmed or "â€”"
    if len(trimmed) == 2:
        return trimmed[0] + "*"
    return trimmed[:2] + ("*" * max(4, len(trimmed) - 2))


def _extract_merchant_name(agent1_json: dict) -> str:
    context = (agent1_json or {}).get("context", {})
    keys = [
        "merchant_masked_name",
        "masked_name",
        "merchant_name_masked",
        "merchant_name",
        "store_name"
    ]
    for key in keys:
        val = context.get(key)
        if val:
            return _mask_name(val)
    return "â€”"


def _format_percent(value) -> str:
    if value is None:
        return "â€”"
    try:
        num = float(value)
    except (TypeError, ValueError):
        return "â€”"
    if num < 0 or num > 100:
        return "â€”"
    return f"{num:.1f}%"


def _humanize_age_segment(code: str) -> str:
    if not code or not isinstance(code, str):
        return None
    parts = code.split("_")
    if len(parts) < 3:
        return code
    gender_code = parts[1]
    age_code = parts[2]
    gender_map = {"MAL": "ë‚¨ì„±", "FME": "ì—¬ì„±"}
    age_map = {
        "1020": "10-20ëŒ€",
        "30": "30ëŒ€",
        "40": "40ëŒ€",
        "50": "50ëŒ€",
        "60": "60ëŒ€"
    }
    gender = gender_map.get(gender_code, "")
    age = age_map.get(age_code, age_code)
    label = " ".join([v for v in [gender, age] if v])
    return label or code


def _collect_major_customers(agent1_json: dict) -> str:
    debug_snapshot = _get_debug_snapshot(agent1_json)
    kpis = (agent1_json or {}).get("kpis", {})
    segments = []
    age_segments = debug_snapshot.get("age_top_segments") or kpis.get("age_top_segments") or []
    for item in age_segments[:3]:
        label = item.get("label") if isinstance(item, dict) else None
        value = item.get("value") if isinstance(item, dict) else None
        if label and value is not None:
            segments.append(f"{label} {_format_percent(value)}")
    if not segments:
        top_seg = kpis.get("top_age_segment")
        human = _humanize_age_segment(top_seg)
        if human:
            segments.append(human)
        youth_share = kpis.get("youth_share_avg")
        if youth_share is not None:
            segments.append(f"ì²­ë…„ {_format_percent(youth_share)}")
    unique = []
    for item in segments:
        if item and item not in unique:
            unique.append(item)
    return ", ".join(unique[:3]) if unique else "â€”"


def _format_customer_mix(detail: dict | None) -> str:
    if not detail or not isinstance(detail, dict):
        return "â€”"
    ordered_labels = ["ìœ ë™", "ê±°ì£¼", "ì§ì¥"]
    parts = []
    for label in ordered_labels:
        value = detail.get(label)
        if value is not None:
            percent = _format_percent(value)
            if percent != "â€”":
                parts.append(f"{label} {percent}")
    for label, value in detail.items():
        if label in ordered_labels:
            continue
        if value is None:
            continue
        percent = _format_percent(value)
        if percent != "â€”":
            parts.append(f"{label} {percent}")
    return ", ".join(parts[:3]) if parts else "â€”"


def _collect_overview_row(agent1_json: dict) -> tuple[pd.DataFrame, dict]:
    context = (agent1_json or {}).get("context", {})
    parsed = context.get("parsed", {})
    merchant = context.get("merchant", {})
    industry_candidate = merchant.get("category") or parsed.get("merchant_industry_label") or parsed.get("industry")
    industry_labels = {
        "cafe": "ì¹´í˜",
        "restaurant": "ìŒì‹ì ",
        "retail": "ì†Œë§¤"
    }
    industry = industry_labels.get(industry_candidate, industry_candidate or "â€”")
    addr = merchant.get("address") or context.get("address_masked") or context.get("address") or context.get("addr_base")
    if isinstance(addr, (list, tuple)):
        addr = " / ".join([str(v) for v in addr if v])
    address = addr if addr else "â€”"

    debug_snapshot = _get_debug_snapshot(agent1_json)
    kpis = (agent1_json or {}).get("kpis", {})
    new_rate = debug_snapshot.get("new_pct", kpis.get("new_rate_avg"))
    revisit_rate = debug_snapshot.get("revisit_pct", kpis.get("revisit_rate_avg"))
    new_text = _format_percent(new_rate)
    revisit_text = _format_percent(revisit_rate)
    if new_text == "â€”" and revisit_text == "â€”":
        new_revisit = "â€”"
    else:
        new_revisit = f"ì‹ ê·œ {new_text} / ì¬ë°©ë¬¸ {revisit_text}"

    customer_mix_detail = debug_snapshot.get("customer_mix_detail") or kpis.get("customer_mix_detail")
    customer_type = _format_customer_mix(customer_mix_detail)
    spend_band = (
        debug_snapshot.get("avg_ticket_band_label")
        or kpis.get("avg_ticket_band_label")
        or context.get("avg_ticket_band")
        or "â€”"
    )
    if isinstance(spend_band, str):
        spend_band = spend_band.strip()
        spend_band = re.sub(r"(ìƒìœ„)(\d)", r"\1 \2", spend_band)
    elif spend_band is None:
        spend_band = "â€”"

    data = {
        "ì—…ì¢…": industry,
        "ì£¼ì†Œ": address,
        "ì£¼ìš” ê³ ê°ì¸µ": _collect_major_customers(agent1_json),
        "ê³ ê° ìœ í˜•": customer_type if customer_type else "â€”",
        "ì‹ ê·œ/ì¬ë°©ë¬¸": new_revisit,
        "ê°ë‹¨ê°€ êµ¬ê°„": spend_band if spend_band else "â€”"
    }
    return pd.DataFrame([data]), data


def _build_diagnosis(agent1_json: dict) -> str:
    debug_snapshot = _get_debug_snapshot(agent1_json)
    kpis = (agent1_json or {}).get("kpis", {})
    sentences = []

    mix_detail = debug_snapshot.get("customer_mix_detail") or kpis.get("customer_mix_detail")
    mix_items = []
    if isinstance(mix_detail, dict):
        sorted_mix = sorted(
            [(label, val) for label, val in mix_detail.items() if val is not None],
            key=lambda x: x[1],
            reverse=True,
        )
        for label, value in sorted_mix[:2]:
            percent = _format_percent(value)
            if percent != "â€”":
                mix_items.append(f"{label} {percent}")
    if mix_items:
        sentences.append(" Â· ".join(mix_items) + " ê³ ê° êµ¬ì„±ì…ë‹ˆë‹¤.")

    rate_parts = []
    new_text = _format_percent(debug_snapshot.get("new_pct", kpis.get("new_rate_avg")))
    revisit_text = _format_percent(debug_snapshot.get("revisit_pct", kpis.get("revisit_rate_avg")))
    youth_text = _format_percent(debug_snapshot.get("youth_pct", kpis.get("youth_share_avg")))
    if new_text != "â€”":
        rate_parts.append(f"ì‹ ê·œ {new_text}")
    if revisit_text != "â€”":
        rate_parts.append(f"ì¬ë°©ë¬¸ {revisit_text}")
    if youth_text != "â€”":
        rate_parts.append(f"ì²­ë…„ ê³ ê° {youth_text}")
    if rate_parts:
        sentences.append(" Â· ".join(rate_parts) + "ì…ë‹ˆë‹¤.")

    if not sentences:
        return "â€”"
    return " ".join(sentences[:2])


def _build_goal_lines(agent1_json: dict) -> tuple[str, list[str]]:
    period = (agent1_json or {}).get("period", {})
    months = period.get("months")
    weeks_requested = period.get("weeks_requested")
    if weeks_requested:
        try:
            weeks_val = int(weeks_requested)
        except (TypeError, ValueError):
            weeks_val = None
        if weeks_val and months:
            period_text = f"í–¥í›„ {weeks_val}ì£¼ (ì•½ {months}ê°œì›”)"
        elif weeks_val:
            period_text = f"í–¥í›„ {weeks_val}ì£¼"
        else:
            period_text = "ê¸°ê°„ ì •ë³´ â€”"
    elif months:
        period_text = f"ìµœê·¼ {months}ê°œì›”"
    else:
        period_text = "ê¸°ê°„ ì •ë³´ â€”"

    debug_snapshot = _get_debug_snapshot(agent1_json)
    kpis = (agent1_json or {}).get("kpis", {})
    mapping = [
        ("revisit_rate_avg", "ì¬ë°©ë¬¸ìœ¨"),
        ("new_rate_avg", "ì‹ ê·œ ê³ ê° ë¹„ì¤‘"),
        ("youth_share_avg", "ì²­ë…„ ê³ ê° ë¹„ì¤‘")
    ]
    lines = []
    for key, label in mapping:
        sanitized_key = {
            "revisit_rate_avg": "revisit_pct",
            "new_rate_avg": "new_pct",
            "youth_share_avg": "youth_pct",
        }.get(key)
        value = debug_snapshot.get(sanitized_key, kpis.get(key))
        if value is not None:
            lines.append(f"{label}: í˜„í™© {_format_percent(value)} â†’ ëª©í‘œ êµ¬ê°„ â€”")
    if not lines:
        lines.append("KPI ëª©í‘œ êµ¬ê°„ â€”")
    return period_text, lines[:3]


def _format_list(values) -> str:
    if not values:
        return "â€”"
    if isinstance(values, (list, tuple)):
        items = [str(v) for v in values if v]
        return " Â· ".join(items) if items else "â€”"
    return str(values)


def _format_kpi(kpi_obj: dict) -> str:
    if not isinstance(kpi_obj, dict):
        return "â€”"
    target = kpi_obj.get("target") or "â€”"
    uplift = kpi_obj.get("expected_uplift")
    rng = kpi_obj.get("range")
    parts = [f"íƒ€ê¹ƒ: {target}"]
    if uplift is not None:
        parts.append(f"ê¸°ëŒ€ ìƒìŠ¹ {uplift}")
    if isinstance(rng, (list, tuple)) and len(rng) == 2 and any(r is not None for r in rng):
        parts.append(f"ëª©í‘œ êµ¬ê°„ {rng[0]}~{rng[1]}")
    else:
        parts.append("ëª©í‘œ êµ¬ê°„ â€”")
    return " Â· ".join(parts)


def _split_cards(cards: list[dict]) -> tuple[list[dict], list[dict]]:
    if not cards:
        return [], []
    main, booster = [], []
    for card in cards:
        title = str(card.get("title", ""))
        if "ë³´ê°•" in title or "ë°ì´í„°" in title:
            booster.append(card)
        else:
            main.append(card)
    return main, booster


def render_summary_view(
    agent1_json: dict,
    agent2_json: dict,
    overview_df: pd.DataFrame | None = None,
    table_dict: dict | None = None,
) -> None:
    merchant_title = _extract_merchant_name(agent1_json)
    st.header(f"ğŸ“Š {merchant_title} ê°€ë§¹ì  ë°©ë¬¸ ê³ ê° í˜„í™© ë¶„ì„")

    context = (agent1_json or {}).get("context", {})
    if context and not context.get("merchant"):
        st.warning("ì§ˆë¬¸ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê°€ë§¹ì ì„ ì°¾ì§€ ëª»í•´ í‘œë³¸ ì „ì²´ ìš”ì•½ì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤.")

    debug_info = _get_debug_section(agent1_json)
    if overview_df is None or table_dict is None:
        render_info = debug_info.get("render") if isinstance(debug_info, dict) else None
        if isinstance(render_info, dict) and isinstance(render_info.get("table_dict"), dict):
            table_dict = render_info.get("table_dict")
            overview_df = pd.DataFrame([table_dict])
        else:
            overview_df, table_dict = _collect_overview_row(agent1_json)
            if isinstance(debug_info, dict):
                debug_info.setdefault("render", {})["table_dict"] = table_dict
    if overview_df is None:
        if table_dict:
            overview_df = pd.DataFrame([table_dict])
        else:
            overview_df = pd.DataFrame()
    try:
        print("ğŸ“Š overview_table:", json.dumps(overview_df.to_dict(orient="records"), ensure_ascii=False))
    except Exception:
        pass
    st.subheader("í˜„í™© í‘œ")
    st.table(overview_df)

    st.subheader("í•œ ì¤„ ì§„ë‹¨")
    st.markdown(f"- {_build_diagnosis(agent1_json)}")

    period_text, goal_lines = _build_goal_lines(agent1_json)
    st.subheader("ëª©í‘œ")
    st.markdown(f"- ê¸°ê°„ ê°€ì •: {period_text}")
    for line in goal_lines:
        st.markdown(f"- {line}")

    cards = (agent2_json or {}).get("recommendations", [])
    main_cards, booster_cards = _split_cards(cards)
    display_cards = main_cards[:2]
    if booster_cards:
        display_cards.extend(booster_cards[:1])

    st.subheader("ì‹¤í–‰ ì¹´ë“œ")
    if not display_cards:
        st.info("ì‹¤í–‰ ì¹´ë“œê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    for card in display_cards:
        with st.container():
            st.markdown(f"**{card.get('title', 'â€”')}**")
            st.markdown(f"- íƒ€ê²Ÿ: {card.get('what', 'â€”')}")
            st.markdown(f"- ì±„ë„: {_format_list(card.get('where'))}")
            st.markdown(f"- ë°©ë²•: {_format_list(card.get('how'))}")
            st.markdown(f"- ì¹´í”¼: {_format_list(card.get('copy'))}")
            st.markdown(f"- KPI: {_format_kpi(card.get('kpi'))}")
            st.markdown(f"- ë¦¬ìŠ¤í¬/ì™„í™”: {_format_list(card.get('risks'))}")
            st.markdown(f"- ê·¼ê±°: {_format_list(card.get('evidence'))}")

    limits = (agent1_json or {}).get("limits", [])
    st.subheader("í•œê³„/ë°ì´í„° ë³´ê°•")
    st.markdown("**í˜„ì¬ í•œê³„**")
    if limits:
        for item in limits[:5]:
            st.markdown(f"- {item}")
    else:
        st.markdown("- í•œê³„ ì •ë³´ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    improvement_suggestions = []
    for item in limits:
        text = str(item)
        if "ë‚ ì”¨" in text:
            improvement_suggestions.append("ë‚ ì”¨ ë°ì´í„° ì—°ê³„ë¥¼ í†µí•´ ìš°ì²œ ê°€ì„¤ì„ ê²€ì¦í•©ë‹ˆë‹¤.")
        elif "í‘œë³¸" in text or "ë°ì´í„°" in text:
            improvement_suggestions.append("ëˆ„ë½ êµ¬ê°„ì„ ì ê²€í•´ í‘œë³¸ì„ ë³´ê°•í•©ë‹ˆë‹¤.")
    if not improvement_suggestions:
        improvement_suggestions.append("ë‹¤ìŒ ìŠ¤í”„ë¦°íŠ¸ì—ì„œ ê²°í•© ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì¬ì ê²€í•©ë‹ˆë‹¤.")

    st.markdown("**ë‹¤ìŒ ìŠ¤í”„ë¦°íŠ¸ ë³´ê°• ê³„íš**")
    for suggestion in improvement_suggestions[:3]:
        st.markdown(f"- {suggestion}")

# ===== ê²½ë¡œ & í‚¤ =====
DATA_DIR = Path("data")
SHINHAN_DIR = DATA_DIR / "shinhan"
EXTERNAL_DIR = DATA_DIR / "external"

API_KEY = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY", "")
if not API_KEY:
    st.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ì•± ì„¤ì •ì˜ App secretsì— ë“±ë¡í•˜ì„¸ìš”)")

# ===== ì‚¬ì´ë“œë°”: ë°ì´í„° ìƒíƒœ =====
st.sidebar.header("ë°ì´í„° ìƒíƒœ")
st.sidebar.write(f"ğŸ“ SHINHAN_DIR ì¡´ì¬: {SHINHAN_DIR.exists()}")
st.sidebar.write(f"ğŸ“ EXTERNAL_DIR ì¡´ì¬: {EXTERNAL_DIR.exists()}")

# ===== íƒ­ êµ¬ì„± =====
analysis_tab, diagnostics_tab = st.tabs(["ğŸ“ˆ ë¶„ì„", "ğŸ§ª ì§„ë‹¨"])

with analysis_tab:
    default_q = "ì„±ë™êµ¬ {ê³ í–¥***} ê¸°ì¤€ìœ¼ë¡œ, ì¬ë°©ë¬¸ìœ¨ 4ì£¼ í”Œëœ ì‘ì„±í•´ì¤˜."
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", value=default_q)
    st.caption("ìƒí˜¸ëŠ” ë°˜ë“œì‹œ {}ë¡œ ê°ì‹¸ ì£¼ì„¸ìš”. ì˜ˆ) ì„±ë™êµ¬ {ë™ëŒ€******}")

    if st.button("ë¶„ì„ ì‹¤í–‰", type="primary"):
        from bigcon_2agent_mvp_v3 import agent1_pipeline, build_agent2_prompt, call_gemini_agent2

        with st.spinner("Agent-1: ë°ì´í„° ì§‘ê³„/ìš”ì•½ ì¤‘..."):
            try:
                a1 = agent1_pipeline(question, SHINHAN_DIR, EXTERNAL_DIR)
                try:
                    overview_df, table_dict = _collect_overview_row(a1)
                except Exception:
                    overview_df, table_dict = pd.DataFrame(), {}
                if isinstance(a1, dict):
                    dbg = _get_debug_section(a1)
                    dbg.setdefault('render', {})['table_dict'] = table_dict
                    a1['debug'] = dbg
                st.session_state['_latest_overview'] = (overview_df, table_dict)
                st.session_state['_latest_agent1'] = a1
                st.success("Agent-1 JSON ìƒì„± ì™„ë£Œ")
                with st.expander("ğŸ” Agent-1 ì¶œë ¥(JSON) ë³´ê¸°", expanded=False):
                    st.json(a1)
            except Exception:
                st.error("Agent-1 ì‹¤í–‰ ì˜¤ë¥˜")
                st.code(traceback.format_exc())
                st.stop()

        with st.spinner("Agent-2: ì¹´ë“œ ìƒì„± ì¤‘..."):
            try:
                os.environ["GEMINI_API_KEY"] = API_KEY
                prompt_text = build_agent2_prompt(a1)
                result = call_gemini_agent2(prompt_text)
                st.success("Agent-2 ì¹´ë“œ ìƒì„± ì™„ë£Œ")
            except Exception:
                st.error("Agent-2 ì‹¤í–‰ ì˜¤ë¥˜")
                st.code(traceback.format_exc())
                st.stop()

        try:
            overview_cached = st.session_state.get('_latest_overview', (None, None))
            render_summary_view(a1, result, overview_df=overview_cached[0], table_dict=overview_cached[1])
        except Exception:
            st.error("ìš”ì•½ ë·°ë¥¼ ë Œë”ë§í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            st.code(traceback.format_exc())

        with st.expander("ğŸ§¾ Agent-2 ì¶œë ¥(JSON) ë³´ê¸°", expanded=False):
            st.json(result)

    if show_debug:
        latest_agent1 = st.session_state.get('_latest_agent1')
        with st.expander("ğŸ” ë””ë²„ê·¸ ìƒì„¸", expanded=True):
            render_debug_view(latest_agent1, show_raw=DEBUG_SHOW_RAW)

    if not st.session_state.get("_intro_shown"):
        st.info("âœ… ì—…ë¡œë“œ ì„±ê³µ! ì´ì œ ì§ˆë¬¸ ì…ë ¥ í›„ [ë¶„ì„ ì‹¤í–‰]ì„ ëˆŒëŸ¬ ì¹´ë“œ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì„¸ìš”.")
        st.session_state["_intro_shown"] = True

with diagnostics_tab:
    st.subheader("ìƒí˜¸ ì¹´íƒˆë¡œê·¸")

    try:
        base_df = _load_set1_cached()
    except FileNotFoundError:
        st.error("Set1 CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (data/shinhan/big_data_set1_f.csv)")
        base_df = pd.DataFrame(columns=["ENCODED_MCT", "MCT_NM", "SIGUNGU", "CATEGORY", "MCT_BRD_NUM"])
        st.session_state.pop('diagnostics_catalog', None)
    except Exception as exc:  # pragma: no cover - defensive guard
        st.error(f"Set1 CSVë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {exc}")
        base_df = pd.DataFrame(columns=["ENCODED_MCT", "MCT_NM", "SIGUNGU", "CATEGORY", "MCT_BRD_NUM"])
        st.session_state.pop('diagnostics_catalog', None)

    unique_sigungu = sorted({str(v) for v in base_df.get("SIGUNGU", pd.Series(dtype="string")).dropna().unique() if str(v).strip()})
    sigungu_options = ["ì „ì²´"] + unique_sigungu if unique_sigungu else ["ì „ì²´"]
    default_sigungu = "ì„±ë™êµ¬" if "ì„±ë™êµ¬" in sigungu_options else sigungu_options[0]
    selected_sigungu = st.selectbox("ì‹œêµ°êµ¬ ì„ íƒ", options=sigungu_options, index=sigungu_options.index(default_sigungu))

    max_rows = int(st.number_input("ìµœëŒ€ í‘œê¸° í–‰ìˆ˜", min_value=10, max_value=5000, value=100, step=10))
    search_query = st.text_input("ìƒí˜¸ ê²€ìƒ‰", value="", placeholder="ìƒí˜¸ ì¼ë¶€ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    note_filter = st.multiselect("ìƒíƒœ í•„í„°(note)", options=["missing_id", "ok", "duplicate_name"])
    sort_option = st.selectbox("ì •ë ¬", options=["ì í¬ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ", "ìƒí˜¸ëª… ì˜¤ë¦„ì°¨ìˆœ"])

    if st.button("ìƒí˜¸ ì¹´íƒˆë¡œê·¸ ìƒì„±", key="btn_catalog"):
        if base_df.empty:
            st.warning("ì¹´íƒˆë¡œê·¸ë¥¼ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state.pop('diagnostics_catalog', None)
        else:
            with st.spinner("ìƒí˜¸ ì¹´íƒˆë¡œê·¸ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                sigungu_filter = None if selected_sigungu == "ì „ì²´" else selected_sigungu
                catalog_df = build_catalog(base_df, sigungu_filter=sigungu_filter)
                catalog_df = catalog_df.reset_index(drop=True)
            st.session_state['diagnostics_catalog'] = {
                'source': catalog_df,
                'sigungu': sigungu_filter,
                'signature': None,
                'exports': None,
            }
            st.success("ìƒí˜¸ ì¹´íƒˆë¡œê·¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

    diag_state = st.session_state.get('diagnostics_catalog')
    if not diag_state or diag_state.get('source') is None:
        st.info("ìƒë‹¨ì˜ [ìƒí˜¸ ì¹´íƒˆë¡œê·¸ ìƒì„±] ë²„íŠ¼ì„ ëˆŒëŸ¬ ì§„ë‹¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
    else:
        catalog_df = diag_state['source']
        display_df = catalog_df.copy()

        if search_query.strip():
            display_df = display_df[display_df['MCT_NM'].fillna("").str.contains(search_query.strip(), case=False, na=False)]
        if note_filter:
            display_df = display_df[display_df['note'].isin(note_filter)]

        if sort_option == "ì í¬ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ" and 'n_locations' in display_df:
            display_df = display_df.sort_values('n_locations', ascending=False)
        elif sort_option == "ìƒí˜¸ëª… ì˜¤ë¦„ì°¨ìˆœ" and 'MCT_NM' in display_df:
            display_df = display_df.sort_values('MCT_NM', na_position='last')

        display_df = display_df.reset_index(drop=True)
        summary = summarize_catalog(display_df)

        signature = _hash_dataframe(display_df)
        if signature != diag_state.get('signature'):
            exports = export_reports(display_df, summary)
            diag_state['signature'] = signature
            diag_state['exports'] = exports
            st.session_state['diagnostics_catalog'] = diag_state

        summary_metrics = [
            ("ì´ ìƒí˜¸", f"{summary.get('total_rows', 0):,}"),
            ("ê³ ìœ  ìƒí˜¸", f"{summary.get('unique_names', 0):,}"),
            ("ID ëˆ„ë½ ë¹„ìœ¨", f"{summary.get('pct_missing_id', 0.0):.2f}%"),
            ("ë™ëª… ë‹¤ì í¬ ë¹„ìœ¨", f"{summary.get('pct_duplicate_name', 0.0):.2f}%"),
        ]
        metric_cols = st.columns(len(summary_metrics))
        for col, (label, value) in zip(metric_cols, summary_metrics):
            col.metric(label, value)

        top_dups = summary.get('top_duplicated') or []
        if top_dups:
            st.markdown("**ë‹¤ì í¬ ìƒìœ„ 10ê°œ**")
            st.table(pd.DataFrame(top_dups))

        display_columns = ["SIGUNGU", "MCT_NM", "MCT_BRD_NUM", "n_locations", "has_encoded_mct", "note"]
        for col in display_columns:
            if col not in display_df.columns:
                display_df[col] = pd.NA
        st.markdown("**ìƒí˜¸ ëª©ë¡ (ìš”ì•½)**")
        st.dataframe(display_df[display_columns].head(max_rows))

        if 'encoded_mct_list' in display_df.columns:
            with st.expander("encoded_mct_list ë³´ê¸°", expanded=False):
                st.dataframe(display_df[["SIGUNGU", "MCT_NM", "encoded_mct_list"]].head(max_rows))

        exports = diag_state.get('exports') or {}
        csv_path = exports.get('catalog_csv')
        json_path = exports.get('summary_json')
        dl_cols = st.columns(2)
        if csv_path and Path(csv_path).exists():
            with open(csv_path, "rb") as fp:
                dl_cols[0].download_button(
                    "ì¹´íƒˆë¡œê·¸ CSV ë‹¤ìš´ë¡œë“œ",
                    data=fp.read(),
                    file_name=Path(csv_path).name,
                    mime="text/csv",
                )
        else:
            dl_cols[0].write("CSV íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if json_path and Path(json_path).exists():
            with open(json_path, "rb") as fp:
                dl_cols[1].download_button(
                    "ìš”ì•½ JSON ë‹¤ìš´ë¡œë“œ",
                    data=fp.read(),
                    file_name=Path(json_path).name,
                    mime="application/json",
                )
        else:
            dl_cols[1].write("JSON íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    st.markdown("---")
    st.subheader("ì ‘ê·¼ ê°ì‚¬ (Access Audit)")

    audit_default_index = sigungu_options.index(default_sigungu)
    audit_sigungu = st.selectbox(
        "ì‹œêµ°êµ¬ ì„ íƒ (ì ‘ê·¼ ê°ì‚¬)",
        options=sigungu_options,
        index=audit_default_index,
        key="audit_sigungu",
    )

    audit_mode = st.radio(
        "ìƒ˜í”Œ ëª¨ë“œ",
        options=["Random", "Search"],
        key="audit_mode",
        horizontal=True,
    )

    audit_terms: list[str] | None = None
    if audit_mode == "Random":
        audit_n = int(
            st.number_input(
                "ìƒ˜í”Œ ìˆ˜",
                min_value=1,
                max_value=200,
                value=10,
                step=1,
                key="audit_sample_count",
            )
        )
        audit_prefix = int(
            st.number_input(
                "ë§ˆìŠ¤í‚¹ ì ‘ë‘ ê¸¸ì´",
                min_value=1,
                max_value=4,
                value=2,
                step=1,
                key="audit_prefix_len",
            )
        )
    else:
        search_input = st.text_area(
            "ìƒí˜¸ ê²€ìƒ‰ì–´ (ì¤„ë‹¹ 1ê°œ, {ê³ í–¥***} í˜•ì‹)",
            value="",
            key="audit_search_terms",
            height=120,
        )
        audit_terms = [line.strip() for line in search_input.splitlines() if line.strip()]
        audit_n = 0
        audit_prefix = 2

    audit_brand_match = st.checkbox(
        "ë™ì¼ ë¸Œëœë“œë¥¼ ì •ë‹µìœ¼ë¡œ ì¸ì •", value=True, key="audit_brand_match"
    )

    if st.button("ì ‘ê·¼ ê°ì‚¬ ì‹¤í–‰", key="btn_access_audit"):
        if audit_mode == "Search" and not audit_terms:
            st.warning("ê²€ìƒ‰ ëª¨ë“œì—ì„œëŠ” ìµœì†Œ 1ê°œì˜ ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        else:
            try:
                with st.spinner("ì ‘ê·¼ ê°ì‚¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    progress = st.progress(0)
                    audit_df = run_access_audit(
                        mode=audit_mode.lower(),
                        sigungu=None if audit_sigungu == "ì „ì²´" else audit_sigungu,
                        n=audit_n or 10,
                        search_terms=audit_terms,
                        mask_prefix_len=audit_prefix,
                        brand_match=audit_brand_match,
                        seed=42,
                    )
                    progress.progress(100)
                summary = summarize_access(audit_df)
                signature = _hash_dataframe(audit_df)
                audit_state = {
                    "df": audit_df,
                    "summary": summary,
                    "sigungu": audit_sigungu,
                    "mode": audit_mode,
                    "signature": signature,
                    "exports": None,
                }
                st.session_state['diagnostics_access_audit'] = audit_state
                st.success("ì ‘ê·¼ ê°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            except FileNotFoundError:
                st.error("Shinhan CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Set1/2/3)")
            except Exception:
                st.error("ì ‘ê·¼ ê°ì‚¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                st.code(traceback.format_exc())

    audit_state = st.session_state.get('diagnostics_access_audit')
    if not audit_state or audit_state.get('df') is None:
        st.info("ìƒë‹¨ì˜ [ì ‘ê·¼ ê°ì‚¬ ì‹¤í–‰] ë²„íŠ¼ì„ ëˆŒëŸ¬ ê²°ê³¼ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
    else:
        audit_df = audit_state['df']
        summary = audit_state.get('summary', {})
        signature = _hash_dataframe(audit_df)
        if signature != audit_state.get('signature'):
            audit_state['signature'] = signature
            audit_state['exports'] = None
            st.session_state['diagnostics_access_audit'] = audit_state

        if audit_df is not None and not audit_df.empty and not audit_state.get('exports'):
            audit_state['exports'] = export_access(audit_df, summary)
            st.session_state['diagnostics_access_audit'] = audit_state

        st.markdown(
            f"ì´ {len(audit_df):,}ê±´ ìƒ˜í”Œ Â· Resolver ì„±ê³µë¥  {summary.get('resolved_rate', 0.0):.1f}%"
        )

        metric_rows = [
            [
                ("Resolver ì„±ê³µë¥ ", f"{summary.get('resolved_rate', 0.0):.1f}%"),
                ("ì •í™•ë„", f"{summary.get('accuracy', 0.0):.1f}%"),
                ("Out-of-range ê±´ìˆ˜", f"{summary.get('out_of_range_count', 0)}"),
            ],
            [
                ("S1 ì ‘ê·¼ë¥ ", f"{summary.get('s1_access_rate', 0.0):.1f}%"),
                ("S2 ì ‘ê·¼ë¥ ", f"{summary.get('s2_access_rate', 0.0):.1f}%"),
                ("S3 ì ‘ê·¼ë¥ ", f"{summary.get('s3_access_rate', 0.0):.1f}%"),
            ],
            [
                ("S1 ì»¬ëŸ¼ ì»¤ë²„ë¦¬ì§€", f"{summary.get('s1_coverage', 0.0):.1f}%"),
                ("S2 ì»¬ëŸ¼ ì»¤ë²„ë¦¬ì§€", f"{summary.get('s2_coverage', 0.0):.1f}%"),
                ("S3 ì»¬ëŸ¼ ì»¤ë²„ë¦¬ì§€", f"{summary.get('s3_coverage', 0.0):.1f}%"),
            ],
            [
                ("S1 NaN ì¤‘ê°„ê°’", f"{summary.get('s1_nan_median', 0.0):.1f}%"),
                ("S2 NaN ì¤‘ê°„ê°’", f"{summary.get('s2_nan_median', 0.0):.1f}%"),
                ("S3 NaN ì¤‘ê°„ê°’", f"{summary.get('s3_nan_median', 0.0):.1f}%"),
            ],
        ]

        for items in metric_rows:
            cols = st.columns(len(items))
            for col, (label, value) in zip(cols, items):
                col.metric(label, value)

        warn_conditions = []
        if 'is_correct' in audit_df.columns:
            warn_conditions.append(~audit_df['is_correct'].astype(bool))
        if 's1_found' in audit_df.columns:
            warn_conditions.append(~audit_df['s1_found'].astype(bool))
        if 's2_found' in audit_df.columns:
            warn_conditions.append(~audit_df['s2_found'].astype(bool))
        if 's3_found' in audit_df.columns:
            warn_conditions.append(~audit_df['s3_found'].astype(bool))
        if 'out_of_range_flags' in audit_df.columns:
            warn_conditions.append(audit_df['out_of_range_flags'].fillna("") != "")

        warn_mask = None
        for cond in warn_conditions:
            warn_mask = cond if warn_mask is None else (warn_mask | cond)

        if warn_mask is None:
            warn_df = audit_df.iloc[0:0]
        else:
            warn_df = audit_df[warn_mask]

        warn_cols = [
            "input_text",
            "resolved_id",
            "truth_id",
            "truth_brand",
            "is_correct",
            "s1_found",
            "s2_found",
            "s3_found",
            "s1_cols_present",
            "s1_cols_expected",
            "s2_cols_present",
            "s2_cols_expected",
            "s3_cols_present",
            "s3_cols_expected",
            "out_of_range_flags",
            "path",
        ]
        if warn_df.empty:
            st.success("ê²½ê³  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  ìƒ˜í”Œì´ ì •ìƒ ì ‘ê·¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            for col in warn_cols:
                if col not in warn_df.columns:
                    warn_df[col] = pd.NA
            st.markdown("**ì‹¤íŒ¨/ê²½ê³  ìƒ˜í”Œ (ìƒìœ„ 100ê±´)**")
            st.dataframe(warn_df[warn_cols].head(100))

        exports = audit_state.get('exports') or {}
        dl_cols = st.columns(2)
        csv_path = exports.get('csv') if isinstance(exports, dict) else None
        json_path = exports.get('summary') if isinstance(exports, dict) else None
        if csv_path and Path(csv_path).exists():
            with open(csv_path, "rb") as fp:
                dl_cols[0].download_button(
                    "ì ‘ê·¼ ê°ì‚¬ CSV ë‹¤ìš´ë¡œë“œ",
                    data=fp.read(),
                    file_name=Path(csv_path).name,
                    mime="text/csv",
                )
        else:
            dl_cols[0].write("CSV íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        if json_path and Path(json_path).exists():
            with open(json_path, "rb") as fp:
                dl_cols[1].download_button(
                    "ì ‘ê·¼ ê°ì‚¬ ìš”ì•½ JSON ë‹¤ìš´ë¡œë“œ",
                    data=fp.read(),
                    file_name=Path(json_path).name,
                    mime="application/json",
                )
        else:
            dl_cols[1].write("JSON íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
